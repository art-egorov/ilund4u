{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer","stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#description","title":"Description","text":"<p>iLund4u is a bioinformatics tool for search and annotation of hotspots in a large set of proteomes. </p> <p>Supported input: gff3 for annotation files (prokka/pharokka generated); fasta for protein sequences       Programming language: Python3  OS: MacOS, Linux Python dependencies: biopython, bcbio-gff, scipy, configs, argparse, pandas, matplotlib, seaborn, progess, leidanalg, igraph. pyhmmer, msa4u, lovis4u   Python version: &gt;= 3.8 OS-level dependencies: MMseqs2 (included in the package) License: WTFPL Version: 0.0.8 (November 2024)</p>"},{"location":"#workflow","title":"Workflow","text":""},{"location":"#what-you-can-do-with-ilund4u","title":"What you can do with iLund4u","text":"<ul> <li>Cluster up to millions of genomes based on proteome composition similarity</li> <li>Annotate core and accessory genes within each cluster</li> <li>Identify and annotate genomic islands</li> <li>Annotate the functions of accessory proteins encoded within islands</li> <li>Cluster genomic islands to identify variability hotspots</li> <li>Get visualisation of clusters and hotspots with our LoVis4u library</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li>iLund4u can be installed directly from pypi:</li> </ul> <pre><code>python3 -m pip install ilund4u\n</code></pre> <ul> <li>The development version is available at github :</li> </ul> <pre><code>git clone https://github.com/art-egorov/ilund4u.git\ncd ilund4u\npython3 -m pip install --upgrade pip\npython3 -m pip install setuptools wheel\npython3 setup.py sdist\npython3 -m pip install .\n</code></pre> <p>! If you're a linux user, run <code>ilund4u --linux</code> post-install command once to update paths in the premade config files that set by default for MacOS users.</p>"},{"location":"#databases","title":"Databases","text":"<p>iLund4u has two precomputed databases of hotspots built on phage and plasmid sequences. The database of phages was built based on running hotspot annotation mode on all available PhageScope database sequences (~870K genomes, version of September 2024). For plasmids database we took IMG/PR database of plasmids (~700K sequences, version of June 2024).  </p> <p>To download iLund4u database from our server you can use the following argument: <code>--database &lt;phages|plasmids&gt;</code>. For example, to get plasmids database you need to run: <pre><code>ilund4u --database plasmids\n</code></pre></p> <p>Database sizes (compressed): Phages: 6.48GB; Plasmids: 1.07GB </p>"},{"location":"#reference","title":"Reference","text":"<p>If you find iLund4u useful, please cite: Artyom. A. Egorov, Vasili Hauryliuk, Gemma C. Atkinson, Systematic annotation of hyper-variability hotspots in phage genomes and plasmids, bioRxiv 22024.10.15.618418; doi: 10.1101/2024.10.15.618418</p>"},{"location":"#contact","title":"Contact","text":"<p>Please contact us by e-mail artemdotegorovATmeddotludotse or use Issues to report any technical problems. You can also use Discussions section for sharing your ideas or feature requests! </p>"},{"location":"#authors","title":"Authors","text":"<p>iLund4u is developed by Artyom Egorov at the Atkinson Lab, Department of Experimental Medical Science, Lund University, Sweden. We are open for suggestions to extend and improve iLund4u functionality. Please don't hesitate to share your ideas or feature requests.</p>"},{"location":"pypi/","title":"Pypi","text":""},{"location":"pypi/#description","title":"Description","text":"<p>iLund4u is a bioinformatics tool for search and annotation of hotspots in a large set of proteomes. </p> <p>Supported input: gff3 for annotation files (prokka/pharokka generated); fasta for protein sequences       Programming language: Python3  OS: MacOS, Linux Python dependencies: biopython, bcbio-gff, scipy, configs, argparse, pandas, matplotlib, seaborn, progess, leidanalg, igraph. pyhmmer, msa4u, lovis4u   Python version: &gt;= 3.8 OS-level dependencies: MMseqs2 (included in the package) License: WTFPL Version: 0.0.8 (November 2024)</p> <p>Detailed documentation with user guide is available at iLund4u Homepage</p>"},{"location":"pypi/#reference","title":"Reference","text":"<p>If you find iLund4u useful, please cite: Artyom. A. Egorov, Vasili Hauryliuk, Gemma C. Atkinson, Systematic annotation of hyper-variability hotspots in phage genomes and plasmids, bioRxiv 22024.10.15.618418; doi: 10.1101/2024.10.15.618418</p>"},{"location":"pypi/#contact","title":"Contact","text":"<p>Please contact us by e-mail artemdotegorovATmeddotludotse or use Issues to report any technical problems. You can also use Discussions section for sharing your ideas or feature requests! </p>"},{"location":"pypi/#authors","title":"Authors","text":"<p>iLund4u is developed by Artyom Egorov at the Atkinson Lab, Department of Experimental Medical Science, Lund University, Sweden. We are open for suggestions to extend and improve iLund4u functionality. Please don't hesitate to share your ideas or feature requests.</p>"},{"location":"API/data_manager/","title":"Data manager","text":"<p>This module provides data managing classes and methods for the tool.</p>"},{"location":"API/data_manager/#ilund4u.data_manager.DatabaseManager","title":"<code>DatabaseManager</code>","text":"<p>Manager for loading and building iLund4u database.</p> <p>Attributes:</p> <ul> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>class DatabaseManager:\n    \"\"\"Manager for loading and building iLund4u database.\n\n    Attributes:\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, parameters: ilund4u.manager.Parameters):\n        \"\"\"DatabaseManager class constructor.\n\n        Arguments:\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.prms = parameters\n\n    def build_database(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                       db_path: str) -&gt; None:\n        \"\"\"Write database.\n\n        Arguments:\n            proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n            hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n            db_path (str): Path to the database folder.\n\n        Returns:\n\n        \"\"\"\n        if os.path.exists(db_path):\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Warning: database folder will be rewritten.\")\n            shutil.rmtree(db_path)\n        os.mkdir(db_path)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Database building...\", file=sys.stdout)\n        proteomes.save_as_db(db_path)\n        hotspots.save_as_db(db_path)\n        database_info_txt = f\"Date and time of building: {time.strftime('%Y.%m.%d-%H:%M')}\\n\" \\\n                            f\"iLund4u version: {self.prms.args['version']}\"\n        with open(os.path.join(db_path, \"db_info.txt\"), \"w\") as db_info:\n            db_info.write(database_info_txt)\n        with open(os.path.join(db_path, \"parameters.json\"), \"w\") as parameters:\n            json.dump(self.prms.args, parameters)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Database was successfully saved to {db_path}\", file=sys.stdout)\n        return None\n\n    def load_database(self, db_path: str) -&gt; ilund4u.data_processing.Database:\n        \"\"\"Load database from its folder path and create a Database class object.\n\n        Arguments:\n            db_path (str): Path to the pre-built database folder.\n\n        Returns:\n            ilund4u.data_processing.Database: Database class object.\n\n        \"\"\"\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Loading database from {db_path}...\", file=sys.stdout)\n        proteomes = ilund4u.data_processing.Proteomes.db_init(db_path, self.prms)\n        hotspots = ilund4u.data_processing.Hotspots.db_init(db_path, proteomes, self.prms)\n        db_paths = dict(db_path=db_path, rep_fasta=os.path.join(db_path, \"representative_seqs.fa\"),\n                        proteins_db=os.path.join(db_path, \"mmseqs_db\", \"all_proteins\"))\n        if os.path.exists(os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")):\n            db_paths[\"protein_group_stat\"] = os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")\n\n        database = ilund4u.data_processing.Database(proteomes, hotspots, db_paths, self.prms)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf The {db_path} database was successfully loaded\", file=sys.stdout)\n        return database\n</code></pre>"},{"location":"API/data_manager/#ilund4u.data_manager.DatabaseManager.__init__","title":"<code>__init__(parameters)</code>","text":"<p>DatabaseManager class constructor.</p> <p>Parameters:</p> <ul> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def __init__(self, parameters: ilund4u.manager.Parameters):\n    \"\"\"DatabaseManager class constructor.\n\n    Arguments:\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.prms = parameters\n</code></pre>"},{"location":"API/data_manager/#ilund4u.data_manager.DatabaseManager.build_database","title":"<code>build_database(proteomes, hotspots, db_path)</code>","text":"<p>Write database.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>Path to the database folder.</p> </li> </ul> <p>Returns:</p> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def build_database(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                   db_path: str) -&gt; None:\n    \"\"\"Write database.\n\n    Arguments:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        db_path (str): Path to the database folder.\n\n    Returns:\n\n    \"\"\"\n    if os.path.exists(db_path):\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Warning: database folder will be rewritten.\")\n        shutil.rmtree(db_path)\n    os.mkdir(db_path)\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u25cb Database building...\", file=sys.stdout)\n    proteomes.save_as_db(db_path)\n    hotspots.save_as_db(db_path)\n    database_info_txt = f\"Date and time of building: {time.strftime('%Y.%m.%d-%H:%M')}\\n\" \\\n                        f\"iLund4u version: {self.prms.args['version']}\"\n    with open(os.path.join(db_path, \"db_info.txt\"), \"w\") as db_info:\n        db_info.write(database_info_txt)\n    with open(os.path.join(db_path, \"parameters.json\"), \"w\") as parameters:\n        json.dump(self.prms.args, parameters)\n    if self.prms.args[\"verbose\"]:\n        print(f\"  \u29bf Database was successfully saved to {db_path}\", file=sys.stdout)\n    return None\n</code></pre>"},{"location":"API/data_manager/#ilund4u.data_manager.DatabaseManager.load_database","title":"<code>load_database(db_path)</code>","text":"<p>Load database from its folder path and create a Database class object.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>Path to the pre-built database folder.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Database</code>         \u2013          <p>ilund4u.data_processing.Database: Database class object.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def load_database(self, db_path: str) -&gt; ilund4u.data_processing.Database:\n    \"\"\"Load database from its folder path and create a Database class object.\n\n    Arguments:\n        db_path (str): Path to the pre-built database folder.\n\n    Returns:\n        ilund4u.data_processing.Database: Database class object.\n\n    \"\"\"\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u25cb Loading database from {db_path}...\", file=sys.stdout)\n    proteomes = ilund4u.data_processing.Proteomes.db_init(db_path, self.prms)\n    hotspots = ilund4u.data_processing.Hotspots.db_init(db_path, proteomes, self.prms)\n    db_paths = dict(db_path=db_path, rep_fasta=os.path.join(db_path, \"representative_seqs.fa\"),\n                    proteins_db=os.path.join(db_path, \"mmseqs_db\", \"all_proteins\"))\n    if os.path.exists(os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")):\n        db_paths[\"protein_group_stat\"] = os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")\n\n    database = ilund4u.data_processing.Database(proteomes, hotspots, db_paths, self.prms)\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u29bf The {db_path} database was successfully loaded\", file=sys.stdout)\n    return database\n</code></pre>"},{"location":"API/data_processing/","title":"Data processing","text":"<p>This module contains all key data processing classes and methods of the tool.</p>"},{"location":"API/data_processing/#ilund4u.data_processing.CDS","title":"<code>CDS</code>","text":"<p>CDS object represents an annotated protein.</p> <p>Attributes:</p> <ul> <li> <code>cds_id</code>             (<code>str</code>)         \u2013          <p>CDS identifier.</p> </li> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where CDS is encoded.</p> </li> <li> <code>start</code>             (<code>int</code>)         \u2013          <p>1-based start genomic coordinate.</p> </li> <li> <code>end</code>             (<code>int</code>)         \u2013          <p>1-based end genomic coordinates.</p> </li> <li> <code>length</code>             (<code>int</code>)         \u2013          <p>length of the CDS.</p> </li> <li> <code>strand</code>             (<code>int</code>)         \u2013          <p>Genomic strand (1: plus strand, -1: minus strand).</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the feature which will be used as a label.</p> </li> <li> <code>group</code>             (<code>str</code>)         \u2013          <p>CDS group that represents a set of homologous proteins.</p> </li> <li> <code>g_class</code>             (<code>str</code>)         \u2013          <p>Class of CDS group (variable, intermediate, conserved).</p> </li> <li> <code>hmmscan_results</code>             (<code>dict</code>)         \u2013          <p>Results of pyhmmer hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class CDS:\n    \"\"\"CDS object represents an annotated protein.\n\n    Attributes:\n        cds_id (str): CDS identifier.\n        proteome_id (str): Proteome identifier where CDS is encoded.\n        start (int): 1-based start genomic coordinate.\n        end (int): 1-based end genomic coordinates.\n        length (int): length of the CDS.\n        strand (int): Genomic strand (1: plus strand, -1: minus strand).\n        name (str): Name of the feature which will be used as a label.\n        group (str): CDS group that represents a set of homologous proteins.\n        g_class (str): Class of CDS group (variable, intermediate, conserved).\n        hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n    \"\"\"\n\n    def __init__(self, cds_id: str, proteome_id: str, start: int, end: int, strand: int, name: str,\n                 group: typing.Union[None, str] = None, g_class: typing.Union[None, str] = None,\n                 hmmscan_results: typing.Union[None, dict] = None):\n        \"\"\"CDS class constructor.\n\n        Arguments:\n            cds_id (str): CDS identifier.\n            proteome_id (str): Proteome identifier where CDS is encoded.\n            start (int): 1-based start genomic coordinate.\n            end (int): 1-based end genomic coordinates.\n            strand (int): Genomic strand (1: plus strand, -1: minus strand).\n            name (str): Name of the feature which will be used as a label.\n            group (str): CDS group that represents a set of homologous proteins.\n            g_class (str): Class of CDS group (variable, intermediate, conserved).\n            hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n        \"\"\"\n        self.cds_id = cds_id\n        self.proteome_id = proteome_id\n        self.start = start\n        self.end = end\n        self.length = int((end - start + 1) / 3)\n        self.strand = strand\n        self.name = name\n        self.group = group\n        self.g_class = g_class\n        self.hmmscan_results = hmmscan_results\n\n    def get_cds_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"length\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.CDS.__init__","title":"<code>__init__(cds_id, proteome_id, start, end, strand, name, group=None, g_class=None, hmmscan_results=None)</code>","text":"<p>CDS class constructor.</p> <p>Parameters:</p> <ul> <li> <code>cds_id</code>             (<code>str</code>)         \u2013          <p>CDS identifier.</p> </li> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where CDS is encoded.</p> </li> <li> <code>start</code>             (<code>int</code>)         \u2013          <p>1-based start genomic coordinate.</p> </li> <li> <code>end</code>             (<code>int</code>)         \u2013          <p>1-based end genomic coordinates.</p> </li> <li> <code>strand</code>             (<code>int</code>)         \u2013          <p>Genomic strand (1: plus strand, -1: minus strand).</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the feature which will be used as a label.</p> </li> <li> <code>group</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>CDS group that represents a set of homologous proteins.</p> </li> <li> <code>g_class</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Class of CDS group (variable, intermediate, conserved).</p> </li> <li> <code>hmmscan_results</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Results of pyhmmer hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, cds_id: str, proteome_id: str, start: int, end: int, strand: int, name: str,\n             group: typing.Union[None, str] = None, g_class: typing.Union[None, str] = None,\n             hmmscan_results: typing.Union[None, dict] = None):\n    \"\"\"CDS class constructor.\n\n    Arguments:\n        cds_id (str): CDS identifier.\n        proteome_id (str): Proteome identifier where CDS is encoded.\n        start (int): 1-based start genomic coordinate.\n        end (int): 1-based end genomic coordinates.\n        strand (int): Genomic strand (1: plus strand, -1: minus strand).\n        name (str): Name of the feature which will be used as a label.\n        group (str): CDS group that represents a set of homologous proteins.\n        g_class (str): Class of CDS group (variable, intermediate, conserved).\n        hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n    \"\"\"\n    self.cds_id = cds_id\n    self.proteome_id = proteome_id\n    self.start = start\n    self.end = end\n    self.length = int((end - start + 1) / 3)\n    self.strand = strand\n    self.name = name\n    self.group = group\n    self.g_class = g_class\n    self.hmmscan_results = hmmscan_results\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.CDS.get_cds_db_row","title":"<code>get_cds_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_cds_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"length\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Database","title":"<code>Database</code>","text":"<p>Database object represents iLund4u database with proteomes and hotspots objects.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Database proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Database hotspots object.</p> </li> <li> <code>db_paths</code>             (<code>dict</code>)         \u2013          <p>Dictionary of database paths.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Database:\n    \"\"\"Database object represents iLund4u database with proteomes and hotspots objects.\n\n    Attributes:\n        proteomes (Proteomes): Database proteomes object.\n        hotspots (Hotspots): Database hotspots object.\n        db_paths (dict): Dictionary of database paths.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, proteomes: Proteomes, hotspots: Hotspots, db_paths: dict,\n                 parameters: ilund4u.manager.Parameters):\n        \"\"\"Database class constructor.\n\n        Args:\n            proteomes (Proteomes): Database proteomes object.\n            hotspots (Hotspots): Database hotspots object.\n            db_paths (dict): Dictionary of database paths.\n            prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = proteomes\n        self.hotspots = hotspots\n        self.db_paths = db_paths\n        self.prms = parameters\n\n    def mmseqs_search_versus_protein_database(self, query_fasta: str, fast=False) -&gt; pd.DataFrame:\n        \"\"\"Run mmseqs search versus protein database.\n\n        Arguments:\n            query_fasta (str): path to a query fasta file with protein sequence(s).\n            fast (bool): if true, then search will be performed only against representative sequences.\n\n        Returns:\n            pd.DataFrame: mmseqs search results table.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Running mmseqs for protein search versus the {'representative' if fast else 'full'}\"\n                      f\" database of proteins...\",\n                      file=sys.stdout)\n            if not os.path.exists(self.prms.args[\"output_dir\"]):\n                os.mkdir(self.prms.args[\"output_dir\"])\n            mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n            if os.path.exists(mmseqs_output_folder):\n                shutil.rmtree(mmseqs_output_folder)\n            os.mkdir(mmseqs_output_folder)\n            mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DBs\")\n            os.mkdir(mmseqs_output_folder_db)\n            mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n            mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n            query_length = len(list(Bio.SeqIO.parse(query_fasta, \"fasta\")))\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", query_fasta,\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\")], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)\n            target_db = self.db_paths[\"proteins_db\"]\n            if fast:\n                if not os.path.exists(os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")):\n                    subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", self.db_paths[\"rep_fasta\"],\n                                    os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")],\n                                   stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n                target_db = os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"search\",\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\"), target_db,\n                            os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                            os.path.join(mmseqs_output_folder, \"tmp\"), \"-e\",\n                            str(self.prms.args[\"mmseqs_search_evalue\"]),\n                            \"-s\", str(self.prms.args[\"mmseqs_search_s\"])], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"convertalis\",\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\"),\n                            self.db_paths[\"proteins_db\"],\n                            os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                            os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"), \"--format-output\",\n                            \"query,target,qlen,tlen,alnlen,fident,qstart,qend,tstart,tend,evalue\",\n                            \"--format-mode\", \"4\"], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            mmseqs_search_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"),\n                                                  sep=\"\\t\")\n            mmseqs_search_results[\"qcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"qlen\"], axis=1)\n            mmseqs_search_results[\"tcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"tlen\"], axis=1)\n            mmseqs_search_results = mmseqs_search_results[\n                (mmseqs_search_results[\"qcov\"] &gt;= self.prms.args[\"mmseqs_search_qcov\"]) &amp;\n                (mmseqs_search_results[\"tcov\"] &gt;= self.prms.args[\"mmseqs_search_tcov\"]) &amp;\n                (mmseqs_search_results[\"fident\"] &gt;= self.prms.args[\"mmseqs_search_fident\"])]\n            queries_with_res = len(set(mmseqs_search_results[\"query\"].to_list()))\n            target_to_group = dict()\n            for proteome in self.proteomes.proteomes.to_list():\n                for cds in proteome.cdss.to_list():\n                    target_to_group[cds.cds_id] = cds.group\n            mmseqs_search_results[\"group\"] = mmseqs_search_results[\"target\"].apply(lambda t: target_to_group[t])\n            mmseqs_search_results.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"mmseqs_homology_search_full.tsv\"),\n                                         sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                if queries_with_res &gt; 0:\n                    print(f\"  \u29bf A homologous group was found for {queries_with_res}/{query_length} query protein\"\n                          f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n                else:\n                    print(f\"  \u29bf No homologous group was found for {query_length} query protein\"\n                          f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n            return mmseqs_search_results\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs search versus protein database.\") from error\n\n    def protein_search_mode(self, query_fasta: str, query_label: typing.Union[None, str] = None,\n                            predefined_protein_group: typing.Union[None, str] = None) -&gt; None:\n        \"\"\"Run protein search mode which finds homologues of your query proteins in the database and returns\n            comprehensive output including visualisation and hotspot annotation.\n\n        Arguments:\n            query_fasta (str): Fasta with query protein sequence.\n            query_label (str): Label to be shown on lovis4u visualisation.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Load fasta\n            if not predefined_protein_group:\n                query_records = list(Bio.SeqIO.parse(query_fasta, \"fasta\"))\n                if len(query_records) &gt; 1:\n                    raise ilund4u.manager.ilund4uError(\"Only single query protein is allowed for protein mode\")\n                query_record = query_records[0]\n                # Run mmseqs for homology search\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\" and self.prms.args[\n                    \"fast_mmseqs_search_mode\"]:\n                    print(\"\u25cb Fast mode is not available with 'proteins' search modea and was deactivated.\",\n                          file=sys.stdout)\n                    self.prms.args[\"fast_mmseqs_search_mode\"] = False\n                mmseqs_results = self.mmseqs_search_versus_protein_database(query_fasta,\n                                                                            self.prms.args[\"fast_mmseqs_search_mode\"])\n                if len(mmseqs_results.index) == 0:\n                    print(\"\u25cb Termination since no homology to hotspot db proteins was found\", file=sys.stdout)\n                    return None\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                    homologous_protein_ids = mmseqs_results[\"target\"].to_list()\n                    homologous_groups = mmseqs_results[\"group\"].to_list()\n                elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                    mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"],\n                                               ascending=[True, False, False, False], inplace=True)\n                    mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n                    homologous_protein_ids = []\n                    homologous_group = mmseqs_results.at[query_record.id, \"group\"]\n                    homologous_groups = [homologous_group]\n            else:\n                homologous_group = predefined_protein_group\n                homologous_groups = [homologous_group]\n                homologous_protein_ids = []\n                self.prms.args[\"protein_search_target_mode\"] = \"group\"\n            if \"protein_group_stat\" in self.db_paths.keys():\n                protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                    \"representative_protein\")\n                groups_to_select = list(protein_group_stat_table.index.intersection(homologous_groups))\n                if groups_to_select:\n                    protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                    protein_group_stat_table.to_csv(\n                        os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                        sep=\"\\t\", index=True, index_label=\"representative_protein\")\n\n            # Searching for hotspots\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Searching for hotspots with your query protein homologues...\", file=sys.stdout)\n            found_hotspots = collections.defaultdict(list)\n            island_annotations = []\n            location_stat = dict(flanking=0, cargo=0)\n            n_flanked = 0\n            for hotspot in self.hotspots.hotspots.to_list():\n                if not self.prms.args[\"report_not_flanked\"] and not hotspot.flanked:\n                    continue\n                for island in hotspot.islands:\n                    proteome = self.proteomes.proteomes.at[island.proteome]\n                    if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                        locus_proteins = island.get_locus_proteins(proteome.cdss)\n                        overlapping = list(set(homologous_protein_ids) &amp; set(locus_proteins))\n                    elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                        locus_groups = island.get_locus_groups(proteome.cdss)\n                        overlapping = homologous_group in locus_groups\n                    if overlapping:\n                        if self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                            homologous_protein_ids_island = []\n                            locus_proteins = island.get_locus_proteins(proteome.cdss)\n                            for lp, lpg in zip(locus_proteins, locus_groups):\n                                if lpg == homologous_group:\n                                    homologous_protein_ids_island.append(lp)\n                                    homologous_protein_ids.append(lp)\n                            overlapping = homologous_protein_ids_island\n                        isl_groups = island.get_island_groups(proteome.cdss)\n                        isl_proteins = island.get_island_proteins(proteome.cdss)\n                        for op in overlapping:\n                            if op in isl_proteins:\n                                location_stat[\"cargo\"] += 1\n                                n_flanked += 1\n                            else:\n                                location_stat[\"flanking\"] += 1\n                        island_annotation = hotspot.island_annotation.loc[island.island_id].copy()\n                        island_annotation.drop(labels=[\"island_index\", \"strength\", \"degree\"], inplace=True)\n                        island_annotation.at[\"indexes\"] = \",\".join(map(str, island.indexes))\n                        island_annotation.at[\"size\"] = island.size\n                        island_annotation.at[\"island_proteins\"] = \",\".join(island.get_island_proteins(proteome.cdss))\n                        island_annotation.at[\"island_protein_groups\"] = \",\".join(isl_groups)\n                        island_annotation.at[\"query_homologues\"] = \",\".join(overlapping)\n                        island_annotations.append(island_annotation)\n                        found_hotspots[hotspot.hotspot_id].append(island)\n            if sum(location_stat.values()) == 0:\n                print(\"\u25cb Termination since no homologous protein was found in hotspots (neither as flanking or cargo)\",\n                      file=sys.stdout)\n                return None\n            found_islands = [island.island_id for islands in found_hotspots.values() for island in islands]\n            island_annotations = pd.DataFrame(island_annotations)\n            island_annotations.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                                \"found_island_annotation.tsv\")),\n                                      sep=\"\\t\", index_label=\"island_id\")\n            found_hotspots_annotation = self.hotspots.annotation.loc[found_hotspots.keys()]\n            found_hotspots_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"found_hotspot_annotation.tsv\"),\n                                             sep=\"\\t\", index_label=\"hotspot_id\")\n            found_hotspot_communities = list(set(found_hotspots_annotation[\"hotspot_community\"].to_list()))\n            # Get hotspot community stat\n            hotspot_community_annot_rows = []\n            r_types = [\"cargo\", \"flanking\"]\n            for h_com, hotspot_ids in self.hotspots.communities.items():\n                if h_com not in found_hotspot_communities:\n                    continue\n                h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n                hotspot_com_groups = dict(cargo=set(), flanking=set())\n                hotspots = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n                n_islands, n_flanked = 0, 0\n                for hotspot in hotspots:\n                    n_islands += hotspot.size\n                    n_flanked += hotspot.flanked\n                    hotspot_groups = hotspot.get_hotspot_groups(self.proteomes)\n                    db_stat = hotspot.calculate_database_hits_stats(self.proteomes, self.prms, protein_mode=True)\n                    for r_type in r_types:\n                        hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        for r_type in r_types:\n                            h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                    N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                    pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n                hotspot_community_annot_rows.append(hc_annot_row)\n            hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n            for db_name in self.prms.args[\"databases_classes\"]:\n                hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                    hotspot_community_annot.apply(\n                        lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n            hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                        \"found_hotspot_community_annotation.tsv\"),\n                                           sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Query protein homologues were found in {len(found_hotspot_communities)} hotspot \"\n                      f\"communit{'y' if len(found_hotspot_communities) == 1 else 'ies'} \"\n                      f\"({len(found_hotspots.keys())} hotspot{'s' if len(found_hotspots.keys()) &gt; 1 else ''}) on \"\n                      f\"{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''}\\n\"\n                      f\"    Found as cargo: {location_stat['cargo']}, as flanking gene: {location_stat['flanking']}\"\n                      f\"\\n    {n_flanked}/{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''} where found\"\n                      f\" as cargo are both side flanked (have conserved genes on both sides)\",\n                      file=sys.stdout)\n\n            homologous_protein_fasta = os.path.join(self.prms.args[\"output_dir\"], \"homologous_proteins.fa\")\n            full_fasta_file = Bio.SeqIO.index(self.proteomes.proteins_fasta_file, \"fasta\")\n            with open(homologous_protein_fasta, \"w\") as out_handle:\n                if not predefined_protein_group:\n                    Bio.SeqIO.write(query_record, out_handle, \"fasta\")\n                for acc in homologous_protein_ids:\n                    out_handle.write(full_fasta_file.get_raw(acc).decode())\n            # MSA visualisation\n            if len(homologous_protein_ids) &gt; 1 or not predefined_protein_group:\n                msa4u_p = msa4u.manager.Parameters()\n                msa4u_p.arguments[\"label\"] = \"id\"\n                msa4u_p.arguments[\"verbose\"] = False\n                msa4u_p.arguments[\"output_filename\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                    \"msa4u_homologous_proteines.pdf\")\n                msa4u_p.arguments[\"output_filename_aln\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                        \"homologous_proteins_aln.fa\")\n                fasta = msa4u.manager.Fasta(fasta=homologous_protein_fasta, parameters=msa4u_p)\n                mafft_output = fasta.run_mafft()\n                msa = msa4u.manager.MSA(mafft_output, msa4u_p)\n                msa.plot()\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf Homologous proteins were saved to {homologous_protein_fasta} and the MSA was \"\n                          f\"visualised with MSA4u\")\n            print(f\"\u25cb Visualisation of the hotspot(s) with your query protein homologues using lovis4u...\",\n                  file=sys.stdout)\n            # lovis4u visualisation\n            vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                  os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_with_query\")]\n            for vis_output_folder in vis_output_folders:\n                if os.path.exists(vis_output_folder):\n                    shutil.rmtree(vis_output_folder)\n                os.mkdir(vis_output_folder)\n            additional_annotation = dict()\n            for hpid in homologous_protein_ids:\n                additional_annotation[hpid] = dict(stroke_colour=\"#000000\", fill_colour=\"#000000\")\n                if query_label:\n                    additional_annotation[hpid][\"name\"] = query_label\n            drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n            for community in found_hotspot_communities:\n                drawing_manager.plot_hotspots(self.hotspots.communities[community],\n                                              output_folder=os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                              additional_annotation=additional_annotation)\n            drawing_manager.plot_hotspots(list(found_hotspots.keys()),\n                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                     \"lovis4u_with_query\"),\n                                          island_ids=found_islands,\n                                          additional_annotation=additional_annotation)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Done!\")\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to perform protein search versus the database.\") from error\n\n    def proteome_annotation_mode(self, query_gff: str) -&gt; None:\n        \"\"\"Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and\n            variable proteins in the query proteome in case a community with similar proteomes was found in the database.\n\n        Arguments:f\n            query_gff (str): GFF with query proteome.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Load query gff\n            proteomes_helper_obj = ilund4u.data_processing.Proteomes(parameters=self.prms)\n            proteomes_helper_obj.load_sequences_from_extended_gff(input_f=[query_gff])\n            query_proteome = proteomes_helper_obj.proteomes.iat[0]\n            # Get and parse mmseqs search results\n            mmseqs_results = self.mmseqs_search_versus_protein_database(proteomes_helper_obj.proteins_fasta_file,\n                                                                        self.prms.args[\"fast_mmseqs_search_mode\"])\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Searching for similar proteomes in the database network\", file=sys.stdout)\n            mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"], ascending=[True, False, False, False],\n                                       inplace=True)\n            mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n            proteins_wo_hits = []\n            for cds in query_proteome.cdss.to_list():\n                if cds.cds_id in mmseqs_results.index:\n                    cds.group = mmseqs_results.at[cds.cds_id, \"group\"]\n                else:\n                    cds.group = f\"{cds.cds_id}\"\n                    proteins_wo_hits.append(cds.group)\n            if \"protein_group_stat\" in self.db_paths.keys():\n                protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                    \"representative_protein\")\n                groups_to_select = list(protein_group_stat_table.index.intersection(mmseqs_results[\"group\"].tolist()))\n                if groups_to_select:\n                    protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                    protein_group_stat_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                                                    sep=\"\\t\", index=True, index_label=\"representative_protein\")\n            # Running pyhmmer annotation\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Preparing data for protein annotation with pyhmmer hmmscan...\", file=sys.stdout)\n            alignment_table = ilund4u.methods.run_pyhmmer(proteomes_helper_obj.proteins_fasta_file,\n                                                          len(query_proteome.cdss.index), self.prms)\n            if not alignment_table.empty:\n                found_hits_for = alignment_table.index.to_list()\n                proteome_cdss = query_proteome.cdss.to_list()\n                proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.cds_id in found_hits_for]\n                if proteome_cdss_with_hits:\n                    cdss_with_hits = query_proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                    for cds in cdss_with_hits:\n                        alignment_table_row = alignment_table.loc[cds.cds_id]\n                        cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                   db_name=alignment_table_row[\"target_db\"],\n                                                   target=alignment_table_row[\"target\"],\n                                                   evalue=alignment_table_row[\"hit_evalue\"])\n            # Connect to the database proteome network\n            proteome_names = pd.Series({idx: sid for idx, sid in enumerate(self.proteomes.annotation.index)})\n            proteome_sizes = self.proteomes.annotation[[\"proteome_size_unique\", \"index\"]]\n            proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n            cluster_to_sequences = collections.defaultdict(list)\n            for p_index, proteome in enumerate(self.proteomes.proteomes.to_list()):\n                cds_groups = set(proteome.cdss.apply(lambda cds: cds.group).to_list())\n                for cds_g in cds_groups:\n                    cluster_to_sequences[cds_g].append(proteome.proteome_id)\n            cluster_to_proteome_index = dict()\n            for cluster, sequences in cluster_to_sequences.items():\n                indexes = sorted([self.proteomes.seq_to_ind[seq_id] for seq_id in sequences])\n                cluster_to_proteome_index[cluster] = indexes\n            query_clusters = set(query_proteome.cdss.apply(lambda cds: cds.group).to_list())\n            query_size = len(query_clusters)\n            counts = collections.defaultdict(int)\n            for cl in query_clusters:\n                if cl not in proteins_wo_hits:\n                    js = cluster_to_proteome_index[cl]\n                    for j in js:\n                        counts[j] += 1\n            weights = pd.Series(counts)\n            proteome_sizes_connected = proteome_sizes.iloc[weights.index]\n            norm_factor = pd.Series(\n                0.5 * (query_size + proteome_sizes_connected) / (query_size * proteome_sizes_connected), \\\n                index=weights.index)\n            weights = weights.mul(norm_factor)\n            weights = weights[weights &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n            query_network_df = pd.DataFrame(dict(weight=weights, seq_id=proteome_names.iloc[weights.index]))\n            query_network_df.sort_values(by=\"weight\", inplace=True, ascending=False)\n            query_network_df.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_proteome_network.tsv\"),\n                                    sep=\"\\t\", index_label=\"t_index\")\n            query_network_df = query_network_df.set_index(\"seq_id\")\n            max_weight = round(query_network_df[\"weight\"].max(), 2)\n            if len(weights.index) == 0:\n                print(\"\u25cb Termination since no similar proteome was found in the database\", file=sys.stdout)\n                sys.exit()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(weights.index)} similar proteomes were found in the database network with \"\n                      f\"max proteome similarity = {max_weight}\", file=sys.stdout)\n            similar_proteoms_ids = query_network_df.index.to_list()\n            # Assign the closest community\n            similar_communities_rows = list()\n            for pcom_id, pcom_pr_ids in self.proteomes.communities.items():\n                com_size = len(pcom_pr_ids)\n                overlapping = list(set(pcom_pr_ids) &amp; set(similar_proteoms_ids))\n                if overlapping:\n                    weights_subset = query_network_df.loc[overlapping][\"weight\"].to_list()\n                    similar_communities_rows.append(dict(com_id=pcom_id, com_size=com_size,\n                                                         connection_fr=len(overlapping) / com_size,\n                                                         avg_weight=np.mean(weights_subset)))\n            similar_communities = pd.DataFrame(similar_communities_rows)\n            similar_communities.sort_values(by=[\"avg_weight\", \"connection_fr\", \"com_size\"], inplace=True,\n                                            ascending=[False, False, False])\n            similar_communities.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"similar_proteome_communities.tsv\"),\n                                       sep=\"\\t\", index=False)\n            selected_community_dict = similar_communities.iloc[0].to_dict()\n            selected_community_dict[\"com_id\"] = int(selected_community_dict[\"com_id\"])\n            query_proteome_community = selected_community_dict[\"com_id\"]\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf The query proteome was assigned to a community (id: {int(selected_community_dict['com_id'])})\"\n                      f\" with connection to {round(selected_community_dict['connection_fr'], 2)} of its members with \"\n                      f\"avg weight = {round(selected_community_dict['avg_weight'], 2)}\", file=sys.stdout)\n            # Define cds classes\n            com_protein_classes = dict()\n            for com_proteome in self.proteomes.communities[selected_community_dict[\"com_id\"]]:\n                for cds in self.proteomes.proteomes.at[com_proteome].cdss:\n                    com_protein_classes[cds.group] = cds.g_class\n            defined_classes_rows = []\n            for cds in query_proteome.cdss:\n                if cds.group in com_protein_classes.keys():\n                    cds.g_class = com_protein_classes[cds.group]\n                else:\n                    cds.g_class = \"variable\"\n                defined_classes_rows.append(dict(cds_id=cds.cds_id, cds_class=cds.g_class))\n            defined_classes = pd.DataFrame(defined_classes_rows)\n            defined_classes.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_protein_clusters.tsv\"),\n                                   sep=\"\\t\", index=False)\n            defined_classes_c = collections.Counter(defined_classes[\"cds_class\"].to_list())\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Protein class distribution in query proteome: \"\n                      f\"{', '.join(f'{v} {k}' for k, v in defined_classes_c.items())}\"\n                      , file=sys.stdout)\n            # Annotate variable islands\n            query_proteome.annotate_variable_islands(self.prms)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(query_proteome.islands.index)} variable island\"\n                      f\"{'s were' if len(query_proteome.islands.index) &gt; 1 else ' was'} annotated in the \"\n                      f\"query proteome\", file=sys.stdout)\n            # Connect query proteome islands to the island network\n            community_hotspots = [h for h in self.hotspots.hotspots.to_list() if\n                                  h.proteome_community == query_proteome_community]\n            if not self.prms.args[\"report_not_flanked\"]:\n                community_hotspots_flanked = [h for h in community_hotspots if h.flanked == 1]\n                community_hotspots = community_hotspots_flanked\n            if community_hotspots:\n                hotspots_islands = [island for hotspot in community_hotspots for island in hotspot.islands]\n                island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(hotspots_islands)}\n                com_island_n_sizes = pd.Series()\n                com_neighbours = pd.Series()\n                cluster_to_island = collections.defaultdict(list)\n                for island in hotspots_islands:\n                    island_proteome = self.proteomes.proteomes.at[island.proteome]\n                    island_id = island.island_id\n                    island_index = island_id_to_index[island_id]\n                    conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(island_proteome.cdss))\n                    com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                    com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                    for cing in conserved_island_neighbours_groups:\n                        cluster_to_island[cing].append(island_index)\n                hotspot_hits_statistic_rows = []\n                for q_island in query_proteome.islands.to_list():\n                    q_isl_neighbours = list(set(q_island.get_cons_neighbours_groups(query_proteome.cdss)))\n                    q_isl_size = len(q_isl_neighbours)\n                    q_isl_counts = collections.defaultdict(int)\n                    for qin in q_isl_neighbours:\n                        js = cluster_to_island[qin]\n                        for j in js:\n                            q_isl_counts[j] += 1\n                    q_isl_weights = pd.Series(q_isl_counts)\n                    q_isl_connected_n_sizes = com_island_n_sizes.iloc[q_isl_weights.index]\n                    q_isl_norm_factors = pd.Series(\n                        0.5 * (q_isl_size + q_isl_connected_n_sizes) / (q_isl_size * q_isl_connected_n_sizes),\n                        index=q_isl_weights.index)\n                    q_isl_weights = q_isl_weights.mul(q_isl_norm_factors)\n                    q_isl_weights = q_isl_weights[\n                        q_isl_weights &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                    q_isl_similar_islands = q_isl_weights.index.to_list()\n                    similar_hotspots_rows = list()\n                    for c_hotspot in community_hotspots:\n                        hotspot_islands = c_hotspot.islands\n                        hotspot_islsnds_idx = [island_id_to_index[isl.island_id] for isl in hotspot_islands]\n                        overlapping = list(set(q_isl_similar_islands) &amp; set(hotspot_islsnds_idx))\n                        if overlapping:\n                            weights_subset = q_isl_weights.loc[overlapping].to_list()\n                            connection_fr = len(overlapping) / c_hotspot.size\n                            if connection_fr &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]:\n                                similar_hotspots_rows.append(dict(hotspot_id=c_hotspot.hotspot_id,\n                                                                  hotspot_size=c_hotspot.size,\n                                                                  connection_fr=connection_fr,\n                                                                  avg_weight=np.mean(weights_subset)))\n                    if similar_hotspots_rows:\n                        similar_hotspots_rows = pd.DataFrame(similar_hotspots_rows)\n                        similar_hotspots_rows.sort_values(by=[\"avg_weight\", \"connection_fr\", \"hotspot_size\"],\n                                                          inplace=True,\n                                                          ascending=[False, False, False])\n                        best_hit_for_q_isl = similar_hotspots_rows.iloc[0].to_dict()\n                        hotspot_hits_statistic_rows.append(dict(query_island_id=q_island.island_id,\n                                                                query_island_proteins=\",\".join(\n                                                                    q_island.get_island_proteins(query_proteome.cdss)),\n                                                                closest_hotspot=best_hit_for_q_isl[\"hotspot_id\"],\n                                                                closest_hotspot_size=best_hit_for_q_isl[\"hotspot_size\"],\n                                                                closest_hotspot_avg_weight=best_hit_for_q_isl[\n                                                                    \"avg_weight\"],\n                                                                closest_hotspot_connection_fr=best_hit_for_q_isl[\n                                                                    \"connection_fr\"]))\n                hotspot_hits_statistic = pd.DataFrame(hotspot_hits_statistic_rows)\n                hotspot_hits_statistic.sort_values(by=[\"closest_hotspot_avg_weight\", \"closest_hotspot_connection_fr\"],\n                                                   inplace=True, ascending=[False, False])\n                hotspot_hits_statistic.to_csv(\n                    os.path.join(self.prms.args[\"output_dir\"], \"island_to_hotspot_mapping.tsv\"),\n                    sep=\"\\t\", index=False)\n                hotspot_hits_statistic = hotspot_hits_statistic.drop_duplicates(subset=\"closest_hotspot\", keep=\"first\")\n                hotspot_hits_statistic.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"island_to_hotspot_mapping_deduplicated.tsv\"),\n                                              sep=\"\\t\", index=False)\n                subset_hotspot_annotation = self.hotspots.annotation.loc[hotspot_hits_statistic[\"closest_hotspot\"]]\n                subset_hotspot_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                              \"annotation_of_mapped_hotspots.tsv\"), sep=\"\\t\",\n                                                 index=False)\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf {len(hotspot_hits_statistic.index)} of annotated variable island\"\n                          f\"{'s were' if len(hotspot_hits_statistic.index) &gt; 1 else ' was'} mapped to \"\n                          f\"the database hotspots\", file=sys.stdout)\n            else:\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf The proteome community does not have any annotated hotspots\", file=sys.stdout)\n            # Update proteome and hotspots objects\n            self.proteomes.proteomes.at[query_proteome.proteome_id] = query_proteome\n            self.proteomes.annotation.loc[query_proteome.proteome_id] = proteomes_helper_obj.annotation.loc[\n                query_proteome.proteome_id]\n            self.proteomes.communities[query_proteome_community].append(query_proteome.proteome_id)\n            if community_hotspots:\n                for index, row in hotspot_hits_statistic.iterrows():\n                    hotspot_id = row[\"closest_hotspot\"]\n                    query_island_id = row[\"query_island_id\"]\n                    query_island = query_proteome.islands.at[query_island_id]\n                    closest_hotspot = self.hotspots.hotspots.at[hotspot_id]\n                    closest_hotspot.islands.append(query_island)\n            # Visualisation\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Lovis4u visualisation of communities and proteomes...\", file=sys.stdout)\n            drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"hotspot\",\n                                                    filename=\"lovis4u_proteome_community_hotspots.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"regular\",\n                                                    filename=\"lovis4u_proteome_community.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"regular\",\n                                                    proteome_ids=[query_proteome.proteome_id],\n                                                    filename=\"lovis4u_query_proteome_variable.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"hotspot\",\n                                                    proteome_ids=[query_proteome.proteome_id],\n                                                    filename=\"lovis4u_query_proteome_hotspot.pdf\")\n            if community_hotspots:\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u25cb Lovis4u visualisation found hotspots..\", file=sys.stdout)\n                if len(hotspot_hits_statistic.index) &gt; 0:\n                    vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_full\"),\n                                          os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_with_query\")]\n                    for vis_output_folder in vis_output_folders:\n                        if os.path.exists(vis_output_folder):\n                            shutil.rmtree(vis_output_folder)\n                        os.mkdir(vis_output_folder)\n                for index, row in hotspot_hits_statistic.iterrows():\n                    hotspot_id = row[\"closest_hotspot\"]\n                    query_island = row[\"query_island_id\"]\n                    for hc, c_hotspots in self.hotspots.communities.items():\n                        if hotspot_id in c_hotspots:\n                            drawing_manager.plot_hotspots([hotspot_id],\n                                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                     \"lovis4u_hotspot_with_query\"),\n                                                          island_ids=[query_island])\n                            drawing_manager.plot_hotspots(c_hotspots,\n                                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                     \"lovis4u_hotspot_full\"),\n                                                          keep_while_deduplication=[query_island])\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Done!\")\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run proteome annotation mode versus the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Database.__init__","title":"<code>__init__(proteomes, hotspots, db_paths, parameters)</code>","text":"<p>Database class constructor.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Database proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Database hotspots object.</p> </li> <li> <code>db_paths</code>             (<code>dict</code>)         \u2013          <p>Dictionary of database paths.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, proteomes: Proteomes, hotspots: Hotspots, db_paths: dict,\n             parameters: ilund4u.manager.Parameters):\n    \"\"\"Database class constructor.\n\n    Args:\n        proteomes (Proteomes): Database proteomes object.\n        hotspots (Hotspots): Database hotspots object.\n        db_paths (dict): Dictionary of database paths.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = proteomes\n    self.hotspots = hotspots\n    self.db_paths = db_paths\n    self.prms = parameters\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Database.mmseqs_search_versus_protein_database","title":"<code>mmseqs_search_versus_protein_database(query_fasta, fast=False)</code>","text":"<p>Run mmseqs search versus protein database.</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>path to a query fasta file with protein sequence(s).</p> </li> <li> <code>fast</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if true, then search will be performed only against representative sequences.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: mmseqs search results table.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def mmseqs_search_versus_protein_database(self, query_fasta: str, fast=False) -&gt; pd.DataFrame:\n    \"\"\"Run mmseqs search versus protein database.\n\n    Arguments:\n        query_fasta (str): path to a query fasta file with protein sequence(s).\n        fast (bool): if true, then search will be performed only against representative sequences.\n\n    Returns:\n        pd.DataFrame: mmseqs search results table.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Running mmseqs for protein search versus the {'representative' if fast else 'full'}\"\n                  f\" database of proteins...\",\n                  file=sys.stdout)\n        if not os.path.exists(self.prms.args[\"output_dir\"]):\n            os.mkdir(self.prms.args[\"output_dir\"])\n        mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n        if os.path.exists(mmseqs_output_folder):\n            shutil.rmtree(mmseqs_output_folder)\n        os.mkdir(mmseqs_output_folder)\n        mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DBs\")\n        os.mkdir(mmseqs_output_folder_db)\n        mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n        mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n        query_length = len(list(Bio.SeqIO.parse(query_fasta, \"fasta\")))\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", query_fasta,\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\")], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)\n        target_db = self.db_paths[\"proteins_db\"]\n        if fast:\n            if not os.path.exists(os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")):\n                subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", self.db_paths[\"rep_fasta\"],\n                                os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")],\n                               stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            target_db = os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"search\",\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\"), target_db,\n                        os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                        os.path.join(mmseqs_output_folder, \"tmp\"), \"-e\",\n                        str(self.prms.args[\"mmseqs_search_evalue\"]),\n                        \"-s\", str(self.prms.args[\"mmseqs_search_s\"])], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"convertalis\",\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\"),\n                        self.db_paths[\"proteins_db\"],\n                        os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                        os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"), \"--format-output\",\n                        \"query,target,qlen,tlen,alnlen,fident,qstart,qend,tstart,tend,evalue\",\n                        \"--format-mode\", \"4\"], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        mmseqs_search_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"),\n                                              sep=\"\\t\")\n        mmseqs_search_results[\"qcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"qlen\"], axis=1)\n        mmseqs_search_results[\"tcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"tlen\"], axis=1)\n        mmseqs_search_results = mmseqs_search_results[\n            (mmseqs_search_results[\"qcov\"] &gt;= self.prms.args[\"mmseqs_search_qcov\"]) &amp;\n            (mmseqs_search_results[\"tcov\"] &gt;= self.prms.args[\"mmseqs_search_tcov\"]) &amp;\n            (mmseqs_search_results[\"fident\"] &gt;= self.prms.args[\"mmseqs_search_fident\"])]\n        queries_with_res = len(set(mmseqs_search_results[\"query\"].to_list()))\n        target_to_group = dict()\n        for proteome in self.proteomes.proteomes.to_list():\n            for cds in proteome.cdss.to_list():\n                target_to_group[cds.cds_id] = cds.group\n        mmseqs_search_results[\"group\"] = mmseqs_search_results[\"target\"].apply(lambda t: target_to_group[t])\n        mmseqs_search_results.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"mmseqs_homology_search_full.tsv\"),\n                                     sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            if queries_with_res &gt; 0:\n                print(f\"  \u29bf A homologous group was found for {queries_with_res}/{query_length} query protein\"\n                      f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n            else:\n                print(f\"  \u29bf No homologous group was found for {query_length} query protein\"\n                      f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n        return mmseqs_search_results\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs search versus protein database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Database.protein_search_mode","title":"<code>protein_search_mode(query_fasta, query_label=None, predefined_protein_group=None)</code>","text":"<p>Run protein search mode which finds homologues of your query proteins in the database and returns     comprehensive output including visualisation and hotspot annotation.</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>Fasta with query protein sequence.</p> </li> <li> <code>query_label</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Label to be shown on lovis4u visualisation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def protein_search_mode(self, query_fasta: str, query_label: typing.Union[None, str] = None,\n                        predefined_protein_group: typing.Union[None, str] = None) -&gt; None:\n    \"\"\"Run protein search mode which finds homologues of your query proteins in the database and returns\n        comprehensive output including visualisation and hotspot annotation.\n\n    Arguments:\n        query_fasta (str): Fasta with query protein sequence.\n        query_label (str): Label to be shown on lovis4u visualisation.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Load fasta\n        if not predefined_protein_group:\n            query_records = list(Bio.SeqIO.parse(query_fasta, \"fasta\"))\n            if len(query_records) &gt; 1:\n                raise ilund4u.manager.ilund4uError(\"Only single query protein is allowed for protein mode\")\n            query_record = query_records[0]\n            # Run mmseqs for homology search\n            if self.prms.args[\"protein_search_target_mode\"] == \"proteins\" and self.prms.args[\n                \"fast_mmseqs_search_mode\"]:\n                print(\"\u25cb Fast mode is not available with 'proteins' search modea and was deactivated.\",\n                      file=sys.stdout)\n                self.prms.args[\"fast_mmseqs_search_mode\"] = False\n            mmseqs_results = self.mmseqs_search_versus_protein_database(query_fasta,\n                                                                        self.prms.args[\"fast_mmseqs_search_mode\"])\n            if len(mmseqs_results.index) == 0:\n                print(\"\u25cb Termination since no homology to hotspot db proteins was found\", file=sys.stdout)\n                return None\n            if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                homologous_protein_ids = mmseqs_results[\"target\"].to_list()\n                homologous_groups = mmseqs_results[\"group\"].to_list()\n            elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"],\n                                           ascending=[True, False, False, False], inplace=True)\n                mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n                homologous_protein_ids = []\n                homologous_group = mmseqs_results.at[query_record.id, \"group\"]\n                homologous_groups = [homologous_group]\n        else:\n            homologous_group = predefined_protein_group\n            homologous_groups = [homologous_group]\n            homologous_protein_ids = []\n            self.prms.args[\"protein_search_target_mode\"] = \"group\"\n        if \"protein_group_stat\" in self.db_paths.keys():\n            protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                \"representative_protein\")\n            groups_to_select = list(protein_group_stat_table.index.intersection(homologous_groups))\n            if groups_to_select:\n                protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                protein_group_stat_table.to_csv(\n                    os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                    sep=\"\\t\", index=True, index_label=\"representative_protein\")\n\n        # Searching for hotspots\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Searching for hotspots with your query protein homologues...\", file=sys.stdout)\n        found_hotspots = collections.defaultdict(list)\n        island_annotations = []\n        location_stat = dict(flanking=0, cargo=0)\n        n_flanked = 0\n        for hotspot in self.hotspots.hotspots.to_list():\n            if not self.prms.args[\"report_not_flanked\"] and not hotspot.flanked:\n                continue\n            for island in hotspot.islands:\n                proteome = self.proteomes.proteomes.at[island.proteome]\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                    locus_proteins = island.get_locus_proteins(proteome.cdss)\n                    overlapping = list(set(homologous_protein_ids) &amp; set(locus_proteins))\n                elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                    locus_groups = island.get_locus_groups(proteome.cdss)\n                    overlapping = homologous_group in locus_groups\n                if overlapping:\n                    if self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                        homologous_protein_ids_island = []\n                        locus_proteins = island.get_locus_proteins(proteome.cdss)\n                        for lp, lpg in zip(locus_proteins, locus_groups):\n                            if lpg == homologous_group:\n                                homologous_protein_ids_island.append(lp)\n                                homologous_protein_ids.append(lp)\n                        overlapping = homologous_protein_ids_island\n                    isl_groups = island.get_island_groups(proteome.cdss)\n                    isl_proteins = island.get_island_proteins(proteome.cdss)\n                    for op in overlapping:\n                        if op in isl_proteins:\n                            location_stat[\"cargo\"] += 1\n                            n_flanked += 1\n                        else:\n                            location_stat[\"flanking\"] += 1\n                    island_annotation = hotspot.island_annotation.loc[island.island_id].copy()\n                    island_annotation.drop(labels=[\"island_index\", \"strength\", \"degree\"], inplace=True)\n                    island_annotation.at[\"indexes\"] = \",\".join(map(str, island.indexes))\n                    island_annotation.at[\"size\"] = island.size\n                    island_annotation.at[\"island_proteins\"] = \",\".join(island.get_island_proteins(proteome.cdss))\n                    island_annotation.at[\"island_protein_groups\"] = \",\".join(isl_groups)\n                    island_annotation.at[\"query_homologues\"] = \",\".join(overlapping)\n                    island_annotations.append(island_annotation)\n                    found_hotspots[hotspot.hotspot_id].append(island)\n        if sum(location_stat.values()) == 0:\n            print(\"\u25cb Termination since no homologous protein was found in hotspots (neither as flanking or cargo)\",\n                  file=sys.stdout)\n            return None\n        found_islands = [island.island_id for islands in found_hotspots.values() for island in islands]\n        island_annotations = pd.DataFrame(island_annotations)\n        island_annotations.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                            \"found_island_annotation.tsv\")),\n                                  sep=\"\\t\", index_label=\"island_id\")\n        found_hotspots_annotation = self.hotspots.annotation.loc[found_hotspots.keys()]\n        found_hotspots_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"found_hotspot_annotation.tsv\"),\n                                         sep=\"\\t\", index_label=\"hotspot_id\")\n        found_hotspot_communities = list(set(found_hotspots_annotation[\"hotspot_community\"].to_list()))\n        # Get hotspot community stat\n        hotspot_community_annot_rows = []\n        r_types = [\"cargo\", \"flanking\"]\n        for h_com, hotspot_ids in self.hotspots.communities.items():\n            if h_com not in found_hotspot_communities:\n                continue\n            h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n            hotspot_com_groups = dict(cargo=set(), flanking=set())\n            hotspots = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n            n_islands, n_flanked = 0, 0\n            for hotspot in hotspots:\n                n_islands += hotspot.size\n                n_flanked += hotspot.flanked\n                hotspot_groups = hotspot.get_hotspot_groups(self.proteomes)\n                db_stat = hotspot.calculate_database_hits_stats(self.proteomes, self.prms, protein_mode=True)\n                for r_type in r_types:\n                    hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n            hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n            for r_type in r_types:\n                hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n            for db_name in self.prms.args[\"databases_classes\"]:\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n            hotspot_community_annot_rows.append(hc_annot_row)\n        hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n        for db_name in self.prms.args[\"databases_classes\"]:\n            hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                hotspot_community_annot.apply(\n                    lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n        hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                    \"found_hotspot_community_annotation.tsv\"),\n                                       sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Query protein homologues were found in {len(found_hotspot_communities)} hotspot \"\n                  f\"communit{'y' if len(found_hotspot_communities) == 1 else 'ies'} \"\n                  f\"({len(found_hotspots.keys())} hotspot{'s' if len(found_hotspots.keys()) &gt; 1 else ''}) on \"\n                  f\"{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''}\\n\"\n                  f\"    Found as cargo: {location_stat['cargo']}, as flanking gene: {location_stat['flanking']}\"\n                  f\"\\n    {n_flanked}/{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''} where found\"\n                  f\" as cargo are both side flanked (have conserved genes on both sides)\",\n                  file=sys.stdout)\n\n        homologous_protein_fasta = os.path.join(self.prms.args[\"output_dir\"], \"homologous_proteins.fa\")\n        full_fasta_file = Bio.SeqIO.index(self.proteomes.proteins_fasta_file, \"fasta\")\n        with open(homologous_protein_fasta, \"w\") as out_handle:\n            if not predefined_protein_group:\n                Bio.SeqIO.write(query_record, out_handle, \"fasta\")\n            for acc in homologous_protein_ids:\n                out_handle.write(full_fasta_file.get_raw(acc).decode())\n        # MSA visualisation\n        if len(homologous_protein_ids) &gt; 1 or not predefined_protein_group:\n            msa4u_p = msa4u.manager.Parameters()\n            msa4u_p.arguments[\"label\"] = \"id\"\n            msa4u_p.arguments[\"verbose\"] = False\n            msa4u_p.arguments[\"output_filename\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                \"msa4u_homologous_proteines.pdf\")\n            msa4u_p.arguments[\"output_filename_aln\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                    \"homologous_proteins_aln.fa\")\n            fasta = msa4u.manager.Fasta(fasta=homologous_protein_fasta, parameters=msa4u_p)\n            mafft_output = fasta.run_mafft()\n            msa = msa4u.manager.MSA(mafft_output, msa4u_p)\n            msa.plot()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Homologous proteins were saved to {homologous_protein_fasta} and the MSA was \"\n                      f\"visualised with MSA4u\")\n        print(f\"\u25cb Visualisation of the hotspot(s) with your query protein homologues using lovis4u...\",\n              file=sys.stdout)\n        # lovis4u visualisation\n        vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                              os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_with_query\")]\n        for vis_output_folder in vis_output_folders:\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n        additional_annotation = dict()\n        for hpid in homologous_protein_ids:\n            additional_annotation[hpid] = dict(stroke_colour=\"#000000\", fill_colour=\"#000000\")\n            if query_label:\n                additional_annotation[hpid][\"name\"] = query_label\n        drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n        for community in found_hotspot_communities:\n            drawing_manager.plot_hotspots(self.hotspots.communities[community],\n                                          output_folder=os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                          additional_annotation=additional_annotation)\n        drawing_manager.plot_hotspots(list(found_hotspots.keys()),\n                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                 \"lovis4u_with_query\"),\n                                      island_ids=found_islands,\n                                      additional_annotation=additional_annotation)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Done!\")\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to perform protein search versus the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Database.proteome_annotation_mode","title":"<code>proteome_annotation_mode(query_gff)</code>","text":"<p>Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and     variable proteins in the query proteome in case a community with similar proteomes was found in the database.</p> <p>Arguments:f     query_gff (str): GFF with query proteome.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def proteome_annotation_mode(self, query_gff: str) -&gt; None:\n    \"\"\"Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and\n        variable proteins in the query proteome in case a community with similar proteomes was found in the database.\n\n    Arguments:f\n        query_gff (str): GFF with query proteome.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Load query gff\n        proteomes_helper_obj = ilund4u.data_processing.Proteomes(parameters=self.prms)\n        proteomes_helper_obj.load_sequences_from_extended_gff(input_f=[query_gff])\n        query_proteome = proteomes_helper_obj.proteomes.iat[0]\n        # Get and parse mmseqs search results\n        mmseqs_results = self.mmseqs_search_versus_protein_database(proteomes_helper_obj.proteins_fasta_file,\n                                                                    self.prms.args[\"fast_mmseqs_search_mode\"])\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Searching for similar proteomes in the database network\", file=sys.stdout)\n        mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"], ascending=[True, False, False, False],\n                                   inplace=True)\n        mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n        proteins_wo_hits = []\n        for cds in query_proteome.cdss.to_list():\n            if cds.cds_id in mmseqs_results.index:\n                cds.group = mmseqs_results.at[cds.cds_id, \"group\"]\n            else:\n                cds.group = f\"{cds.cds_id}\"\n                proteins_wo_hits.append(cds.group)\n        if \"protein_group_stat\" in self.db_paths.keys():\n            protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                \"representative_protein\")\n            groups_to_select = list(protein_group_stat_table.index.intersection(mmseqs_results[\"group\"].tolist()))\n            if groups_to_select:\n                protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                protein_group_stat_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                                                sep=\"\\t\", index=True, index_label=\"representative_protein\")\n        # Running pyhmmer annotation\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Preparing data for protein annotation with pyhmmer hmmscan...\", file=sys.stdout)\n        alignment_table = ilund4u.methods.run_pyhmmer(proteomes_helper_obj.proteins_fasta_file,\n                                                      len(query_proteome.cdss.index), self.prms)\n        if not alignment_table.empty:\n            found_hits_for = alignment_table.index.to_list()\n            proteome_cdss = query_proteome.cdss.to_list()\n            proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.cds_id in found_hits_for]\n            if proteome_cdss_with_hits:\n                cdss_with_hits = query_proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                for cds in cdss_with_hits:\n                    alignment_table_row = alignment_table.loc[cds.cds_id]\n                    cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                               db_name=alignment_table_row[\"target_db\"],\n                                               target=alignment_table_row[\"target\"],\n                                               evalue=alignment_table_row[\"hit_evalue\"])\n        # Connect to the database proteome network\n        proteome_names = pd.Series({idx: sid for idx, sid in enumerate(self.proteomes.annotation.index)})\n        proteome_sizes = self.proteomes.annotation[[\"proteome_size_unique\", \"index\"]]\n        proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n        cluster_to_sequences = collections.defaultdict(list)\n        for p_index, proteome in enumerate(self.proteomes.proteomes.to_list()):\n            cds_groups = set(proteome.cdss.apply(lambda cds: cds.group).to_list())\n            for cds_g in cds_groups:\n                cluster_to_sequences[cds_g].append(proteome.proteome_id)\n        cluster_to_proteome_index = dict()\n        for cluster, sequences in cluster_to_sequences.items():\n            indexes = sorted([self.proteomes.seq_to_ind[seq_id] for seq_id in sequences])\n            cluster_to_proteome_index[cluster] = indexes\n        query_clusters = set(query_proteome.cdss.apply(lambda cds: cds.group).to_list())\n        query_size = len(query_clusters)\n        counts = collections.defaultdict(int)\n        for cl in query_clusters:\n            if cl not in proteins_wo_hits:\n                js = cluster_to_proteome_index[cl]\n                for j in js:\n                    counts[j] += 1\n        weights = pd.Series(counts)\n        proteome_sizes_connected = proteome_sizes.iloc[weights.index]\n        norm_factor = pd.Series(\n            0.5 * (query_size + proteome_sizes_connected) / (query_size * proteome_sizes_connected), \\\n            index=weights.index)\n        weights = weights.mul(norm_factor)\n        weights = weights[weights &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n        query_network_df = pd.DataFrame(dict(weight=weights, seq_id=proteome_names.iloc[weights.index]))\n        query_network_df.sort_values(by=\"weight\", inplace=True, ascending=False)\n        query_network_df.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_proteome_network.tsv\"),\n                                sep=\"\\t\", index_label=\"t_index\")\n        query_network_df = query_network_df.set_index(\"seq_id\")\n        max_weight = round(query_network_df[\"weight\"].max(), 2)\n        if len(weights.index) == 0:\n            print(\"\u25cb Termination since no similar proteome was found in the database\", file=sys.stdout)\n            sys.exit()\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf {len(weights.index)} similar proteomes were found in the database network with \"\n                  f\"max proteome similarity = {max_weight}\", file=sys.stdout)\n        similar_proteoms_ids = query_network_df.index.to_list()\n        # Assign the closest community\n        similar_communities_rows = list()\n        for pcom_id, pcom_pr_ids in self.proteomes.communities.items():\n            com_size = len(pcom_pr_ids)\n            overlapping = list(set(pcom_pr_ids) &amp; set(similar_proteoms_ids))\n            if overlapping:\n                weights_subset = query_network_df.loc[overlapping][\"weight\"].to_list()\n                similar_communities_rows.append(dict(com_id=pcom_id, com_size=com_size,\n                                                     connection_fr=len(overlapping) / com_size,\n                                                     avg_weight=np.mean(weights_subset)))\n        similar_communities = pd.DataFrame(similar_communities_rows)\n        similar_communities.sort_values(by=[\"avg_weight\", \"connection_fr\", \"com_size\"], inplace=True,\n                                        ascending=[False, False, False])\n        similar_communities.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"similar_proteome_communities.tsv\"),\n                                   sep=\"\\t\", index=False)\n        selected_community_dict = similar_communities.iloc[0].to_dict()\n        selected_community_dict[\"com_id\"] = int(selected_community_dict[\"com_id\"])\n        query_proteome_community = selected_community_dict[\"com_id\"]\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf The query proteome was assigned to a community (id: {int(selected_community_dict['com_id'])})\"\n                  f\" with connection to {round(selected_community_dict['connection_fr'], 2)} of its members with \"\n                  f\"avg weight = {round(selected_community_dict['avg_weight'], 2)}\", file=sys.stdout)\n        # Define cds classes\n        com_protein_classes = dict()\n        for com_proteome in self.proteomes.communities[selected_community_dict[\"com_id\"]]:\n            for cds in self.proteomes.proteomes.at[com_proteome].cdss:\n                com_protein_classes[cds.group] = cds.g_class\n        defined_classes_rows = []\n        for cds in query_proteome.cdss:\n            if cds.group in com_protein_classes.keys():\n                cds.g_class = com_protein_classes[cds.group]\n            else:\n                cds.g_class = \"variable\"\n            defined_classes_rows.append(dict(cds_id=cds.cds_id, cds_class=cds.g_class))\n        defined_classes = pd.DataFrame(defined_classes_rows)\n        defined_classes.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_protein_clusters.tsv\"),\n                               sep=\"\\t\", index=False)\n        defined_classes_c = collections.Counter(defined_classes[\"cds_class\"].to_list())\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Protein class distribution in query proteome: \"\n                  f\"{', '.join(f'{v} {k}' for k, v in defined_classes_c.items())}\"\n                  , file=sys.stdout)\n        # Annotate variable islands\n        query_proteome.annotate_variable_islands(self.prms)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf {len(query_proteome.islands.index)} variable island\"\n                  f\"{'s were' if len(query_proteome.islands.index) &gt; 1 else ' was'} annotated in the \"\n                  f\"query proteome\", file=sys.stdout)\n        # Connect query proteome islands to the island network\n        community_hotspots = [h for h in self.hotspots.hotspots.to_list() if\n                              h.proteome_community == query_proteome_community]\n        if not self.prms.args[\"report_not_flanked\"]:\n            community_hotspots_flanked = [h for h in community_hotspots if h.flanked == 1]\n            community_hotspots = community_hotspots_flanked\n        if community_hotspots:\n            hotspots_islands = [island for hotspot in community_hotspots for island in hotspot.islands]\n            island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(hotspots_islands)}\n            com_island_n_sizes = pd.Series()\n            com_neighbours = pd.Series()\n            cluster_to_island = collections.defaultdict(list)\n            for island in hotspots_islands:\n                island_proteome = self.proteomes.proteomes.at[island.proteome]\n                island_id = island.island_id\n                island_index = island_id_to_index[island_id]\n                conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(island_proteome.cdss))\n                com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                for cing in conserved_island_neighbours_groups:\n                    cluster_to_island[cing].append(island_index)\n            hotspot_hits_statistic_rows = []\n            for q_island in query_proteome.islands.to_list():\n                q_isl_neighbours = list(set(q_island.get_cons_neighbours_groups(query_proteome.cdss)))\n                q_isl_size = len(q_isl_neighbours)\n                q_isl_counts = collections.defaultdict(int)\n                for qin in q_isl_neighbours:\n                    js = cluster_to_island[qin]\n                    for j in js:\n                        q_isl_counts[j] += 1\n                q_isl_weights = pd.Series(q_isl_counts)\n                q_isl_connected_n_sizes = com_island_n_sizes.iloc[q_isl_weights.index]\n                q_isl_norm_factors = pd.Series(\n                    0.5 * (q_isl_size + q_isl_connected_n_sizes) / (q_isl_size * q_isl_connected_n_sizes),\n                    index=q_isl_weights.index)\n                q_isl_weights = q_isl_weights.mul(q_isl_norm_factors)\n                q_isl_weights = q_isl_weights[\n                    q_isl_weights &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                q_isl_similar_islands = q_isl_weights.index.to_list()\n                similar_hotspots_rows = list()\n                for c_hotspot in community_hotspots:\n                    hotspot_islands = c_hotspot.islands\n                    hotspot_islsnds_idx = [island_id_to_index[isl.island_id] for isl in hotspot_islands]\n                    overlapping = list(set(q_isl_similar_islands) &amp; set(hotspot_islsnds_idx))\n                    if overlapping:\n                        weights_subset = q_isl_weights.loc[overlapping].to_list()\n                        connection_fr = len(overlapping) / c_hotspot.size\n                        if connection_fr &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]:\n                            similar_hotspots_rows.append(dict(hotspot_id=c_hotspot.hotspot_id,\n                                                              hotspot_size=c_hotspot.size,\n                                                              connection_fr=connection_fr,\n                                                              avg_weight=np.mean(weights_subset)))\n                if similar_hotspots_rows:\n                    similar_hotspots_rows = pd.DataFrame(similar_hotspots_rows)\n                    similar_hotspots_rows.sort_values(by=[\"avg_weight\", \"connection_fr\", \"hotspot_size\"],\n                                                      inplace=True,\n                                                      ascending=[False, False, False])\n                    best_hit_for_q_isl = similar_hotspots_rows.iloc[0].to_dict()\n                    hotspot_hits_statistic_rows.append(dict(query_island_id=q_island.island_id,\n                                                            query_island_proteins=\",\".join(\n                                                                q_island.get_island_proteins(query_proteome.cdss)),\n                                                            closest_hotspot=best_hit_for_q_isl[\"hotspot_id\"],\n                                                            closest_hotspot_size=best_hit_for_q_isl[\"hotspot_size\"],\n                                                            closest_hotspot_avg_weight=best_hit_for_q_isl[\n                                                                \"avg_weight\"],\n                                                            closest_hotspot_connection_fr=best_hit_for_q_isl[\n                                                                \"connection_fr\"]))\n            hotspot_hits_statistic = pd.DataFrame(hotspot_hits_statistic_rows)\n            hotspot_hits_statistic.sort_values(by=[\"closest_hotspot_avg_weight\", \"closest_hotspot_connection_fr\"],\n                                               inplace=True, ascending=[False, False])\n            hotspot_hits_statistic.to_csv(\n                os.path.join(self.prms.args[\"output_dir\"], \"island_to_hotspot_mapping.tsv\"),\n                sep=\"\\t\", index=False)\n            hotspot_hits_statistic = hotspot_hits_statistic.drop_duplicates(subset=\"closest_hotspot\", keep=\"first\")\n            hotspot_hits_statistic.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                       \"island_to_hotspot_mapping_deduplicated.tsv\"),\n                                          sep=\"\\t\", index=False)\n            subset_hotspot_annotation = self.hotspots.annotation.loc[hotspot_hits_statistic[\"closest_hotspot\"]]\n            subset_hotspot_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                          \"annotation_of_mapped_hotspots.tsv\"), sep=\"\\t\",\n                                             index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(hotspot_hits_statistic.index)} of annotated variable island\"\n                      f\"{'s were' if len(hotspot_hits_statistic.index) &gt; 1 else ' was'} mapped to \"\n                      f\"the database hotspots\", file=sys.stdout)\n        else:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf The proteome community does not have any annotated hotspots\", file=sys.stdout)\n        # Update proteome and hotspots objects\n        self.proteomes.proteomes.at[query_proteome.proteome_id] = query_proteome\n        self.proteomes.annotation.loc[query_proteome.proteome_id] = proteomes_helper_obj.annotation.loc[\n            query_proteome.proteome_id]\n        self.proteomes.communities[query_proteome_community].append(query_proteome.proteome_id)\n        if community_hotspots:\n            for index, row in hotspot_hits_statistic.iterrows():\n                hotspot_id = row[\"closest_hotspot\"]\n                query_island_id = row[\"query_island_id\"]\n                query_island = query_proteome.islands.at[query_island_id]\n                closest_hotspot = self.hotspots.hotspots.at[hotspot_id]\n                closest_hotspot.islands.append(query_island)\n        # Visualisation\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Lovis4u visualisation of communities and proteomes...\", file=sys.stdout)\n        drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"hotspot\",\n                                                filename=\"lovis4u_proteome_community_hotspots.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"regular\",\n                                                filename=\"lovis4u_proteome_community.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"regular\",\n                                                proteome_ids=[query_proteome.proteome_id],\n                                                filename=\"lovis4u_query_proteome_variable.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"hotspot\",\n                                                proteome_ids=[query_proteome.proteome_id],\n                                                filename=\"lovis4u_query_proteome_hotspot.pdf\")\n        if community_hotspots:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Lovis4u visualisation found hotspots..\", file=sys.stdout)\n            if len(hotspot_hits_statistic.index) &gt; 0:\n                vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_full\"),\n                                      os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_with_query\")]\n                for vis_output_folder in vis_output_folders:\n                    if os.path.exists(vis_output_folder):\n                        shutil.rmtree(vis_output_folder)\n                    os.mkdir(vis_output_folder)\n            for index, row in hotspot_hits_statistic.iterrows():\n                hotspot_id = row[\"closest_hotspot\"]\n                query_island = row[\"query_island_id\"]\n                for hc, c_hotspots in self.hotspots.communities.items():\n                    if hotspot_id in c_hotspots:\n                        drawing_manager.plot_hotspots([hotspot_id],\n                                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                 \"lovis4u_hotspot_with_query\"),\n                                                      island_ids=[query_island])\n                        drawing_manager.plot_hotspots(c_hotspots,\n                                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                 \"lovis4u_hotspot_full\"),\n                                                      keep_while_deduplication=[query_island])\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Done!\")\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run proteome annotation mode versus the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspot","title":"<code>Hotspot</code>","text":"<p>Hotspot object represent a hotspot as a set of islands.</p> <p>Attributes:</p> <ul> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot identifier.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Number of islands.</p> </li> <li> <code>proteome_community</code>             (<code>int</code>)         \u2013          <p>Identifier of proteome community where hotspot is annotated.</p> </li> <li> <code>islands</code>             (<code>list</code>)         \u2013          <p>List of islands.</p> </li> <li> <code>conserved_signature</code>             (<code>list</code>)         \u2013          <p>Conserved flanking proteins that are usually found in islands.</p> </li> <li> <code>island_annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of islands.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether hotspot consists of flanked islands or not (that have conserved genes on both sides) [int: 1 or 0]</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Hotspot:\n    \"\"\"Hotspot object represent a hotspot as a set of islands.\n\n    Attributes:\n        hotspot_id (str): Hotspot identifier.\n        size (int): Number of islands.\n        proteome_community (int): Identifier of proteome community where hotspot is annotated.\n        islands (list): List of islands.\n        conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n        island_annotation (pd.DataFrame): Annotation table of islands.\n        flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n            [int: 1 or 0]\n\n    \"\"\"\n\n    def __init__(self, hotspot_id: str, size: int, proteome_community: int, islands: list,\n                 conserved_signature: list, island_annotation: pd.DataFrame, flanked: int):\n        \"\"\"Hotspot class constructor.\n\n        Arguments:\n            hotspot_id (str): Hotspot identifier.\n            size (int): Number of islands.\n            proteome_community (int): Identifier of proteome community where hotspot is annotated.\n            islands (list): List of islands.\n            conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n            island_annotation (pd.DataFrame): Annotation table of islands.\n            flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n                [int: 1 or 0]\n\n        \"\"\"\n        self.hotspot_id = hotspot_id\n        self.size = size\n        self.proteome_community = proteome_community\n        self.islands = islands\n        self.island_annotation = island_annotation\n        self.conserved_signature = conserved_signature\n        self.flanked = flanked\n\n    def get_hotspot_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"island_annotation\", \"islands\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n\n    def calculate_database_hits_stats(self, proteomes: Proteomes, prms: ilund4u.manager.Parameters,\n                                      protein_mode=False) -&gt; collections.defaultdict:\n        \"\"\"Calculate statistics of pyhmmer annotation for island proteins.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n            prms (ilund4u.manager.Parameters): Parameters object.\n\n        Returns:\n            collections.defaultdict: hits to the databases.\n\n        \"\"\"\n        hotspot_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n        for island in self.islands:\n            proteome = proteomes.proteomes.at[island.proteome]\n            island.calculate_database_hits_stat(proteome.cdss)\n            island_dbstat = island.databases_hits_stat\n            db_names = prms.args[\"databases_classes\"]\n            for db_name in db_names:\n                r_types = [\"cargo\", \"flanking\"]\n                for r_type in r_types:\n                    if not protein_mode:\n                        self.island_annotation.at[island.island_id, f\"{db_name}_{r_type}\"] = \\\n                            \",\".join(island_dbstat[db_name][r_type].values())\n                    try:\n                        hotspot_stat[db_name][r_type].update(island_dbstat[db_name][r_type])\n                    except:\n                        # ! For old db version | to remove later\n                        db_name_transform_dict = {\"defence\": \"Defence\", \"AMR\": \"AMR\",\n                                                  \"virulence\": \"Virulence\", \"anti-defence\": \"Anti-defence\"}\n                        hotspot_stat[db_name][r_type].update(island_dbstat[db_name_transform_dict[db_name]][r_type])\n\n        return hotspot_stat\n\n    def get_hotspot_groups(self, proteomes: Proteomes) -&gt; dict:\n        \"\"\"Get protein groups found on island cargo or as flanking genes\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            dict: cargo and flanking gene protein groups.\n\n        \"\"\"\n        groups = dict(cargo=set(), flanking=set())\n        for island in self.islands:\n            proteome = proteomes.proteomes.at[island.proteome]\n            groups[\"cargo\"].update(set(island.get_island_groups(proteome.cdss)))\n            groups[\"flanking\"].update(set(island.get_flanking_groups(proteome.cdss)))\n        return groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspot.__init__","title":"<code>__init__(hotspot_id, size, proteome_community, islands, conserved_signature, island_annotation, flanked)</code>","text":"<p>Hotspot class constructor.</p> <p>Parameters:</p> <ul> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot identifier.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Number of islands.</p> </li> <li> <code>proteome_community</code>             (<code>int</code>)         \u2013          <p>Identifier of proteome community where hotspot is annotated.</p> </li> <li> <code>islands</code>             (<code>list</code>)         \u2013          <p>List of islands.</p> </li> <li> <code>conserved_signature</code>             (<code>list</code>)         \u2013          <p>Conserved flanking proteins that are usually found in islands.</p> </li> <li> <code>island_annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of islands.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether hotspot consists of flanked islands or not (that have conserved genes on both sides) [int: 1 or 0]</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, hotspot_id: str, size: int, proteome_community: int, islands: list,\n             conserved_signature: list, island_annotation: pd.DataFrame, flanked: int):\n    \"\"\"Hotspot class constructor.\n\n    Arguments:\n        hotspot_id (str): Hotspot identifier.\n        size (int): Number of islands.\n        proteome_community (int): Identifier of proteome community where hotspot is annotated.\n        islands (list): List of islands.\n        conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n        island_annotation (pd.DataFrame): Annotation table of islands.\n        flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n            [int: 1 or 0]\n\n    \"\"\"\n    self.hotspot_id = hotspot_id\n    self.size = size\n    self.proteome_community = proteome_community\n    self.islands = islands\n    self.island_annotation = island_annotation\n    self.conserved_signature = conserved_signature\n    self.flanked = flanked\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspot.calculate_database_hits_stats","title":"<code>calculate_database_hits_stats(proteomes, prms, protein_mode=False)</code>","text":"<p>Calculate statistics of pyhmmer annotation for island proteins.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>defaultdict</code>         \u2013          <p>collections.defaultdict: hits to the databases.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_database_hits_stats(self, proteomes: Proteomes, prms: ilund4u.manager.Parameters,\n                                  protein_mode=False) -&gt; collections.defaultdict:\n    \"\"\"Calculate statistics of pyhmmer annotation for island proteins.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n        prms (ilund4u.manager.Parameters): Parameters object.\n\n    Returns:\n        collections.defaultdict: hits to the databases.\n\n    \"\"\"\n    hotspot_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n    for island in self.islands:\n        proteome = proteomes.proteomes.at[island.proteome]\n        island.calculate_database_hits_stat(proteome.cdss)\n        island_dbstat = island.databases_hits_stat\n        db_names = prms.args[\"databases_classes\"]\n        for db_name in db_names:\n            r_types = [\"cargo\", \"flanking\"]\n            for r_type in r_types:\n                if not protein_mode:\n                    self.island_annotation.at[island.island_id, f\"{db_name}_{r_type}\"] = \\\n                        \",\".join(island_dbstat[db_name][r_type].values())\n                try:\n                    hotspot_stat[db_name][r_type].update(island_dbstat[db_name][r_type])\n                except:\n                    # ! For old db version | to remove later\n                    db_name_transform_dict = {\"defence\": \"Defence\", \"AMR\": \"AMR\",\n                                              \"virulence\": \"Virulence\", \"anti-defence\": \"Anti-defence\"}\n                    hotspot_stat[db_name][r_type].update(island_dbstat[db_name_transform_dict[db_name]][r_type])\n\n    return hotspot_stat\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspot.get_hotspot_db_row","title":"<code>get_hotspot_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_hotspot_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"island_annotation\", \"islands\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspot.get_hotspot_groups","title":"<code>get_hotspot_groups(proteomes)</code>","text":"<p>Get protein groups found on island cargo or as flanking genes</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>cargo and flanking gene protein groups.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_hotspot_groups(self, proteomes: Proteomes) -&gt; dict:\n    \"\"\"Get protein groups found on island cargo or as flanking genes\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        dict: cargo and flanking gene protein groups.\n\n    \"\"\"\n    groups = dict(cargo=set(), flanking=set())\n    for island in self.islands:\n        proteome = proteomes.proteomes.at[island.proteome]\n        groups[\"cargo\"].update(set(island.get_island_groups(proteome.cdss)))\n        groups[\"flanking\"].update(set(island.get_flanking_groups(proteome.cdss)))\n    return groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots","title":"<code>Hotspots</code>","text":"<p>Hotspots object represents a set of annotated hotspots.</p> <p>Attributes:</p> <ul> <li> <code>hotspots</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Hotspot objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of hotspots.</p> </li> <li> <code>communities</code>             (<code>dict</code>)         \u2013          <p>Dictionary representing annotated communities of hotspots (key - community_id, value - list of hotspot ids)</p> </li> <li> <code>island_rep_proteins_fasta</code>             (<code>str</code>)         \u2013          <p>Path to a fasta file containing island representative proteins.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Hotspots:\n    \"\"\"Hotspots object represents a set of annotated hotspots.\n\n    Attributes:\n        hotspots (pd.Series): Series (list) of Hotspot objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n        communities (dict): Dictionary representing annotated communities of hotspots\n            (key - community_id, value - list of hotspot ids)\n        island_rep_proteins_fasta (str): Path to a fasta file containing island representative proteins.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, hotspots: pd.Series, annotation: pd.DataFrame, parameters: ilund4u.manager.Parameters):\n        \"\"\"Hotspots class constructor.\n\n        Arguments:\n            hotspots (pd.Series): Series (list) of Hotspot objects.\n            annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.hotspots = hotspots\n        self.annotation = annotation\n        self.communities = dict()\n        self.prms = parameters\n        self.__id_to_ind = {iid: idx for idx, iid in enumerate(self.annotation.index)}\n        self.island_rep_proteins_fasta = os.path.join(parameters.args[\"output_dir\"], \"island_rep_proteins.fa\")\n\n    def save_as_db(self, db_folder: str) -&gt; None:\n        \"\"\"Save Hotspots to the iLnd4u database.\n\n        Arguments:\n            db_folder (str): Database folder path.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            attributes_to_ignore = [\"hotspots\", \"annotation\", \"communities_annot\", \"prms\"]\n            attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n            attributes[\"island_rep_proteins_fasta\"] = os.path.basename(attributes[\"island_rep_proteins_fasta\"])\n            with open(os.path.join(db_folder, \"hotspots.attributes.json\"), 'w') as json_file:\n                json.dump(attributes, json_file)\n            self.annotation.to_csv(os.path.join(db_folder, \"hotspots.annotations.tsv\"), sep=\"\\t\",\n                                   index_label=\"hotspot_id\")\n            island_annotation_table = pd.DataFrame()\n            hotspot_db_ind = []\n            for hotspot in self.hotspots.to_list():\n                hotspot_db_ind.append(hotspot.get_hotspot_db_row())\n                h_island_annot = hotspot.island_annotation.copy()\n                h_island_annot[\"hotspot_id\"] = hotspot.hotspot_id\n                island_annotation_table = pd.concat([island_annotation_table, h_island_annot])\n\n            with open(os.path.join(db_folder, \"hotspot.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(hotspot_db_ind, json_file)\n\n            os.system(f\"cp {os.path.join(self.prms.args['output_dir'], 'protein_group_accumulated_statistics.tsv')} \"\n                      f\"{db_folder}\")\n\n            island_annotation_table.to_csv(os.path.join(db_folder, \"hotspot.ind.island.annotations.tsv\"), sep=\"\\t\",\n                                           index_label=\"island\")\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to write hotspots to the database.\") from error\n\n    @classmethod\n    def db_init(cls, db_path: str, proteomes: Proteomes, parameters: ilund4u.manager.Parameters):\n        \"\"\"Class method to load a Proteomes object from a database.\n\n        Arguments:\n            db_path (str): path to the database.\n            proteomes (Proteomes): Proteomes object.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            cls: Hotspots object.\n\n        \"\"\"\n        try:\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading hotspot objects...\", file=sys.stdout)\n            island_annotation = pd.read_table(os.path.join(db_path, \"hotspot.ind.island.annotations.tsv\"),\n                                              sep=\"\\t\", low_memory=False).set_index(\"island\")\n            with open(os.path.join(db_path, \"hotspot.ind.attributes.json\"), \"r\") as json_file:\n                hotspot_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(hotspot_ind_attributes), suffix='%(index)d/%(max)d')\n            hotspot_list = []\n            for hotspot_dict in hotspot_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                hotspot_dict[\"island_annotation\"] = island_annotation[\n                    island_annotation[\"hotspot_id\"] == hotspot_dict[\"hotspot_id\"]].copy()\n                hotspot_proteomes = proteomes.proteomes.loc[\n                    proteomes.communities[hotspot_dict[\"proteome_community\"]]].to_list()\n                islands_list = [island for proteome in hotspot_proteomes for island in proteome.islands.to_list()]\n                islands_series = pd.Series(islands_list, index=[island.island_id for island in islands_list])\n                hotspot_dict[\"islands\"] = islands_series.loc[hotspot_dict[\"island_annotation\"].index].to_list()\n                hotspot_list.append(Hotspot(**hotspot_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            hotspots = pd.Series(hotspot_list, index=[hotspot.hotspot_id for hotspot in hotspot_list])\n            annotation = pd.read_table(os.path.join(db_path, \"hotspots.annotations.tsv\"),\n                                       sep=\"\\t\", dtype={\"community\": \"Int32\"}).set_index(\"hotspot_id\")\n            cls_obj = cls(hotspots, annotation, parameters)\n            with open(os.path.join(db_path, \"hotspots.attributes.json\"), \"r\") as json_file:\n                attributes = json.load(json_file)\n            cls_obj.communities = {int(k): v for k, v in attributes[\"communities\"].items()}\n            cls_obj.island_rep_proteins_fasta = os.path.join(db_path, attributes[\"island_rep_proteins_fasta\"])\n            return cls_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to read hotspots from the database.\") from error\n\n    def pyhmmer_annotation(self, proteomes: Proteomes) -&gt; None:\n        \"\"\"Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Preparing data for additional island protein annotation with pyhmmer hmmscan...\",\n                      file=sys.stdout)\n            hotspots_repr_proteins = set()\n            for hotspot in self.hotspots.to_list():\n                for island in hotspot.islands:\n                    proteome = proteomes.proteomes.at[island.proteome]\n                    isl_groups = island.get_locus_groups(proteome.cdss)\n                    hotspots_repr_proteins.update(isl_groups)\n            initial_fasta_file = Bio.SeqIO.index(proteomes.proteins_fasta_file, \"fasta\")\n            with open(self.island_rep_proteins_fasta, \"wb\") as out_handle:\n                for acc in hotspots_repr_proteins:\n                    try:\n                        out_handle.write(initial_fasta_file.get_raw(acc))\n                    except:\n                        pass\n            alignment_table = ilund4u.methods.run_pyhmmer(self.island_rep_proteins_fasta, len(hotspots_repr_proteins),\n                                                          self.prms)\n            if not alignment_table.empty:\n                found_hits_for = alignment_table.index.to_list()\n                for proteome in proteomes.proteomes.to_list():\n                    proteome_cdss = proteome.cdss.to_list()\n                    proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.group in found_hits_for]\n                    if proteome_cdss_with_hits:\n                        cdss_with_hits = proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                        for cds in cdss_with_hits:\n                            alignment_table_row = alignment_table.loc[cds.group]\n                            cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                       db_name=alignment_table_row[\"target_db\"],\n                                                       target=alignment_table_row[\"target\"],\n                                                       evalue=alignment_table_row[\"hit_evalue\"])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run pyhmmer hmmscan annotation.\") from error\n\n    def build_hotspot_network(self):\n        \"\"\"Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Hotspot network construction...\", file=sys.stdout)\n            hotspot_signature_sizes = pd.Series()\n            hotspot_proteome_community = dict()\n            flanked_stat = dict()\n            signature_cluster_to_hotspot = collections.defaultdict(collections.deque)\n            for hid, hotspot in enumerate(self.hotspots):\n                flanked_stat[hid] = hotspot.flanked\n                hotspot_proteome_community[hid] = hotspot.proteome_community\n                hotspot_signature_sizes.at[hid] = len(hotspot.conserved_signature)\n                for cs_cluster in hotspot.conserved_signature:\n                    signature_cluster_to_hotspot[cs_cluster].append(hid)\n            edges, weights = [], []\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots), suffix=\"%(index)d/%(max)d\")\n            for i, hotspot_i in enumerate(self.hotspots):\n                bar.next()\n                pc_i = hotspot_i.proteome_community\n                signature_i = hotspot_i.conserved_signature\n                size_i = hotspot_signature_sizes.iat[i]\n                counts_i = collections.defaultdict(int)\n                for sc in signature_i:\n                    js = signature_cluster_to_hotspot[sc]\n                    for j in js.copy():\n                        if i &lt; j:\n                            if pc_i != hotspot_proteome_community[j]:  # to think about it\n                                counts_i[j] += 1\n                        else:\n                            js.popleft()\n                weights_i = pd.Series(counts_i)\n                connected_n_sizes = hotspot_signature_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                          index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"hotspot_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    if flanked_stat[i] == flanked_stat[j]:\n                        edges.append([i, j])\n                        weights.append(round(w, 4))\n            bar.finish()\n            print(\"\u25cb Hotspot network partitioning using the Leiden algorithm...\")\n            graph = igraph.Graph(len(hotspot_signature_sizes.index), edges, directed=False)\n            graph.vs[\"index\"] = hotspot_signature_sizes.index.to_list()\n            graph.vs[\"hotspot_id\"] = [h.hotspot_id for h in self.hotspots]\n            graph.vs[\"flanked\"] = [h.flanked for h in self.hotspots]\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(self.prms.args[\"output_dir\"], f\"hotspot_network.gml\"))\n            partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_h\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n            hotspot_communities_annot_rows = []\n            communities_sizes = []\n            self.annotation[\"hotspot_community\"] = pd.Series(dtype='Int64')\n            for community_index, community in enumerate(partition_leiden):\n                community_size = len(community)\n                subgraph = graph.subgraph(community)\n                hotspots = subgraph.vs[\"hotspot_id\"]\n                n_flanked = sum(subgraph.vs[\"flanked\"])\n                self.communities[community_index] = hotspots\n                self.annotation.loc[hotspots, \"hotspot_community\"] = community_index\n                if community_size &gt; 1:\n                    communities_sizes.append(community_size)\n                    subgraph_edges = subgraph.get_edgelist()\n                    num_of_edges = len(subgraph_edges)\n                    num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                    weights = subgraph.es[\"weight\"]\n                    avg_weight = round(np.mean(weights), 3)\n                    max_identity = max(weights)\n                else:\n                    num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n                hotspot_communities_annot_rows.append([community_index, community_size, avg_weight, n_flanked,\n                                                       max_identity, num_of_edges_fr, \";\".join(hotspots)])\n            communities_annot = pd.DataFrame(hotspot_communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\",\n                                                                                      \"n_flanked\", \"max_weight\",\n                                                                                      \"fr_edges\", \"hotspots\"])\n            communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                               \"hotspot_communities.tsv\")),\n                                     sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {sum(communities_sizes)} hotspots were merged to {len(communities_sizes)} not singleton \"\n                      f\"communities\")\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to build hotspot network.\") from error\n\n    def calculate_hotspot_and_island_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n        \"\"\"Calculate hotspot statistics based using hmmscan results and save annotation tables.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            pd.DataFrame: hotspot community annotation table.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Hotspot and island statistics calculation...\", file=sys.stdout)\n            hotspot_community_annot_rows = []\n            r_types = [\"cargo\", \"flanking\"]\n            # Create new columns\n            for r_type in r_types:\n                self.annotation[f\"N_{r_type}_groups\"] = pd.Series(dtype='Int64')\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    self.annotation[f\"N_{db_name}_{r_type}_groups\"] = pd.Series(dtype='Int64')\n            self.annotation[\"conserved_signature\"] = pd.Series(dtype=\"str\")\n            # Get stat\n            for h_com, hotspot_ids in self.communities.items():\n                h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n                hotspot_com_groups = dict(cargo=set(), flanking=set())\n                hotspots = self.hotspots.loc[hotspot_ids].to_list()\n                n_islands, n_flanked = 0, 0\n                for hotspot in hotspots:\n                    n_islands += hotspot.size\n                    n_flanked += hotspot.flanked\n                    hotspot_groups = hotspot.get_hotspot_groups(proteomes)\n                    db_stat = hotspot.calculate_database_hits_stats(proteomes, self.prms)\n                    self.annotation.at[hotspot.hotspot_id, \"conserved_signature\"] = \";\".join(\n                        hotspot.conserved_signature)\n                    for r_type in r_types:\n                        hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                        self.annotation.at[hotspot.hotspot_id, f\"N_{r_type}_groups\"] = len(hotspot_groups[r_type])\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        for r_type in r_types:\n                            h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                            self.annotation.at[hotspot.hotspot_id, f\"N_{db_name}_{r_type}_groups\"] = \\\n                                len(set(db_stat[db_name][r_type].values()))\n                hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                    N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                    pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n                hotspot_community_annot_rows.append(hc_annot_row)\n\n            hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n            for db_name in self.prms.args[\"databases_classes\"]:\n                self.annotation[f\"{db_name}_cargo_normalised\"] = \\\n                    self.annotation.apply(lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"],\n                                                            4), axis=1)\n                hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                    hotspot_community_annot.apply(\n                        lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n            self.annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"hotspot_annotation.tsv\"), sep=\"\\t\",\n                                   index_label=\"hotspot_id\")\n            hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                        \"hotspot_community_annotation.tsv\"), sep=\"\\t\", index=False)\n            # Save island annotation table\n            # Get non-hotspot island stat\n            island_annotation_table_rows = []\n            for pcom, com_proteomes in proteomes.communities.items():\n                for proteome_id in com_proteomes:\n                    proteome = proteomes.proteomes.at[proteome_id]\n                    for island in proteome.islands.to_list():\n                        island_annot = dict(island=island.island_id, proteome=proteome.proteome_id,\n                                            proteome_commuity=pcom, hotspot_id=island.hotspot_id,\n                                            flanked=island.flanked, island_size=island.size)\n                        island.calculate_database_hits_stat(proteome.cdss)\n                        island_dbstat = island.databases_hits_stat\n                        db_names = self.prms.args[\"databases_classes\"]\n                        for db_name in db_names:\n                            r_types = [\"cargo\", \"flanking\"]\n                            for r_type in r_types:\n                                island_annot[f\"N_{db_name}_{r_type}\"] = len(island_dbstat[db_name][r_type].values())\n                        isl_groups = island.get_island_groups(proteome.cdss)\n                        isl_proteins = island.get_island_proteins(proteome.cdss)\n                        island_annot[\"island_proteins\"] = \",\".join(isl_proteins)\n                        island_annot[\"island_protein_groups\"] = \",\".join(isl_groups)\n                        island_annotation_table_rows.append(island_annot)\n            island_annotation_table = pd.DataFrame(island_annotation_table_rows)\n            island_annotation_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"island_annotation.tsv\"),\n                                           sep=\"\\t\", index=False)\n            return hotspot_community_annot\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to calculate hotspot and hotspot community statistics based \"\n                                               \"on hmmscan results\") from error\n\n    def get_each_protein_group_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n        \"\"\"Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots\n            where it's encoded.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            pd.DataFrame: Protein group statistics table\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Protein group statistics calculation...\", file=sys.stdout)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots.index), suffix='%(index)d/%(max)d')\n            protein_group_statistics_dict = collections.defaultdict(\n                lambda: {\"Hotspot_communities\": set(), \"Hotspots\": set(), \"Hotpot_islands\": set(),\n                         \"Non_hotspot_islands\": set(), \"Proteome_communities\": set(), \"Flanked_hotspot_islands\": set(),\n                         \"Flanked_non_hotspot_islands\": set(), \"Counts\": 0, \"db\": \"None\", \"db_hit\": \"None\", \"Name\": \"\",\n                         \"RepLength\": 0})\n            # Hotspot stat\n            for h_com, hotspots_ids in self.communities.items():\n                hotspots = self.hotspots.loc[hotspots_ids].to_list()\n                for hotspot in hotspots:\n                    if self.prms.args[\"verbose\"]:\n                        bar.next()\n                    for island in hotspot.islands:\n                        proteome = proteomes.proteomes.at[island.proteome]\n                        island_proteins = island.get_island_proteins(proteome.cdss)\n                        island_protein_groups = island.get_island_groups(proteome.cdss)\n                        for isp, ispg in zip(island_proteins, island_protein_groups):\n                            if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                cds_obj = proteome.cdss.at[isp]\n                                protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                if cds_obj.hmmscan_results:\n                                    protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\"target\"]\n                                    protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                    if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                        protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                            \"db_name\"]\n                            protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                            protein_group_statistics_dict[ispg][\"Hotspot_communities\"].add(h_com)\n                            protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(hotspot.proteome_community)\n                            protein_group_statistics_dict[ispg][\"Hotspots\"].add(hotspot.hotspot_id)\n                            protein_group_statistics_dict[ispg][\"Hotpot_islands\"].add(island.island_id)\n                            if island.flanked:\n                                protein_group_statistics_dict[ispg][\"Flanked_hotspot_islands\"].add(island.island_id)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            # Other accessory genes\n            for proteome_com, com_proteomes in proteomes.communities.items():\n                for proteome_id in com_proteomes:\n                    proteome = proteomes.proteomes.at[proteome_id]\n                    for island in proteome.islands.to_list():\n                        if island.hotspot_id == \"-\":\n                            island_proteins = island.get_island_proteins(proteome.cdss)\n                            island_protein_groups = island.get_island_groups(proteome.cdss)\n                            for isp, ispg in zip(island_proteins, island_protein_groups):\n                                if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                    cds_obj = proteome.cdss.at[isp]\n                                    protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                    protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                    if cds_obj.hmmscan_results:\n                                        protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\n                                            \"target\"]\n                                        protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                        if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                            protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                                \"db_name\"]\n                                protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                                protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(proteome_com)\n                                protein_group_statistics_dict[ispg][\"Non_hotspot_islands\"].add(island.island_id)\n                                if island.flanked:\n                                    protein_group_statistics_dict[ispg][\"Flanked_non_hotspot_islands\"].add(\n                                        island.island_id)\n\n            statistic_rows = []\n            for pg, pg_dict in protein_group_statistics_dict.items():\n                row_dict = dict(representative_protein=pg, db=pg_dict[\"db\"], pb_hit=pg_dict[\"db_hit\"],\n                                name=pg_dict[\"Name\"], counts=pg_dict[\"Counts\"], length=pg_dict[\"RepLength\"])\n                for k, v in pg_dict.items():\n                    if isinstance(v, set):\n                        row_dict[f\"N_{k}\"] = len(v)\n                if pg_dict[\"Hotspots\"]:\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        dbsn_norm_values = []\n                        for hotspot in pg_dict[\"Hotspots\"]:\n                            dbsn_norm_value = self.annotation.at[hotspot, f\"{db_name}_cargo_normalised\"]\n                            if pg_dict[\"db\"] == db_name:\n                                dbsn_norm_value -= 1 / self.annotation.at[hotspot, f\"N_cargo_groups\"]\n                            dbsn_norm_values.append(dbsn_norm_value)\n                        row_dict[f\"{db_name}_avg_cargo_fraction\"] = round(np.mean(dbsn_norm_values), 4)\n                        row_dict[f\"{db_name}_max_cargo_fraction\"] = round(max(dbsn_norm_values), 4)\n                    row_dict[\"hotspots\"] = \",\".join(pg_dict[\"Hotspots\"])\n                else:\n                    row_dict[\"hotspots\"] = \"Non\"\n\n                statistic_rows.append(row_dict)\n            statistic_table = pd.DataFrame(statistic_rows)\n            statistic_table.sort_values(by=[\"N_Hotspot_communities\", \"N_Hotspots\"], ascending=[False, False],\n                                        inplace=True)\n            statistic_table.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                \"protein_group_accumulated_statistics.tsv\"), sep=\"\\t\", index=False)\n            return statistic_table\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to calculate protein group statistics.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.__init__","title":"<code>__init__(hotspots, annotation, parameters)</code>","text":"<p>Hotspots class constructor.</p> <p>Parameters:</p> <ul> <li> <code>hotspots</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Hotspot objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of hotspots.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, hotspots: pd.Series, annotation: pd.DataFrame, parameters: ilund4u.manager.Parameters):\n    \"\"\"Hotspots class constructor.\n\n    Arguments:\n        hotspots (pd.Series): Series (list) of Hotspot objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.hotspots = hotspots\n    self.annotation = annotation\n    self.communities = dict()\n    self.prms = parameters\n    self.__id_to_ind = {iid: idx for idx, iid in enumerate(self.annotation.index)}\n    self.island_rep_proteins_fasta = os.path.join(parameters.args[\"output_dir\"], \"island_rep_proteins.fa\")\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.build_hotspot_network","title":"<code>build_hotspot_network()</code>","text":"<p>Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.</p> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_hotspot_network(self):\n    \"\"\"Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Hotspot network construction...\", file=sys.stdout)\n        hotspot_signature_sizes = pd.Series()\n        hotspot_proteome_community = dict()\n        flanked_stat = dict()\n        signature_cluster_to_hotspot = collections.defaultdict(collections.deque)\n        for hid, hotspot in enumerate(self.hotspots):\n            flanked_stat[hid] = hotspot.flanked\n            hotspot_proteome_community[hid] = hotspot.proteome_community\n            hotspot_signature_sizes.at[hid] = len(hotspot.conserved_signature)\n            for cs_cluster in hotspot.conserved_signature:\n                signature_cluster_to_hotspot[cs_cluster].append(hid)\n        edges, weights = [], []\n        bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots), suffix=\"%(index)d/%(max)d\")\n        for i, hotspot_i in enumerate(self.hotspots):\n            bar.next()\n            pc_i = hotspot_i.proteome_community\n            signature_i = hotspot_i.conserved_signature\n            size_i = hotspot_signature_sizes.iat[i]\n            counts_i = collections.defaultdict(int)\n            for sc in signature_i:\n                js = signature_cluster_to_hotspot[sc]\n                for j in js.copy():\n                    if i &lt; j:\n                        if pc_i != hotspot_proteome_community[j]:  # to think about it\n                            counts_i[j] += 1\n                    else:\n                        js.popleft()\n            weights_i = pd.Series(counts_i)\n            connected_n_sizes = hotspot_signature_sizes.iloc[weights_i.index]\n            norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                      index=weights_i.index)\n            weights_i = weights_i.mul(norm_factor_i)\n            weights_i = weights_i[weights_i &gt;= self.prms.args[\"hotspot_similarity_cutoff\"]]\n            for j, w in weights_i.items():\n                if flanked_stat[i] == flanked_stat[j]:\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n        bar.finish()\n        print(\"\u25cb Hotspot network partitioning using the Leiden algorithm...\")\n        graph = igraph.Graph(len(hotspot_signature_sizes.index), edges, directed=False)\n        graph.vs[\"index\"] = hotspot_signature_sizes.index.to_list()\n        graph.vs[\"hotspot_id\"] = [h.hotspot_id for h in self.hotspots]\n        graph.vs[\"flanked\"] = [h.flanked for h in self.hotspots]\n        graph.es[\"weight\"] = weights\n        graph.save(os.path.join(self.prms.args[\"output_dir\"], f\"hotspot_network.gml\"))\n        partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                    resolution_parameter=self.prms.args[\n                                                        \"leiden_resolution_parameter_h\"],\n                                                    weights=\"weight\", n_iterations=-1)\n        graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n        hotspot_communities_annot_rows = []\n        communities_sizes = []\n        self.annotation[\"hotspot_community\"] = pd.Series(dtype='Int64')\n        for community_index, community in enumerate(partition_leiden):\n            community_size = len(community)\n            subgraph = graph.subgraph(community)\n            hotspots = subgraph.vs[\"hotspot_id\"]\n            n_flanked = sum(subgraph.vs[\"flanked\"])\n            self.communities[community_index] = hotspots\n            self.annotation.loc[hotspots, \"hotspot_community\"] = community_index\n            if community_size &gt; 1:\n                communities_sizes.append(community_size)\n                subgraph_edges = subgraph.get_edgelist()\n                num_of_edges = len(subgraph_edges)\n                num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                weights = subgraph.es[\"weight\"]\n                avg_weight = round(np.mean(weights), 3)\n                max_identity = max(weights)\n            else:\n                num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n            hotspot_communities_annot_rows.append([community_index, community_size, avg_weight, n_flanked,\n                                                   max_identity, num_of_edges_fr, \";\".join(hotspots)])\n        communities_annot = pd.DataFrame(hotspot_communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\",\n                                                                                  \"n_flanked\", \"max_weight\",\n                                                                                  \"fr_edges\", \"hotspots\"])\n        communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"hotspot_communities.tsv\")),\n                                 sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {sum(communities_sizes)} hotspots were merged to {len(communities_sizes)} not singleton \"\n                  f\"communities\")\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to build hotspot network.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.calculate_hotspot_and_island_statistics","title":"<code>calculate_hotspot_and_island_statistics(proteomes)</code>","text":"<p>Calculate hotspot statistics based using hmmscan results and save annotation tables.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: hotspot community annotation table.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_hotspot_and_island_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n    \"\"\"Calculate hotspot statistics based using hmmscan results and save annotation tables.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        pd.DataFrame: hotspot community annotation table.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Hotspot and island statistics calculation...\", file=sys.stdout)\n        hotspot_community_annot_rows = []\n        r_types = [\"cargo\", \"flanking\"]\n        # Create new columns\n        for r_type in r_types:\n            self.annotation[f\"N_{r_type}_groups\"] = pd.Series(dtype='Int64')\n            for db_name in self.prms.args[\"databases_classes\"]:\n                self.annotation[f\"N_{db_name}_{r_type}_groups\"] = pd.Series(dtype='Int64')\n        self.annotation[\"conserved_signature\"] = pd.Series(dtype=\"str\")\n        # Get stat\n        for h_com, hotspot_ids in self.communities.items():\n            h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n            hotspot_com_groups = dict(cargo=set(), flanking=set())\n            hotspots = self.hotspots.loc[hotspot_ids].to_list()\n            n_islands, n_flanked = 0, 0\n            for hotspot in hotspots:\n                n_islands += hotspot.size\n                n_flanked += hotspot.flanked\n                hotspot_groups = hotspot.get_hotspot_groups(proteomes)\n                db_stat = hotspot.calculate_database_hits_stats(proteomes, self.prms)\n                self.annotation.at[hotspot.hotspot_id, \"conserved_signature\"] = \";\".join(\n                    hotspot.conserved_signature)\n                for r_type in r_types:\n                    hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                    self.annotation.at[hotspot.hotspot_id, f\"N_{r_type}_groups\"] = len(hotspot_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                        self.annotation.at[hotspot.hotspot_id, f\"N_{db_name}_{r_type}_groups\"] = \\\n                            len(set(db_stat[db_name][r_type].values()))\n            hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n            for r_type in r_types:\n                hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n            for db_name in self.prms.args[\"databases_classes\"]:\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n            hotspot_community_annot_rows.append(hc_annot_row)\n\n        hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n        for db_name in self.prms.args[\"databases_classes\"]:\n            self.annotation[f\"{db_name}_cargo_normalised\"] = \\\n                self.annotation.apply(lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"],\n                                                        4), axis=1)\n            hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                hotspot_community_annot.apply(\n                    lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n        self.annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"hotspot_annotation.tsv\"), sep=\"\\t\",\n                               index_label=\"hotspot_id\")\n        hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                    \"hotspot_community_annotation.tsv\"), sep=\"\\t\", index=False)\n        # Save island annotation table\n        # Get non-hotspot island stat\n        island_annotation_table_rows = []\n        for pcom, com_proteomes in proteomes.communities.items():\n            for proteome_id in com_proteomes:\n                proteome = proteomes.proteomes.at[proteome_id]\n                for island in proteome.islands.to_list():\n                    island_annot = dict(island=island.island_id, proteome=proteome.proteome_id,\n                                        proteome_commuity=pcom, hotspot_id=island.hotspot_id,\n                                        flanked=island.flanked, island_size=island.size)\n                    island.calculate_database_hits_stat(proteome.cdss)\n                    island_dbstat = island.databases_hits_stat\n                    db_names = self.prms.args[\"databases_classes\"]\n                    for db_name in db_names:\n                        r_types = [\"cargo\", \"flanking\"]\n                        for r_type in r_types:\n                            island_annot[f\"N_{db_name}_{r_type}\"] = len(island_dbstat[db_name][r_type].values())\n                    isl_groups = island.get_island_groups(proteome.cdss)\n                    isl_proteins = island.get_island_proteins(proteome.cdss)\n                    island_annot[\"island_proteins\"] = \",\".join(isl_proteins)\n                    island_annot[\"island_protein_groups\"] = \",\".join(isl_groups)\n                    island_annotation_table_rows.append(island_annot)\n        island_annotation_table = pd.DataFrame(island_annotation_table_rows)\n        island_annotation_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"island_annotation.tsv\"),\n                                       sep=\"\\t\", index=False)\n        return hotspot_community_annot\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to calculate hotspot and hotspot community statistics based \"\n                                           \"on hmmscan results\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.db_init","title":"<code>db_init(db_path, proteomes, parameters)</code>  <code>classmethod</code>","text":"<p>Class method to load a Proteomes object from a database.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>path to the database.</p> </li> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code>        \u2013          <p>Hotspots object.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>@classmethod\ndef db_init(cls, db_path: str, proteomes: Proteomes, parameters: ilund4u.manager.Parameters):\n    \"\"\"Class method to load a Proteomes object from a database.\n\n    Arguments:\n        db_path (str): path to the database.\n        proteomes (Proteomes): Proteomes object.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        cls: Hotspots object.\n\n    \"\"\"\n    try:\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading hotspot objects...\", file=sys.stdout)\n        island_annotation = pd.read_table(os.path.join(db_path, \"hotspot.ind.island.annotations.tsv\"),\n                                          sep=\"\\t\", low_memory=False).set_index(\"island\")\n        with open(os.path.join(db_path, \"hotspot.ind.attributes.json\"), \"r\") as json_file:\n            hotspot_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(hotspot_ind_attributes), suffix='%(index)d/%(max)d')\n        hotspot_list = []\n        for hotspot_dict in hotspot_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            hotspot_dict[\"island_annotation\"] = island_annotation[\n                island_annotation[\"hotspot_id\"] == hotspot_dict[\"hotspot_id\"]].copy()\n            hotspot_proteomes = proteomes.proteomes.loc[\n                proteomes.communities[hotspot_dict[\"proteome_community\"]]].to_list()\n            islands_list = [island for proteome in hotspot_proteomes for island in proteome.islands.to_list()]\n            islands_series = pd.Series(islands_list, index=[island.island_id for island in islands_list])\n            hotspot_dict[\"islands\"] = islands_series.loc[hotspot_dict[\"island_annotation\"].index].to_list()\n            hotspot_list.append(Hotspot(**hotspot_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        hotspots = pd.Series(hotspot_list, index=[hotspot.hotspot_id for hotspot in hotspot_list])\n        annotation = pd.read_table(os.path.join(db_path, \"hotspots.annotations.tsv\"),\n                                   sep=\"\\t\", dtype={\"community\": \"Int32\"}).set_index(\"hotspot_id\")\n        cls_obj = cls(hotspots, annotation, parameters)\n        with open(os.path.join(db_path, \"hotspots.attributes.json\"), \"r\") as json_file:\n            attributes = json.load(json_file)\n        cls_obj.communities = {int(k): v for k, v in attributes[\"communities\"].items()}\n        cls_obj.island_rep_proteins_fasta = os.path.join(db_path, attributes[\"island_rep_proteins_fasta\"])\n        return cls_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to read hotspots from the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.get_each_protein_group_statistics","title":"<code>get_each_protein_group_statistics(proteomes)</code>","text":"<p>Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots     where it's encoded.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: Protein group statistics table</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_each_protein_group_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n    \"\"\"Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots\n        where it's encoded.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        pd.DataFrame: Protein group statistics table\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Protein group statistics calculation...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots.index), suffix='%(index)d/%(max)d')\n        protein_group_statistics_dict = collections.defaultdict(\n            lambda: {\"Hotspot_communities\": set(), \"Hotspots\": set(), \"Hotpot_islands\": set(),\n                     \"Non_hotspot_islands\": set(), \"Proteome_communities\": set(), \"Flanked_hotspot_islands\": set(),\n                     \"Flanked_non_hotspot_islands\": set(), \"Counts\": 0, \"db\": \"None\", \"db_hit\": \"None\", \"Name\": \"\",\n                     \"RepLength\": 0})\n        # Hotspot stat\n        for h_com, hotspots_ids in self.communities.items():\n            hotspots = self.hotspots.loc[hotspots_ids].to_list()\n            for hotspot in hotspots:\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                for island in hotspot.islands:\n                    proteome = proteomes.proteomes.at[island.proteome]\n                    island_proteins = island.get_island_proteins(proteome.cdss)\n                    island_protein_groups = island.get_island_groups(proteome.cdss)\n                    for isp, ispg in zip(island_proteins, island_protein_groups):\n                        if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                            cds_obj = proteome.cdss.at[isp]\n                            protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                            protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                            if cds_obj.hmmscan_results:\n                                protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\"target\"]\n                                protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                    protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                        \"db_name\"]\n                        protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                        protein_group_statistics_dict[ispg][\"Hotspot_communities\"].add(h_com)\n                        protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(hotspot.proteome_community)\n                        protein_group_statistics_dict[ispg][\"Hotspots\"].add(hotspot.hotspot_id)\n                        protein_group_statistics_dict[ispg][\"Hotpot_islands\"].add(island.island_id)\n                        if island.flanked:\n                            protein_group_statistics_dict[ispg][\"Flanked_hotspot_islands\"].add(island.island_id)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        # Other accessory genes\n        for proteome_com, com_proteomes in proteomes.communities.items():\n            for proteome_id in com_proteomes:\n                proteome = proteomes.proteomes.at[proteome_id]\n                for island in proteome.islands.to_list():\n                    if island.hotspot_id == \"-\":\n                        island_proteins = island.get_island_proteins(proteome.cdss)\n                        island_protein_groups = island.get_island_groups(proteome.cdss)\n                        for isp, ispg in zip(island_proteins, island_protein_groups):\n                            if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                cds_obj = proteome.cdss.at[isp]\n                                protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                if cds_obj.hmmscan_results:\n                                    protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\n                                        \"target\"]\n                                    protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                    if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                        protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                            \"db_name\"]\n                            protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                            protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(proteome_com)\n                            protein_group_statistics_dict[ispg][\"Non_hotspot_islands\"].add(island.island_id)\n                            if island.flanked:\n                                protein_group_statistics_dict[ispg][\"Flanked_non_hotspot_islands\"].add(\n                                    island.island_id)\n\n        statistic_rows = []\n        for pg, pg_dict in protein_group_statistics_dict.items():\n            row_dict = dict(representative_protein=pg, db=pg_dict[\"db\"], pb_hit=pg_dict[\"db_hit\"],\n                            name=pg_dict[\"Name\"], counts=pg_dict[\"Counts\"], length=pg_dict[\"RepLength\"])\n            for k, v in pg_dict.items():\n                if isinstance(v, set):\n                    row_dict[f\"N_{k}\"] = len(v)\n            if pg_dict[\"Hotspots\"]:\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    dbsn_norm_values = []\n                    for hotspot in pg_dict[\"Hotspots\"]:\n                        dbsn_norm_value = self.annotation.at[hotspot, f\"{db_name}_cargo_normalised\"]\n                        if pg_dict[\"db\"] == db_name:\n                            dbsn_norm_value -= 1 / self.annotation.at[hotspot, f\"N_cargo_groups\"]\n                        dbsn_norm_values.append(dbsn_norm_value)\n                    row_dict[f\"{db_name}_avg_cargo_fraction\"] = round(np.mean(dbsn_norm_values), 4)\n                    row_dict[f\"{db_name}_max_cargo_fraction\"] = round(max(dbsn_norm_values), 4)\n                row_dict[\"hotspots\"] = \",\".join(pg_dict[\"Hotspots\"])\n            else:\n                row_dict[\"hotspots\"] = \"Non\"\n\n            statistic_rows.append(row_dict)\n        statistic_table = pd.DataFrame(statistic_rows)\n        statistic_table.sort_values(by=[\"N_Hotspot_communities\", \"N_Hotspots\"], ascending=[False, False],\n                                    inplace=True)\n        statistic_table.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                            \"protein_group_accumulated_statistics.tsv\"), sep=\"\\t\", index=False)\n        return statistic_table\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to calculate protein group statistics.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.pyhmmer_annotation","title":"<code>pyhmmer_annotation(proteomes)</code>","text":"<p>Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def pyhmmer_annotation(self, proteomes: Proteomes) -&gt; None:\n    \"\"\"Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Preparing data for additional island protein annotation with pyhmmer hmmscan...\",\n                  file=sys.stdout)\n        hotspots_repr_proteins = set()\n        for hotspot in self.hotspots.to_list():\n            for island in hotspot.islands:\n                proteome = proteomes.proteomes.at[island.proteome]\n                isl_groups = island.get_locus_groups(proteome.cdss)\n                hotspots_repr_proteins.update(isl_groups)\n        initial_fasta_file = Bio.SeqIO.index(proteomes.proteins_fasta_file, \"fasta\")\n        with open(self.island_rep_proteins_fasta, \"wb\") as out_handle:\n            for acc in hotspots_repr_proteins:\n                try:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n                except:\n                    pass\n        alignment_table = ilund4u.methods.run_pyhmmer(self.island_rep_proteins_fasta, len(hotspots_repr_proteins),\n                                                      self.prms)\n        if not alignment_table.empty:\n            found_hits_for = alignment_table.index.to_list()\n            for proteome in proteomes.proteomes.to_list():\n                proteome_cdss = proteome.cdss.to_list()\n                proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.group in found_hits_for]\n                if proteome_cdss_with_hits:\n                    cdss_with_hits = proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                    for cds in cdss_with_hits:\n                        alignment_table_row = alignment_table.loc[cds.group]\n                        cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                   db_name=alignment_table_row[\"target_db\"],\n                                                   target=alignment_table_row[\"target\"],\n                                                   evalue=alignment_table_row[\"hit_evalue\"])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run pyhmmer hmmscan annotation.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Hotspots.save_as_db","title":"<code>save_as_db(db_folder)</code>","text":"<p>Save Hotspots to the iLnd4u database.</p> <p>Parameters:</p> <ul> <li> <code>db_folder</code>             (<code>str</code>)         \u2013          <p>Database folder path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def save_as_db(self, db_folder: str) -&gt; None:\n    \"\"\"Save Hotspots to the iLnd4u database.\n\n    Arguments:\n        db_folder (str): Database folder path.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        attributes_to_ignore = [\"hotspots\", \"annotation\", \"communities_annot\", \"prms\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"island_rep_proteins_fasta\"] = os.path.basename(attributes[\"island_rep_proteins_fasta\"])\n        with open(os.path.join(db_folder, \"hotspots.attributes.json\"), 'w') as json_file:\n            json.dump(attributes, json_file)\n        self.annotation.to_csv(os.path.join(db_folder, \"hotspots.annotations.tsv\"), sep=\"\\t\",\n                               index_label=\"hotspot_id\")\n        island_annotation_table = pd.DataFrame()\n        hotspot_db_ind = []\n        for hotspot in self.hotspots.to_list():\n            hotspot_db_ind.append(hotspot.get_hotspot_db_row())\n            h_island_annot = hotspot.island_annotation.copy()\n            h_island_annot[\"hotspot_id\"] = hotspot.hotspot_id\n            island_annotation_table = pd.concat([island_annotation_table, h_island_annot])\n\n        with open(os.path.join(db_folder, \"hotspot.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(hotspot_db_ind, json_file)\n\n        os.system(f\"cp {os.path.join(self.prms.args['output_dir'], 'protein_group_accumulated_statistics.tsv')} \"\n                  f\"{db_folder}\")\n\n        island_annotation_table.to_csv(os.path.join(db_folder, \"hotspot.ind.island.annotations.tsv\"), sep=\"\\t\",\n                                       index_label=\"island\")\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to write hotspots to the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island","title":"<code>Island</code>","text":"<p>Island object represents an annotated island defined as a region with a set of non-conserved proteins.</p> <p>Attributes:</p> <ul> <li> <code>island_id</code>             (<code>str</code>)         \u2013          <p>Island identifier.</p> </li> <li> <code>proteome</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where island is annotated.</p> </li> <li> <code>circular_proteome</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then     first and last genes are considered as neighbours.</p> </li> <li> <code>center</code>             (<code>int</code>)         \u2013          <p>CDS index of the island center.</p> </li> <li> <code>indexes</code>             (<code>list</code>)         \u2013          <p>CDS indexes of the island.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Length of the island (number of CDSs).</p> </li> <li> <code>var_indexes</code>             (<code>list</code>)         \u2013          <p>Indexes of CDS which g_class is \"variable\".</p> </li> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot id if Island was attributed to one of them or \"-\" value if not.</p> </li> <li> <code>left_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the left.</p> </li> <li> <code>right_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the right.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether island is flanked by conserved genes or not [1 or 0].</p> </li> <li> <code>databases_hits_stat</code>             (<code>dict</code>)         \u2013          <p>Statistics from hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Island:\n    \"\"\"Island object represents an annotated island defined as a region with a set of non-conserved proteins.\n\n    Attributes:\n        island_id (str): Island identifier.\n        proteome (str): Proteome identifier where island is annotated.\n        circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n                first and last genes are considered as neighbours.\n        center (int): CDS index of the island center.\n        indexes (list): CDS indexes of the island.\n        size (int): Length of the island (number of CDSs).\n        var_indexes (list): Indexes of CDS which g_class is \"variable\".\n        hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n        left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n        right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n        flanked (int): Whether island is flanked by conserved genes or not [1 or 0].\n        databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n    \"\"\"\n\n    def __init__(self, island_id: str, proteome: str,\n                 circular_proteome: int, center: int, indexes: list, var_indexes: list,\n                 left_cons_neighbours: list, right_cons_neighbours: list,\n                 hotspot_id=\"-\", databases_hits_stat: typing.Union[None, dict] = None):\n        \"\"\"Island class constructor.\n\n        Arguments:\n            island_id (str): Island identifier.\n            proteome (str): Proteome identifier where island is annotated.\n            circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n                first and last genes are considered as neighbours.\n            center (int): CDS index of the island center.\n            indexes (list): CDS indexes of the island.\n            var_indexes (list): Indexes of CDS which g_class is \"variable\".\n            left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n            right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n            hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n            databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n        \"\"\"\n        self.island_id = island_id\n        self.proteome = proteome\n        self.circular_proteome = circular_proteome\n        self.hotspot_id = hotspot_id\n        self.center = center\n        self.indexes = indexes\n        self.size = len(indexes)\n        self.var_indexes = var_indexes\n        self.left_cons_neighbours = left_cons_neighbours\n        self.right_cons_neighbours = right_cons_neighbours\n        if (not left_cons_neighbours or not right_cons_neighbours) and not circular_proteome:\n            self.flanked = 0\n        else:\n            self.flanked = 1\n        if databases_hits_stat is None:\n            databases_hits_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n        self.databases_hits_stat = databases_hits_stat\n\n    def get_island_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"size\", \"flanked\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n\n    def get_cons_neighbours_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of conserved neighbour CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of conserved neighbours.\n\n        \"\"\"\n        all_cons_neighbours = self.left_cons_neighbours + self.right_cons_neighbours\n        cons_neighbours_groups = cdss.iloc[all_cons_neighbours].apply(lambda cds: cds.group).to_list()\n        return cons_neighbours_groups\n\n    def get_locus_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of island CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.group).to_list()\n        return locus_groups\n\n    def get_island_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of island CDSs.\n\n        \"\"\"\n        island_groups = cdss.iloc[self.indexes].apply(lambda cds: cds.group).to_list()\n        return island_groups\n\n    def get_flanking_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island flanking CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of flanking CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        flanking_indexes = [ind for ind in locus_cdss_indexes if ind not in self.indexes]\n        flanking_groups = cdss.iloc[flanking_indexes].apply(lambda cds: cds.group).to_list()\n        return flanking_groups\n\n    def get_locus_proteins(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get ids of locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: ids of island locus CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.cds_id).to_list()\n        return locus_groups\n\n    def get_island_proteins(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get ids of island CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Ids of island CDSs.\n\n        \"\"\"\n        island_proteins = cdss.iloc[self.indexes].apply(lambda cds: cds.cds_id).to_list()\n        return island_proteins\n\n    def get_all_locus_indexes(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get indexes of island locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Indexes of island locus CDSs.\n\n        \"\"\"\n        if self.left_cons_neighbours:\n            island_left_border_cds_ind = self.left_cons_neighbours[0]\n        else:\n            island_left_border_cds_ind = self.indexes[0]\n        if self.right_cons_neighbours:\n            island_right_border_cds_ind = self.right_cons_neighbours[-1]\n        else:\n            island_right_border_cds_ind = self.indexes[-1]\n        if island_left_border_cds_ind &lt; island_right_border_cds_ind:\n            island_cdss_indexes = [i for i in range(island_left_border_cds_ind, island_right_border_cds_ind + 1)]\n        else:\n            island_cdss_indexes = [i for i in range(island_left_border_cds_ind, cdss.size)] + \\\n                                  [i for i in range(0, island_right_border_cds_ind + 1)]\n        return island_cdss_indexes\n\n    def calculate_database_hits_stat(self, cdss: pd.Series) -&gt; None:\n        \"\"\"Update statistics of hmmscan search results.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            None\n\n        \"\"\"\n        all_locus_indexes = self.get_all_locus_indexes(cdss)\n        for ind in all_locus_indexes:\n            lind_cds = cdss.iat[ind]\n            lind_cds_hmmscan_res = lind_cds.hmmscan_results\n            if lind_cds_hmmscan_res:\n                if ind not in self.indexes:\n                    self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"flanking\"][lind_cds.group] = \\\n                        lind_cds_hmmscan_res[\"target\"]\n                else:\n                    self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"cargo\"][lind_cds.group] = \\\n                        lind_cds_hmmscan_res[\"target\"]\n        return None\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.__init__","title":"<code>__init__(island_id, proteome, circular_proteome, center, indexes, var_indexes, left_cons_neighbours, right_cons_neighbours, hotspot_id='-', databases_hits_stat=None)</code>","text":"<p>Island class constructor.</p> <p>Parameters:</p> <ul> <li> <code>island_id</code>             (<code>str</code>)         \u2013          <p>Island identifier.</p> </li> <li> <code>proteome</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where island is annotated.</p> </li> <li> <code>circular_proteome</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then first and last genes are considered as neighbours.</p> </li> <li> <code>center</code>             (<code>int</code>)         \u2013          <p>CDS index of the island center.</p> </li> <li> <code>indexes</code>             (<code>list</code>)         \u2013          <p>CDS indexes of the island.</p> </li> <li> <code>var_indexes</code>             (<code>list</code>)         \u2013          <p>Indexes of CDS which g_class is \"variable\".</p> </li> <li> <code>left_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the left.</p> </li> <li> <code>right_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the right.</p> </li> <li> <code>hotspot_id</code>             (<code>str</code>, default:                 <code>'-'</code> )         \u2013          <p>Hotspot id if Island was attributed to one of them or \"-\" value if not.</p> </li> <li> <code>databases_hits_stat</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Statistics from hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, island_id: str, proteome: str,\n             circular_proteome: int, center: int, indexes: list, var_indexes: list,\n             left_cons_neighbours: list, right_cons_neighbours: list,\n             hotspot_id=\"-\", databases_hits_stat: typing.Union[None, dict] = None):\n    \"\"\"Island class constructor.\n\n    Arguments:\n        island_id (str): Island identifier.\n        proteome (str): Proteome identifier where island is annotated.\n        circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n            first and last genes are considered as neighbours.\n        center (int): CDS index of the island center.\n        indexes (list): CDS indexes of the island.\n        var_indexes (list): Indexes of CDS which g_class is \"variable\".\n        left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n        right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n        hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n        databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n    \"\"\"\n    self.island_id = island_id\n    self.proteome = proteome\n    self.circular_proteome = circular_proteome\n    self.hotspot_id = hotspot_id\n    self.center = center\n    self.indexes = indexes\n    self.size = len(indexes)\n    self.var_indexes = var_indexes\n    self.left_cons_neighbours = left_cons_neighbours\n    self.right_cons_neighbours = right_cons_neighbours\n    if (not left_cons_neighbours or not right_cons_neighbours) and not circular_proteome:\n        self.flanked = 0\n    else:\n        self.flanked = 1\n    if databases_hits_stat is None:\n        databases_hits_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n    self.databases_hits_stat = databases_hits_stat\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.calculate_database_hits_stat","title":"<code>calculate_database_hits_stat(cdss)</code>","text":"<p>Update statistics of hmmscan search results.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_database_hits_stat(self, cdss: pd.Series) -&gt; None:\n    \"\"\"Update statistics of hmmscan search results.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        None\n\n    \"\"\"\n    all_locus_indexes = self.get_all_locus_indexes(cdss)\n    for ind in all_locus_indexes:\n        lind_cds = cdss.iat[ind]\n        lind_cds_hmmscan_res = lind_cds.hmmscan_results\n        if lind_cds_hmmscan_res:\n            if ind not in self.indexes:\n                self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"flanking\"][lind_cds.group] = \\\n                    lind_cds_hmmscan_res[\"target\"]\n            else:\n                self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"cargo\"][lind_cds.group] = \\\n                    lind_cds_hmmscan_res[\"target\"]\n    return None\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_all_locus_indexes","title":"<code>get_all_locus_indexes(cdss)</code>","text":"<p>Get indexes of island locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Indexes of island locus CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_all_locus_indexes(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get indexes of island locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Indexes of island locus CDSs.\n\n    \"\"\"\n    if self.left_cons_neighbours:\n        island_left_border_cds_ind = self.left_cons_neighbours[0]\n    else:\n        island_left_border_cds_ind = self.indexes[0]\n    if self.right_cons_neighbours:\n        island_right_border_cds_ind = self.right_cons_neighbours[-1]\n    else:\n        island_right_border_cds_ind = self.indexes[-1]\n    if island_left_border_cds_ind &lt; island_right_border_cds_ind:\n        island_cdss_indexes = [i for i in range(island_left_border_cds_ind, island_right_border_cds_ind + 1)]\n    else:\n        island_cdss_indexes = [i for i in range(island_left_border_cds_ind, cdss.size)] + \\\n                              [i for i in range(0, island_right_border_cds_ind + 1)]\n    return island_cdss_indexes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_cons_neighbours_groups","title":"<code>get_cons_neighbours_groups(cdss)</code>","text":"<p>Get homology group attribute of conserved neighbour CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of conserved neighbours.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_cons_neighbours_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of conserved neighbour CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of conserved neighbours.\n\n    \"\"\"\n    all_cons_neighbours = self.left_cons_neighbours + self.right_cons_neighbours\n    cons_neighbours_groups = cdss.iloc[all_cons_neighbours].apply(lambda cds: cds.group).to_list()\n    return cons_neighbours_groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_flanking_groups","title":"<code>get_flanking_groups(cdss)</code>","text":"<p>Get homology group attribute of island flanking CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of flanking CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_flanking_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island flanking CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of flanking CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    flanking_indexes = [ind for ind in locus_cdss_indexes if ind not in self.indexes]\n    flanking_groups = cdss.iloc[flanking_indexes].apply(lambda cds: cds.group).to_list()\n    return flanking_groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_island_db_row","title":"<code>get_island_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"size\", \"flanked\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_island_groups","title":"<code>get_island_groups(cdss)</code>","text":"<p>Get homology group attribute of island CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of island CDSs.\n\n    \"\"\"\n    island_groups = cdss.iloc[self.indexes].apply(lambda cds: cds.group).to_list()\n    return island_groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_island_proteins","title":"<code>get_island_proteins(cdss)</code>","text":"<p>Get ids of island CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Ids of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_proteins(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get ids of island CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Ids of island CDSs.\n\n    \"\"\"\n    island_proteins = cdss.iloc[self.indexes].apply(lambda cds: cds.cds_id).to_list()\n    return island_proteins\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_locus_groups","title":"<code>get_locus_groups(cdss)</code>","text":"<p>Get homology group attribute of island locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_locus_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of island CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.group).to_list()\n    return locus_groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Island.get_locus_proteins","title":"<code>get_locus_proteins(cdss)</code>","text":"<p>Get ids of locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>ids of island locus CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_locus_proteins(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get ids of locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: ids of island locus CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.cds_id).to_list()\n    return locus_groups\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteome","title":"<code>Proteome</code>","text":"<p>Proteome object represents a particular annotated proteome and its properties</p> <p>Attributes:</p> <ul> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier.</p> </li> <li> <code>gff_file</code>             (<code>str</code>)         \u2013          <p>Path to the corresponding gff file.</p> </li> <li> <code>circular</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then first and last genes are considered as neighbours.</p> </li> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>List of annotated proteins.</p> </li> <li> <code>islands</code>             (<code>Series</code>)         \u2013          <p>Series of annotated islands.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Proteome:\n    \"\"\"Proteome object represents a particular annotated proteome and its properties\n\n    Attributes:\n        proteome_id (str): Proteome identifier.\n        gff_file (str): Path to the corresponding gff file.\n        circular (int): [1 or 0] int value whether locus is circular or not. If genome is circular then first and last\n            genes are considered as neighbours.\n        cdss (pd.Series): List of annotated proteins.\n        islands (pd.Series): Series of annotated islands.\n\n    \"\"\"\n\n    def __init__(self, proteome_id: str, gff_file: str, circular: int = 1, cdss: typing.Union[None, pd.Series] = None,\n                 islands: typing.Union[None, pd.Series] = None):\n        \"\"\"Proteome class constructor.\n\n        Arguments:\n            proteome_id (str): Proteome identifier.\n            gff_file (str): Path to the corresponding gff file.\n            circular (int): [1 or 0] int value whether locus is circular or not.\n            cdss (pd.Series): List of annotated proteins.\n            islands (pd.Series): Series of annotated islands.\n\n        \"\"\"\n        self.proteome_id = proteome_id\n        self.gff_file = gff_file\n        self.cdss = cdss\n        self.circular = circular\n        self.islands = islands\n\n    def get_proteome_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: modified object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"prms\", \"cdss\", \"islands\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"gff_file\"] = os.path.basename(attributes[\"gff_file\"])\n        attributes[\"cdss\"] = self.cdss.apply(lambda cds: cds.cds_id).to_list()\n        attributes[\"islands\"] = self.islands.apply(lambda island: island.island_id).to_list()\n        return attributes\n\n    def annotate_variable_islands(self, prms: ilund4u.manager.Parameters) -&gt; None:\n        \"\"\"Annotate proteome variable islands defined as a region with a set of non-conserved proteins.\n\n        Arguments:\n            prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            proteome_size = len(self.cdss.index)\n            proteome_cds_classes = self.cdss.apply(lambda cds: cds.g_class).to_list()\n            proteome_not_conserved_indexes = [ind for ind in range(proteome_size) if\n                                              proteome_cds_classes[ind] != \"conserved\"]\n            proteome_conserved_indexes = [ind for ind in range(proteome_size) if\n                                          proteome_cds_classes[ind] == \"conserved\"]\n            n_conserved_proteins = len(proteome_conserved_indexes)\n            var_regions, cur_region = [], []\n            self.islands = pd.Series()\n            for pnci in proteome_not_conserved_indexes:\n                if cur_region:\n                    if pnci == cur_region[-1] + 1:\n                        cur_region.append(pnci)\n                    else:\n                        var_regions.append(cur_region)\n                        cur_region = [pnci]\n                else:\n                    cur_region.append(pnci)\n                if pnci == proteome_not_conserved_indexes[-1] and cur_region:\n                    var_regions.append(cur_region)\n            if not var_regions:\n                return None\n            if self.circular:\n                if var_regions[0][0] == 0 and var_regions[-1][-1] == proteome_size - 1:\n                    last_region = var_regions.pop()\n                    var_regions[0] = last_region + var_regions[0]\n            proteome_islands_l = []\n            for region in var_regions:\n                var_indexes = [ind for ind in region if proteome_cds_classes[ind] == \"variable\"]\n                if not var_indexes:\n                    continue\n                left_border, right_border = region[0], region[-1]\n                left_cons_neighbours, right_cons_neighbours = [], []\n                for dist in range(1, min(prms.args[\"neighbours_max_distance\"], n_conserved_proteins // 2) + 1):\n                    if self.circular:\n                        left_index = (left_border - dist) % proteome_size\n                        right_index = (right_border + dist) % proteome_size\n                    else:\n                        left_index = left_border - dist\n                        right_index = right_border + dist\n                    if left_index in proteome_conserved_indexes and \\\n                            len(left_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                        left_cons_neighbours.append(left_index)\n                    if right_index in proteome_conserved_indexes and \\\n                            len(right_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                        right_cons_neighbours.append(right_index)\n                left_cons_neighbours = left_cons_neighbours[::-1]\n                cons_neighbours = left_cons_neighbours + right_cons_neighbours\n                if len(set(cons_neighbours)) &lt; prms.args[\"neighbours_min_size\"]:\n                    continue\n                island_size = len(var_indexes)\n                island_center = var_indexes[int(island_size / 2)]\n                island_id = f\"{self.proteome_id}:{island_center}\"\n                island = Island(island_id=island_id, proteome=self.proteome_id, circular_proteome=self.circular,\n                                center=island_center, indexes=region, var_indexes=var_indexes,\n                                left_cons_neighbours=left_cons_neighbours, right_cons_neighbours=right_cons_neighbours)\n                proteome_islands_l.append(island)\n            self.islands = pd.Series(proteome_islands_l, index=[pi.island_id for pi in proteome_islands_l])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to annotate variable islands for {self.proteome_id}\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteome.__init__","title":"<code>__init__(proteome_id, gff_file, circular=1, cdss=None, islands=None)</code>","text":"<p>Proteome class constructor.</p> <p>Parameters:</p> <ul> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier.</p> </li> <li> <code>gff_file</code>             (<code>str</code>)         \u2013          <p>Path to the corresponding gff file.</p> </li> <li> <code>circular</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>[1 or 0] int value whether locus is circular or not.</p> </li> <li> <code>cdss</code>             (<code>Series</code>, default:                 <code>None</code> )         \u2013          <p>List of annotated proteins.</p> </li> <li> <code>islands</code>             (<code>Series</code>, default:                 <code>None</code> )         \u2013          <p>Series of annotated islands.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, proteome_id: str, gff_file: str, circular: int = 1, cdss: typing.Union[None, pd.Series] = None,\n             islands: typing.Union[None, pd.Series] = None):\n    \"\"\"Proteome class constructor.\n\n    Arguments:\n        proteome_id (str): Proteome identifier.\n        gff_file (str): Path to the corresponding gff file.\n        circular (int): [1 or 0] int value whether locus is circular or not.\n        cdss (pd.Series): List of annotated proteins.\n        islands (pd.Series): Series of annotated islands.\n\n    \"\"\"\n    self.proteome_id = proteome_id\n    self.gff_file = gff_file\n    self.cdss = cdss\n    self.circular = circular\n    self.islands = islands\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteome.annotate_variable_islands","title":"<code>annotate_variable_islands(prms)</code>","text":"<p>Annotate proteome variable islands defined as a region with a set of non-conserved proteins.</p> <p>Parameters:</p> <ul> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def annotate_variable_islands(self, prms: ilund4u.manager.Parameters) -&gt; None:\n    \"\"\"Annotate proteome variable islands defined as a region with a set of non-conserved proteins.\n\n    Arguments:\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        proteome_size = len(self.cdss.index)\n        proteome_cds_classes = self.cdss.apply(lambda cds: cds.g_class).to_list()\n        proteome_not_conserved_indexes = [ind for ind in range(proteome_size) if\n                                          proteome_cds_classes[ind] != \"conserved\"]\n        proteome_conserved_indexes = [ind for ind in range(proteome_size) if\n                                      proteome_cds_classes[ind] == \"conserved\"]\n        n_conserved_proteins = len(proteome_conserved_indexes)\n        var_regions, cur_region = [], []\n        self.islands = pd.Series()\n        for pnci in proteome_not_conserved_indexes:\n            if cur_region:\n                if pnci == cur_region[-1] + 1:\n                    cur_region.append(pnci)\n                else:\n                    var_regions.append(cur_region)\n                    cur_region = [pnci]\n            else:\n                cur_region.append(pnci)\n            if pnci == proteome_not_conserved_indexes[-1] and cur_region:\n                var_regions.append(cur_region)\n        if not var_regions:\n            return None\n        if self.circular:\n            if var_regions[0][0] == 0 and var_regions[-1][-1] == proteome_size - 1:\n                last_region = var_regions.pop()\n                var_regions[0] = last_region + var_regions[0]\n        proteome_islands_l = []\n        for region in var_regions:\n            var_indexes = [ind for ind in region if proteome_cds_classes[ind] == \"variable\"]\n            if not var_indexes:\n                continue\n            left_border, right_border = region[0], region[-1]\n            left_cons_neighbours, right_cons_neighbours = [], []\n            for dist in range(1, min(prms.args[\"neighbours_max_distance\"], n_conserved_proteins // 2) + 1):\n                if self.circular:\n                    left_index = (left_border - dist) % proteome_size\n                    right_index = (right_border + dist) % proteome_size\n                else:\n                    left_index = left_border - dist\n                    right_index = right_border + dist\n                if left_index in proteome_conserved_indexes and \\\n                        len(left_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                    left_cons_neighbours.append(left_index)\n                if right_index in proteome_conserved_indexes and \\\n                        len(right_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                    right_cons_neighbours.append(right_index)\n            left_cons_neighbours = left_cons_neighbours[::-1]\n            cons_neighbours = left_cons_neighbours + right_cons_neighbours\n            if len(set(cons_neighbours)) &lt; prms.args[\"neighbours_min_size\"]:\n                continue\n            island_size = len(var_indexes)\n            island_center = var_indexes[int(island_size / 2)]\n            island_id = f\"{self.proteome_id}:{island_center}\"\n            island = Island(island_id=island_id, proteome=self.proteome_id, circular_proteome=self.circular,\n                            center=island_center, indexes=region, var_indexes=var_indexes,\n                            left_cons_neighbours=left_cons_neighbours, right_cons_neighbours=right_cons_neighbours)\n            proteome_islands_l.append(island)\n        self.islands = pd.Series(proteome_islands_l, index=[pi.island_id for pi in proteome_islands_l])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to annotate variable islands for {self.proteome_id}\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteome.get_proteome_db_row","title":"<code>get_proteome_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>modified object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_proteome_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: modified object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"prms\", \"cdss\", \"islands\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    attributes[\"gff_file\"] = os.path.basename(attributes[\"gff_file\"])\n    attributes[\"cdss\"] = self.cdss.apply(lambda cds: cds.cds_id).to_list()\n    attributes[\"islands\"] = self.islands.apply(lambda island: island.island_id).to_list()\n    return attributes\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes","title":"<code>Proteomes</code>","text":"<p>Proteomes object represents a set of annotated genomes.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Proteome objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of proteomes.</p> </li> <li> <code>seq_to_ind</code>             (<code>dict</code>)         \u2013          <p>Dictionary with proteome id to proteome index pairs.</p> </li> <li> <code>communities</code>             (<code>dict</code>)         \u2013          <p>Dictionary representing annotated communities of proteomes (key - community_id, value - list of proteome ids)</p> </li> <li> <code>communities_annot</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of proteome communities.</p> </li> <li> <code>proteins_fasta_file</code>             (<code>str</code>)         \u2013          <p>Path to a fasta file containing all protein sequences.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Proteomes:\n    \"\"\"Proteomes object represents a set of annotated genomes.\n\n    Attributes:\n        proteomes (pd.Series): Series (list) of Proteome objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of proteomes.\n        seq_to_ind (dict): Dictionary with proteome id to proteome index pairs.\n        communities (dict): Dictionary representing annotated communities of proteomes\n            (key - community_id, value - list of proteome ids)\n        communities_annot (pd.DataFrame): Annotation table of proteome communities.\n        proteins_fasta_file (str): Path to a fasta file containing all protein sequences.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, parameters: ilund4u.manager.Parameters):\n        \"\"\"Proteomes class constructor.\n\n        Arguments:\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = pd.Series()\n        self.annotation = None\n        self.__col_to_ind = None\n        self.seq_to_ind = None\n        self.communities = dict()\n        self.communities_annot = None\n        self.proteins_fasta_file = os.path.join(parameters.args[\"output_dir\"], \"all_proteins.fa\")\n        self.prms = parameters\n\n    def save_as_db(self, db_folder: str) -&gt; None:\n        \"\"\"Save Proteomes to the iLnd4u database.\n\n        Arguments:\n            db_folder (str): Database folder path.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            attributes_to_ignore = [\"proteomes\", \"annotation\", \"communities_annot\", \"prms\"]\n            attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n            attributes[\"proteins_fasta_file\"] = os.path.basename(attributes[\"proteins_fasta_file\"])\n            with open(os.path.join(db_folder, \"proteomes.attributes.json\"), 'w') as json_file:\n                json.dump(attributes, json_file)\n            self.communities_annot.to_csv(os.path.join(db_folder, \"proteomes.communities_annot.tsv\"), sep=\"\\t\",\n                                          index_label=\"id\")\n            self.annotation[\"protein_clusters\"] = self.annotation[\"protein_clusters\"].apply(lambda x: \";\".join(x))\n            self.annotation.to_csv(os.path.join(db_folder, \"proteomes.annotations.tsv\"), sep=\"\\t\",\n                                   index_label=\"proteome_id\")\n            os.mkdir(os.path.join(db_folder, \"gff\"))\n            proteome_db_ind, cdss_db_ind, islands_db_ind, cds_ids, repr_cds_ids = [], [], [], [], set()\n            for community, proteomes in self.communities.items():\n                for proteome_id in proteomes:\n                    proteome = self.proteomes.at[proteome_id]\n                    proteome_db_ind.append(proteome.get_proteome_db_row())\n                    os.system(f\"cp '{proteome.gff_file}' {os.path.join(db_folder, 'gff')}/\")\n                    for cds in proteome.cdss.to_list():\n                        cds_ids.append(cds.cds_id)\n                        cdss_db_ind.append(cds.get_cds_db_row())\n                        repr_cds_ids.add(cds.group)\n                    for island in proteome.islands.to_list():\n                        islands_db_ind.append(island.get_island_db_row())\n\n            with open(os.path.join(db_folder, \"proteome.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(proteome_db_ind, json_file)\n            with open(os.path.join(db_folder, \"cds.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(cdss_db_ind, json_file)\n            with open(os.path.join(db_folder, \"island.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(islands_db_ind, json_file)\n\n            initial_fasta_file = Bio.SeqIO.index(self.proteins_fasta_file, \"fasta\")\n            with open(os.path.join(db_folder, attributes[\"proteins_fasta_file\"]), \"wb\") as out_handle:\n                for acc in cds_ids:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n\n            with open(os.path.join(db_folder, \"representative_seqs.fa\"), \"wb\") as out_handle:\n                for acc in repr_cds_ids:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n\n            mmseqs_db_folder = os.path.join(db_folder, \"mmseqs_db\")\n            if os.path.exists(mmseqs_db_folder):\n                shutil.rmtree(mmseqs_db_folder)\n            os.mkdir(mmseqs_db_folder)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\",\n                            os.path.join(db_folder, attributes[\"proteins_fasta_file\"]),\n                            os.path.join(mmseqs_db_folder, \"all_proteins\")],\n                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to write Proteomes to the database.\") from error\n\n    @classmethod\n    def db_init(cls, db_path: str, parameters: ilund4u.manager.Parameters):\n        \"\"\"Class method to load a Proteomes object from a database.\n\n        Arguments:\n            db_path (str): path to the database.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            cls: Proteomes object.\n\n        \"\"\"\n        try:\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading cds objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"cds.ind.attributes.json\"), \"r\") as json_file:\n                cds_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(cds_ind_attributes), suffix='%(index)d/%(max)d')\n            cds_list = []\n            for cds_dict in cds_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                cds_list.append(CDS(**cds_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            cdss = pd.Series(cds_list, index=[cds.cds_id for cds in cds_list])\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading island objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"island.ind.attributes.json\"), \"r\") as json_file:\n                island_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(island_ind_attributes), suffix='%(index)d/%(max)d')\n            island_list = []\n            for proteome_dict in island_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                island_list.append(Island(**proteome_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            islands = pd.Series(island_list, index=[island.island_id for island in island_list])\n\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading proteome objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"proteome.ind.attributes.json\"), \"r\") as json_file:\n                proteome_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(proteome_ind_attributes), suffix='%(index)d/%(max)d')\n            proteome_list = []\n            for proteome_dict in proteome_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                proteome_dict[\"gff_file\"] = os.path.join(db_path, \"gff\", proteome_dict[\"gff_file\"])\n                proteome_dict[\"cdss\"] = cdss.loc[proteome_dict[\"cdss\"]]\n                proteome_dict[\"islands\"] = islands.loc[proteome_dict[\"islands\"]]\n                proteome_list.append(Proteome(**proteome_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            proteomes = pd.Series(proteome_list, index=[proteome.proteome_id for proteome in proteome_list])\n\n            cls_obj = cls(parameters)\n            cls_obj.proteomes = proteomes\n            with open(os.path.join(db_path, \"proteomes.attributes.json\"), \"r\") as json_file:\n                proteomes_attributes = json.load(json_file)\n            cls_obj.communities = {int(k): v for k, v in proteomes_attributes[\"communities\"].items()}\n\n            cls_obj.proteins_fasta_file = os.path.join(db_path, proteomes_attributes[\"proteins_fasta_file\"])\n\n            cls_obj.annotation = pd.read_table(os.path.join(db_path, \"proteomes.annotations.tsv\"),\n                                               sep=\"\\t\").set_index(\"proteome_id\")\n            cls_obj.__col_to_ind = {col: idx for idx, col in enumerate(cls_obj.annotation.columns)}\n            cls_obj.communities_annot = pd.read_table(os.path.join(db_path, \"proteomes.communities_annot.tsv\"),\n                                                      sep=\"\\t\").set_index(\"id\")\n            cls_obj.seq_to_ind = {sid: idx for idx, sid in enumerate(cls_obj.annotation.index)}\n\n            return cls_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to read Proteomes from the database.\") from error\n\n    def load_sequences_from_extended_gff(self, input_f: typing.Union[str, list], genome_annotation=None) -&gt; None:\n        \"\"\"Load proteomes from gff files.\n\n        Arguments:\n            input_f (str | list): List of file paths or path to a folder with gff files.\n            genome_annotation (path): Path to a table with annotation of genome circularity.\n                Format: two columns with names: id, circular; tab-separated, 1,0 values.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if isinstance(input_f, str):\n                input_folder = input_f\n                if not os.path.exists(input_folder):\n                    raise ilund4u.manager.ilund4uError(f\"Folder {input_folder} does not exist.\")\n                gff_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder)]\n            elif isinstance(input_f, list):\n                gff_files = input_f\n            else:\n                raise ilund4u.manager.ilund4uError(f\"The input for the GFF parsing function must be either a folder or \"\n                                                   f\"a list of files.\")\n            if not gff_files:\n                raise ilund4u.manager.ilund4uError(f\"Folder {input_f} does not contain files.\")\n            if not os.path.exists(self.prms.args[\"output_dir\"]):\n                os.mkdir(self.prms.args[\"output_dir\"])\n            else:\n                if os.path.exists(self.proteins_fasta_file):\n                    os.remove(self.proteins_fasta_file)\n            genome_circularity_dict = dict()\n            if genome_annotation:\n                try:\n                    genome_annotation_table = pd.read_table(genome_annotation, sep=\"\\t\").set_index(\"id\")\n                    genome_circularity_dict = genome_annotation_table[\"circular\"].to_dict()\n                except:\n                    raise ilund4u.manager.ilund4uError(\"\u25cb Warning: unable to read genome annotation table. \"\n                                                       \"Check the format.\")\n            num_of_gff_files = len(gff_files)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Reading gff file{'s' if len(gff_files) &gt; 1 else ''}...\", file=sys.stdout)\n            if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=num_of_gff_files, suffix='%(index)d/%(max)d')\n            proteome_list, annotation_rows = [], []\n            gff_records_batch = []\n            for gff_file_index, gff_file_path in enumerate(gff_files):\n                try:\n                    if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                        bar.next()\n                    gff_records = list(BCBio.GFF.parse(gff_file_path, limit_info=dict(gff_type=[\"CDS\"])))\n                    if len(gff_records) != 1:\n                        print(f\"\\n\u25cb Warning: gff file {gff_file_path} contains information for more than 1 \"\n                              f\"sequence. File will be skipped.\")\n                        continue\n                    current_gff_records = []\n                    gff_record = gff_records[0]\n                    try:\n                        record_locus_sequence = gff_record.seq\n                    except Bio.Seq.UndefinedSequenceError as error:\n                        raise ilund4u.manager.ilund4uError(f\"gff file doesn't contain corresponding \"\n                                                           f\"sequences.\") from error\n                    if self.prms.args[\"use_filename_as_contig_id\"]:\n                        gff_record.id = os.path.splitext(os.path.basename(gff_file_path))[0]\n                    features_ids = [i.id for i in gff_record.features]\n                    if len(features_ids) != len(set(features_ids)):\n                        raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains duplicated feature \"\n                                                           f\"ids while only unique are allowed.\")\n                    if len(features_ids) &gt; self.prms.args[\"min_proteome_size\"]:\n                        if gff_record.id in genome_circularity_dict.keys():\n                            circular = int(genome_circularity_dict[gff_record.id])\n                        else:\n                            circular = int(self.prms.args[\"circular_genomes\"])\n                        record_proteome = Proteome(proteome_id=gff_record.id, gff_file=gff_file_path, cdss=pd.Series(),\n                                                   circular=circular)\n                        record_cdss = []\n                        all_defined = True\n                        for gff_feature in gff_record.features:\n                            cds_id = gff_feature.id.replace(\";\", \",\")\n                            if gff_record.id not in cds_id:\n                                cds_id = f\"{gff_record.id}-{cds_id}\"  # Attention\n                            transl_table = self.prms.args[\"default_transl_table\"]\n                            if \"transl_table\" in gff_feature.qualifiers.keys():\n                                transl_table = int(gff_feature.qualifiers[\"transl_table\"][0])\n                            name = \"\"\n                            if self.prms.args[\"gff_CDS_name_source\"] in gff_feature.qualifiers:\n                                name = gff_feature.qualifiers[self.prms.args[\"gff_CDS_name_source\"]][0]\n\n                            sequence = gff_feature.translate(record_locus_sequence, table=transl_table, cds=False)[:-1]\n\n                            if not sequence.defined:\n                                all_defined = False\n                                continue\n\n                            current_gff_records.append(Bio.SeqRecord.SeqRecord(seq=sequence, id=cds_id, description=\"\"))\n                            cds = CDS(cds_id=cds_id, proteome_id=gff_record.id,\n                                      start=int(gff_feature.location.start) + 1, end=int(gff_feature.location.end),\n                                      strand=gff_feature.location.strand, name=name)\n                            record_cdss.append(cds)\n                        if all_defined:\n                            gff_records_batch += current_gff_records\n                            record_proteome.cdss = pd.Series(record_cdss, index=[cds.cds_id for cds in record_cdss])\n                            proteome_list.append(record_proteome)\n                            annotation_rows.append(dict(id=gff_record.id, length=len(gff_record.seq),\n                                                        proteome_size=len(features_ids),\n                                                        proteome_size_unique=\"\", protein_clusters=\"\"))\n                        else:\n                            raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains not defined feature\")\n                    if gff_file_index % 1000 == 0 or gff_file_index == num_of_gff_files - 1:\n                        with open(self.proteins_fasta_file, \"a\") as handle:\n                            Bio.SeqIO.write(gff_records_batch, handle, \"fasta\")\n                        gff_records_batch = []\n                except:\n                    print(f\"\u25cb Warning: gff file {gff_file_path} was not read properly and skipped\")\n                    if self.prms.args[\"parsing_debug\"]:\n                        self.prms.args[\"debug\"] = True\n                        raise ilund4u.manager.ilund4uError(\"Gff file {gff_file_path} was not read properly\")\n            if len(gff_files) &gt; 1 and self.prms.args[\"verbose\"]:\n                bar.finish()\n            proteome_ids = [pr.proteome_id for pr in proteome_list]\n            if len(proteome_ids) != len(set(proteome_ids)):\n                raise ilund4u.manager.ilund4uError(f\"The input gff files have duplicated contig ids.\\n  \"\n                                                   f\"You can use `--use-filename-as-id` parameter to use file name \"\n                                                   f\"as contig id which can help to fix the problem.\")\n            self.proteomes = pd.Series(proteome_list, index=[pr.proteome_id for pr in proteome_list])\n            self.annotation = pd.DataFrame(annotation_rows).set_index(\"id\")\n            self.__col_to_ind = {col: idx for idx, col in enumerate(self.annotation.columns)}\n            self.annotation = self.annotation.sort_values(by=\"proteome_size\")\n            self.proteomes = self.proteomes.loc[self.annotation.index]\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(proteome_list)} {'locus was' if len(proteome_list) == 1 else 'loci were'} loaded from\"\n                      f\" the gff files folder\", file=sys.stdout)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to load proteomes from gff files.\") from error\n\n    def mmseqs_cluster(self) -&gt; dict:\n        \"\"\"Cluster all proteins using mmseqs in order to define groups of homologues.\n\n        Returns:\n            dict: protein id to cluster id dictionary.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Running mmseqs for protein clustering...\", file=sys.stdout)\n            mmseqs_input = self.proteins_fasta_file\n            mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n            if os.path.exists(mmseqs_output_folder):\n                shutil.rmtree(mmseqs_output_folder)\n            os.mkdir(mmseqs_output_folder)\n            mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DB\")\n            os.mkdir(mmseqs_output_folder_db)\n            mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n            mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", mmseqs_input,\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\")], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"cluster\",\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"tmp\"),\n                            \"--cluster-mode\", str(self.prms.args[\"mmseqs_cluster_mode\"]),\n                            \"--cov-mode\", str(self.prms.args[\"mmseqs_cov_mode\"]),\n                            \"--min-seq-id\", str(self.prms.args[\"mmseqs_min_seq_id\"]),\n                            \"-c\", str(self.prms.args[\"mmseqs_c\"]),\n                            \"-s\", str(self.prms.args[\"mmseqs_s\"])], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)  # threads!\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createtsv\",\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                            os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\")],\n                           stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            mmseqs_clustering_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\"),\n                                                      sep=\"\\t\", header=None, names=[\"cluster\", \"protein_id\"])\n            mmseqs_clustering_results = mmseqs_clustering_results.set_index(\"protein_id\")[\"cluster\"].to_dict()\n            num_of_unique_clusters = len(set(mmseqs_clustering_results.values()))\n            num_of_proteins = len(mmseqs_clustering_results.keys())\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {num_of_unique_clusters} clusters for {num_of_proteins} proteins were found with mmseqs\\n\"\n                      f\"  \u29bf mmseqs clustering results were saved to \"\n                      f\"{os.path.join(mmseqs_output_folder, 'mmseqs_clustering.tsv')}\", file=sys.stdout)\n            return mmseqs_clustering_results\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs clustering.\") from error\n\n    def process_mmseqs_results(self, mmseqs_results: dict) -&gt; dict:\n        \"\"\"Process results of mmseqs clustering run.\n\n        Arguments:\n            mmseqs_results (dict): results of mmseqs_cluster function.\n\n        Returns:\n            dict: dictionary with protein cluster id to list of protein ids items.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Processing mmseqs results ...\", file=sys.stdout)\n            sequences_to_drop, drop_reason = [], []\n            current_p_length, cpl_added_proteomes = None, None\n            cluster_to_sequences = collections.defaultdict(list)\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix='%(index)d/%(max)d')\n            for p_index, proteome in enumerate(self.proteomes.to_list()):\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                seq_p_size = self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size\"]]\n                if seq_p_size != current_p_length:\n                    current_p_length = seq_p_size\n                    cpl_added_proteomes = []\n                seq_protein_clusters = []\n                for cds in proteome.cdss.to_list():\n                    cds.group = mmseqs_results[cds.cds_id]\n                    seq_protein_clusters.append(cds.group)\n                seq_protein_clusters_set = set(seq_protein_clusters)\n                unique_p_size = len(seq_protein_clusters_set)\n                if seq_protein_clusters_set in cpl_added_proteomes:\n                    sequences_to_drop.append(proteome.proteome_id)\n                    drop_reason.append(f\"Duplicate\")\n                    continue\n                if unique_p_size / seq_p_size &lt; self.prms.args[\"proteome_uniqueness_cutoff\"]:\n                    sequences_to_drop.append(proteome.proteome_id)\n                    drop_reason.append(\"Proteome uniqueness cutoff\")\n                    continue\n                self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size_unique\"]] = unique_p_size\n                self.annotation.iat[p_index, self.__col_to_ind[\"protein_clusters\"]] = list(seq_protein_clusters_set)\n                cpl_added_proteomes.append(seq_protein_clusters_set)\n                for p_cluster in seq_protein_clusters_set:\n                    cluster_to_sequences[p_cluster].append(proteome.proteome_id)\n            dropped_sequences = pd.DataFrame(dict(sequence=sequences_to_drop, reason=drop_reason))\n            dropped_sequences.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"dropped_sequences.tsv\"), sep=\"\\t\",\n                                     index=False)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n                print(f\"  \u29bf {len(sequences_to_drop)} proteomes were excluded after proteome\"\n                      f\" deduplication and filtering\", file=sys.stdout)\n            self.annotation = self.annotation.drop(sequences_to_drop)\n            self.proteomes = self.proteomes.drop(sequences_to_drop)\n            self.annotation = self.annotation.sort_values(by=\"proteome_size_unique\")\n            self.proteomes = self.proteomes.loc[self.annotation.index]\n            self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n            self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n            return cluster_to_sequences\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to process mmseqs output.\") from error\n\n    def build_proteome_network(self, cluster_to_sequences: dict) -&gt; igraph.Graph:\n        \"\"\"Build proteome network where each proteome represented by node and weighted edges between nodes -\n            fraction of shared homologues.\n\n        Arguments:\n            cluster_to_sequences (dict): cluster id to list of proteins dictionary\n                (results of process_mmseqs_results() function)\n\n        Returns:\n            igraph.Graph: proteome network.\n\n        \"\"\"\n        try:\n            cluster_to_proteome_index = dict()\n            for cluster, sequences in cluster_to_sequences.items():\n                indexes = sorted([self.seq_to_ind[seq_id] for seq_id in sequences])\n                cluster_to_proteome_index[cluster] = collections.deque(indexes)\n\n            proteome_sizes = self.annotation[[\"proteome_size_unique\", \"index\"]]\n            first_index_for_size = proteome_sizes.groupby(\"proteome_size_unique\").tail(1).copy()\n            max_p_size = first_index_for_size[\"proteome_size_unique\"].max()\n            cut_off_mult = 1 / self.prms.args[\"proteome_similarity_cutoff\"]\n            first_index_for_size[\"cutoff\"] = first_index_for_size[\"proteome_size_unique\"].apply(\n                lambda size: first_index_for_size[first_index_for_size[\"proteome_size_unique\"] &gt;=\n                                                  min(size * cut_off_mult, max_p_size)][\"index\"].min() + 1)\n            upper_index_cutoff = first_index_for_size.set_index(\"proteome_size_unique\")[\"cutoff\"]\n            proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Proteomes network construction...\", file=sys.stdout)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix=\"%(index)d/%(max)d\")\n            stime = time.time()\n            edges, weights = [], []\n            for i in range(len(self.proteomes.index)):\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                clusters_i = self.annotation.iat[i, self.__col_to_ind[\"protein_clusters\"]]\n                size_i = self.annotation.iat[i, self.__col_to_ind[\"proteome_size_unique\"]]\n                counts_i = collections.defaultdict(int)\n                upper_i_cutoff = upper_index_cutoff.at[size_i]\n                for cl in clusters_i:\n                    js = cluster_to_proteome_index[cl]\n                    for j in js.copy():\n                        if i &lt; j &lt; upper_i_cutoff:\n                            counts_i[j] += 1\n                        elif j &lt;= i:\n                            js.popleft()\n                        else:\n                            break\n                weights_i = pd.Series(counts_i)\n                proteome_sizes_connected = proteome_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(\n                    0.5 * (size_i + proteome_sizes_connected) / (size_i * proteome_sizes_connected), \\\n                    index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            etime = time.time()\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Network building elapsed time: {round(etime - stime, 2)} sec\")\n            graph = igraph.Graph(len(self.proteomes.index), edges, directed=False)\n            graph.vs[\"index\"] = self.annotation[\"index\"].to_list()\n            graph.vs[\"sequence_id\"] = self.annotation.index.to_list()\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(self.prms.args[\"output_dir\"], \"proteome_network.gml\"))\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Proteomes network with {len(edges)} connections was built\\n\"\n                      f\"  \u29bf Network was saved as {os.path.join(self.prms.args['output_dir'], 'proteome_network.gml')}\",\n                      file=sys.stdout)\n            return graph\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to built proteome network.\") from error\n\n    def find_proteome_communities(self, graph: igraph.Graph) -&gt; None:\n        \"\"\"Find proteome communities using Leiden algorithm and update communities attribute.\n\n        Arguments:\n            graph (igraph.Graph): network of proteomes obtained by build_proteome_network function.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Proteome network partitioning using the Leiden algorithm...\")\n            partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_p\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(set(partition_leiden.membership))} proteome communities were found\")\n            communities_annot_rows = []\n            sequences_to_drop = []\n            for community_index, community in enumerate(partition_leiden):\n                community_size = len(community)\n                subgraph = graph.subgraph(community)\n                proteomes = subgraph.vs[\"sequence_id\"]\n                if community_size &gt;= self.prms.args[\"min_proteome_community_size\"]:\n                    self.communities[community_index] = proteomes\n                else:\n                    sequences_to_drop += proteomes\n                if community_size &gt; 1:\n                    subgraph_edges = subgraph.get_edgelist()\n                    num_of_edges = len(subgraph_edges)\n                    num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                    weights = subgraph.es[\"weight\"]\n                    avg_weight = round(np.mean(weights), 3)\n                    max_identity = max(weights)\n                else:\n                    num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n                communities_annot_rows.append([community_index, community_size, avg_weight, max_identity,\n                                               num_of_edges_fr, num_of_edges, \";\".join(proteomes)])\n            communities_annot = pd.DataFrame(communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\", \"max_weight\",\n                                                                              \"fr_edges\", \"n_edges\", \"proteomes\"])\n            communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                               \"proteome_communities.tsv\")), sep=\"\\t\", index=False)\n            communities_annot = communities_annot[communities_annot[\"size\"] &gt;=\n                                                  self.prms.args[\"min_proteome_community_size\"]]\n            self.communities_annot = communities_annot.set_index(\"id\")\n            self.annotation = self.annotation.drop(sequences_to_drop)\n            self.proteomes = self.proteomes.drop(sequences_to_drop)\n            self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n            self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(communities_annot.index)} proteomes communities with size &gt;= \"\n                      f\"{self.prms.args['min_proteome_community_size']} were taken for further analysis\",\n                      file=sys.stdout)\n                print(f\"  \u29bf {len(sequences_to_drop)} proteomes from smaller communities were excluded from the \"\n                      f\"analysis\", file=sys.stdout)\n            if len(communities_annot.index) == 0:\n                print(\"\u25cb Termination since no proteome community was taken for further analysis\")\n                sys.exit()\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to find proteome communities.\") from error\n\n    def define_protein_classes(self) -&gt; None:\n        \"\"\"Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Defining protein classes within each community...\")\n            number_of_communities = len(self.communities_annot.index)\n            protein_classes_trows = []\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix='%(index)d/%(max)d')\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_size = len(com_pr_ids)\n                com_annotation = self.annotation.loc[com_pr_ids]\n                com_protein_clusters = com_annotation[\"protein_clusters\"].sum()\n                com_protein_clusters_count = collections.Counter(com_protein_clusters)\n                com_protein_classes = dict()\n                for pc, counts in com_protein_clusters_count.items():\n                    pc_fraction = counts / com_size\n                    if pc_fraction &lt; self.prms.args[\"variable_protein_cluster_cutoff\"]:\n                        pc_class = \"variable\"\n                    elif pc_fraction &gt; self.prms.args[\"conserved_protein_cluster_cutoff\"]:\n                        pc_class = \"conserved\"\n                    else:\n                        pc_class = \"intermediate\"\n                    com_protein_classes[pc] = pc_class\n                    protein_classes_trows.append(dict(community=com_id, community_size=com_size, protein_group=pc,\n                                                      protein_group_class=pc_class, fraction=round(pc_fraction, 3),\n                                                      protein_group_counts=counts))\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                for com_proteome in com_proteomes:\n                    for cds in com_proteome.cdss:\n                        cds.g_class = com_protein_classes[cds.group]\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            protein_classes_t = pd.DataFrame(protein_classes_trows)\n            protein_classes_t.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_classes.tsv\"), sep=\"\\t\",\n                                     index=False)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to define protein classes.\") from error\n\n    def annotate_variable_islands(self) -&gt; None:\n        \"\"\"Annotate variable islands defined as a region with a set of non-conserved proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Annotating variable islands within each proteome...\", file=sys.stdout)\n            total_number_of_variable_regions = 0\n            for proteome_index, proteome in enumerate(self.proteomes):\n                proteome.annotate_variable_islands(self.prms)\n                total_number_of_variable_regions += proteome.islands.size\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {total_number_of_variable_regions} variable regions are annotated in \"\n                      f\"{len(self.proteomes.index)} proteomes \"\n                      f\"({round(total_number_of_variable_regions / len(self.proteomes.index), 3)} per proteome)\",\n                      file=sys.stdout)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to annotate variable islands.\") from error\n\n    def build_islands_network(self) -&gt; igraph.Graph:\n        \"\"\"Build island network where each node - an island and weighted edges - fraction of shared\n            conserved neighbours homologues.\n\n        Returns:\n            igraph.Graph: Island network.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Island network construction within each proteome community...\")\n            output_network_folder = os.path.join(self.prms.args[\"output_dir\"], \"island_networks\")\n            if os.path.exists(output_network_folder):\n                shutil.rmtree(output_network_folder)\n            os.mkdir(output_network_folder)\n            number_of_communities = len(self.communities_annot.index)\n            stime = time.time()\n            networks = []\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                com_island_n_sizes = pd.Series()\n                com_neighbours = pd.Series()\n                cluster_to_island = collections.defaultdict(collections.deque)\n\n                islands_list = [island for proteome in com_proteomes.to_list() for island in proteome.islands.to_list()]\n                island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(islands_list)}\n                for proteome in com_proteomes:\n                    for island in proteome.islands.to_list():\n                        island_id = island.island_id\n                        island_index = island_id_to_index[island_id]\n                        conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(proteome.cdss))\n                        com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                        com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                        for cing in conserved_island_neighbours_groups:\n                            cluster_to_island[cing].append(island_index)\n                edges, weights = [], []\n\n                for i in range(len(com_island_n_sizes.index)):\n                    neighbours_i = com_neighbours.iat[i]\n                    size_i = com_island_n_sizes.iat[i]\n                    counts_i = collections.defaultdict(int)\n                    for ncl in neighbours_i:\n                        js = cluster_to_island[ncl]\n                        for j in js.copy():\n                            if i &lt; j:\n                                counts_i[j] += 1\n                            else:\n                                js.popleft()\n                    weights_i = pd.Series(counts_i)\n                    connected_n_sizes = com_island_n_sizes.iloc[weights_i.index]\n                    norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                              index=weights_i.index)\n                    weights_i = weights_i.mul(norm_factor_i)\n                    weights_i = weights_i[weights_i &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                    for j, w in weights_i.items():\n                        edges.append([i, j])\n                        weights.append(round(w, 4))\n                graph = igraph.Graph(len(com_island_n_sizes.index), edges, directed=False)\n                graph.vs[\"index\"] = com_island_n_sizes.index.to_list()\n                graph.vs[\"island_id\"] = [isl.island_id for isl in islands_list]\n                graph.vs[\"island_size\"] = [isl.size for isl in islands_list]\n                graph.vs[\"flanked\"] = [isl.flanked for isl in islands_list]\n                graph.vs[\"proteome_id\"] = [isl.proteome for isl in islands_list]\n                graph.es[\"weight\"] = weights\n                graph.save(os.path.join(output_network_folder, f\"{com_id}.gml\"))\n                networks.append(graph)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            etime = time.time()\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Island network building elapsed time: {round(etime - stime, 2)} sec\")\n            return networks\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to build island network.\") from error\n\n    def find_hotspots(self, networks: igraph.Graph) -&gt; None:\n        \"\"\"Find hotspots in an island network using Leiden algorithm.\n\n        Args:\n            networks (igraph.Graph): Island network obtained by build_islands_network() function.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Searching for hotspots within each community...\")\n            number_of_communities = len(self.communities_annot.index)\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n            hotspots_l, h_annotation_rows = [], []\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_size = len(com_pr_ids)\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                com_island_network = networks[com_id]\n                partition_leiden = leidenalg.find_partition(com_island_network, leidenalg.CPMVertexPartition,\n                                                            resolution_parameter=self.prms.args[\n                                                                \"leiden_resolution_parameter_i\"],\n                                                            weights=\"weight\", n_iterations=-1)\n                com_island_network.vs[\"communities_Leiden\"] = partition_leiden.membership\n                islands_list = [island for proteome in com_proteomes for island in proteome.islands.to_list()]\n                for icom_ind, i_com in enumerate(partition_leiden):\n                    subgraph = com_island_network.subgraph(i_com)\n                    proteomes = subgraph.vs[\"proteome_id\"]\n                    hotspot_uniq_size = len(set(proteomes))\n                    hotspot_presence = hotspot_uniq_size / com_size\n                    if hotspot_presence &gt; self.prms.args[\"hotspot_presence_cutoff\"]:\n                        island_indexes, island_ids = subgraph.vs[\"index\"], subgraph.vs[\"island_id\"]\n                        island_size, island_flanked = subgraph.vs[\"island_size\"], subgraph.vs[\"flanked\"]\n                        strength, degree = subgraph.strength(weights=\"weight\"), subgraph.degree()\n                        island_annotation = pd.DataFrame(dict(island=island_ids, island_index=island_indexes,\n                                                              island_size=island_size, proteome=proteomes,\n                                                              flanked=island_flanked, strength=strength,\n                                                              degree=degree)).set_index(\"island\")\n                        if self.prms.args[\"deduplicate_proteomes_within_hotspot\"]:  # To update usage wo it\n                            island_annotation = island_annotation.sort_values(by=\"strength\", ascending=False)\n                            island_annotation = island_annotation.drop_duplicates(subset=\"proteome\", keep=\"first\")\n                            island_annotation = island_annotation.sort_values(by=\"island_index\")\n                            nodes_to_remove = subgraph.vs.select(island_id_notin=island_annotation.index.to_list())\n                            subgraph.delete_vertices(nodes_to_remove)\n                        islands = [islands_list[ind] for ind in island_annotation[\"island_index\"].to_list()]\n                        unique_island_cds_groups = []\n                        conserved_island_groups_count = collections.defaultdict(int)\n                        flanked_count = 0\n                        for island in islands:\n                            flanked_count += island.flanked\n                            island_proteome_cdss = com_proteomes.at[island.proteome].cdss\n                            island_cds_groups = island_proteome_cdss.iloc[island.indexes].apply(\n                                lambda isl: isl.group).to_list()\n                            ic_indexes = island.left_cons_neighbours + island.right_cons_neighbours\n                            island_conserved_groups = island_proteome_cdss.iloc[ic_indexes].apply(\n                                lambda isl: isl.group).to_list()\n                            if set(island_cds_groups) not in unique_island_cds_groups:\n                                unique_island_cds_groups.append(set(island_cds_groups))\n                            for icg in set(island_conserved_groups):\n                                conserved_island_groups_count[icg] += 1\n                        if flanked_count / hotspot_uniq_size &gt;= self.prms.args[\"flanked_fraction_cutoff\"]:\n                            flanked_hotspot = 1\n                        else:\n                            flanked_hotspot = 0\n                        if not self.prms.args[\"report_not_flanked\"] and not flanked_hotspot:\n                            continue\n\n                        signature_cutoff = int(self.prms.args[\"hotspot_signature_presence_cutoff\"] * hotspot_uniq_size)\n                        hotspot_conserved_signature = [g for g, c in conserved_island_groups_count.items() if\n                                                       c &gt;= signature_cutoff]\n                        number_of_unique_islands = len(unique_island_cds_groups)\n                        hotspot = Hotspot(hotspot_id=f\"{com_id}-{icom_ind}\", proteome_community=com_id,\n                                          size=hotspot_uniq_size, islands=islands,\n                                          conserved_signature=hotspot_conserved_signature,\n                                          island_annotation=island_annotation, flanked=flanked_hotspot)\n                        for island in islands:\n                            island.hotspot_id = hotspot.hotspot_id\n                        h_annotation_row = dict(hotspot_id=f\"{com_id}-{icom_ind}\", size=hotspot_uniq_size,\n                                                uniqueness=round(number_of_unique_islands / hotspot_uniq_size, 3),\n                                                number_of_unique_islands=number_of_unique_islands,\n                                                proteome_community=com_id, flanked=flanked_hotspot,\n                                                flanked_fraction=round(flanked_count / hotspot_uniq_size, 3))\n                        h_annotation_rows.append(h_annotation_row)\n                        hotspots_l.append(hotspot)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            h_annotation = pd.DataFrame(h_annotation_rows).set_index(\"hotspot_id\")\n            hotspots_s = pd.Series(hotspots_l, index=[hotspot.hotspot_id for hotspot in hotspots_l])\n            hotspots_obj = Hotspots(hotspots_s, h_annotation, parameters=self.prms)\n            num_of_hotspots = len(hotspots_l)\n            num_of_flanked = sum([hotspot.flanked for hotspot in hotspots_l])\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {num_of_hotspots} hotspots were found in {number_of_communities} proteome communities\"\n                      f\"  (Avg: {round(num_of_hotspots / number_of_communities, 3)} per community)\\n\"\n                      f\"  {num_of_flanked}/{num_of_hotspots} hotspots are flanked (consist of islands that have \"\n                      f\"conserved genes on both sides)\",\n                      file=sys.stdout)\n            return hotspots_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to find communities in the island network.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.__init__","title":"<code>__init__(parameters)</code>","text":"<p>Proteomes class constructor.</p> <p>Parameters:</p> <ul> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, parameters: ilund4u.manager.Parameters):\n    \"\"\"Proteomes class constructor.\n\n    Arguments:\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = pd.Series()\n    self.annotation = None\n    self.__col_to_ind = None\n    self.seq_to_ind = None\n    self.communities = dict()\n    self.communities_annot = None\n    self.proteins_fasta_file = os.path.join(parameters.args[\"output_dir\"], \"all_proteins.fa\")\n    self.prms = parameters\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.annotate_variable_islands","title":"<code>annotate_variable_islands()</code>","text":"<p>Annotate variable islands defined as a region with a set of non-conserved proteins.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def annotate_variable_islands(self) -&gt; None:\n    \"\"\"Annotate variable islands defined as a region with a set of non-conserved proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Annotating variable islands within each proteome...\", file=sys.stdout)\n        total_number_of_variable_regions = 0\n        for proteome_index, proteome in enumerate(self.proteomes):\n            proteome.annotate_variable_islands(self.prms)\n            total_number_of_variable_regions += proteome.islands.size\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {total_number_of_variable_regions} variable regions are annotated in \"\n                  f\"{len(self.proteomes.index)} proteomes \"\n                  f\"({round(total_number_of_variable_regions / len(self.proteomes.index), 3)} per proteome)\",\n                  file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to annotate variable islands.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.build_islands_network","title":"<code>build_islands_network()</code>","text":"<p>Build island network where each node - an island and weighted edges - fraction of shared     conserved neighbours homologues.</p> <p>Returns:</p> <ul> <li> <code>Graph</code>         \u2013          <p>igraph.Graph: Island network.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_islands_network(self) -&gt; igraph.Graph:\n    \"\"\"Build island network where each node - an island and weighted edges - fraction of shared\n        conserved neighbours homologues.\n\n    Returns:\n        igraph.Graph: Island network.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Island network construction within each proteome community...\")\n        output_network_folder = os.path.join(self.prms.args[\"output_dir\"], \"island_networks\")\n        if os.path.exists(output_network_folder):\n            shutil.rmtree(output_network_folder)\n        os.mkdir(output_network_folder)\n        number_of_communities = len(self.communities_annot.index)\n        stime = time.time()\n        networks = []\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            com_island_n_sizes = pd.Series()\n            com_neighbours = pd.Series()\n            cluster_to_island = collections.defaultdict(collections.deque)\n\n            islands_list = [island for proteome in com_proteomes.to_list() for island in proteome.islands.to_list()]\n            island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(islands_list)}\n            for proteome in com_proteomes:\n                for island in proteome.islands.to_list():\n                    island_id = island.island_id\n                    island_index = island_id_to_index[island_id]\n                    conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(proteome.cdss))\n                    com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                    com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                    for cing in conserved_island_neighbours_groups:\n                        cluster_to_island[cing].append(island_index)\n            edges, weights = [], []\n\n            for i in range(len(com_island_n_sizes.index)):\n                neighbours_i = com_neighbours.iat[i]\n                size_i = com_island_n_sizes.iat[i]\n                counts_i = collections.defaultdict(int)\n                for ncl in neighbours_i:\n                    js = cluster_to_island[ncl]\n                    for j in js.copy():\n                        if i &lt; j:\n                            counts_i[j] += 1\n                        else:\n                            js.popleft()\n                weights_i = pd.Series(counts_i)\n                connected_n_sizes = com_island_n_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                          index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n            graph = igraph.Graph(len(com_island_n_sizes.index), edges, directed=False)\n            graph.vs[\"index\"] = com_island_n_sizes.index.to_list()\n            graph.vs[\"island_id\"] = [isl.island_id for isl in islands_list]\n            graph.vs[\"island_size\"] = [isl.size for isl in islands_list]\n            graph.vs[\"flanked\"] = [isl.flanked for isl in islands_list]\n            graph.vs[\"proteome_id\"] = [isl.proteome for isl in islands_list]\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(output_network_folder, f\"{com_id}.gml\"))\n            networks.append(graph)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        etime = time.time()\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Island network building elapsed time: {round(etime - stime, 2)} sec\")\n        return networks\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to build island network.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.build_proteome_network","title":"<code>build_proteome_network(cluster_to_sequences)</code>","text":"<p>Build proteome network where each proteome represented by node and weighted edges between nodes -     fraction of shared homologues.</p> <p>Parameters:</p> <ul> <li> <code>cluster_to_sequences</code>             (<code>dict</code>)         \u2013          <p>cluster id to list of proteins dictionary (results of process_mmseqs_results() function)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Graph</code>         \u2013          <p>igraph.Graph: proteome network.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_proteome_network(self, cluster_to_sequences: dict) -&gt; igraph.Graph:\n    \"\"\"Build proteome network where each proteome represented by node and weighted edges between nodes -\n        fraction of shared homologues.\n\n    Arguments:\n        cluster_to_sequences (dict): cluster id to list of proteins dictionary\n            (results of process_mmseqs_results() function)\n\n    Returns:\n        igraph.Graph: proteome network.\n\n    \"\"\"\n    try:\n        cluster_to_proteome_index = dict()\n        for cluster, sequences in cluster_to_sequences.items():\n            indexes = sorted([self.seq_to_ind[seq_id] for seq_id in sequences])\n            cluster_to_proteome_index[cluster] = collections.deque(indexes)\n\n        proteome_sizes = self.annotation[[\"proteome_size_unique\", \"index\"]]\n        first_index_for_size = proteome_sizes.groupby(\"proteome_size_unique\").tail(1).copy()\n        max_p_size = first_index_for_size[\"proteome_size_unique\"].max()\n        cut_off_mult = 1 / self.prms.args[\"proteome_similarity_cutoff\"]\n        first_index_for_size[\"cutoff\"] = first_index_for_size[\"proteome_size_unique\"].apply(\n            lambda size: first_index_for_size[first_index_for_size[\"proteome_size_unique\"] &gt;=\n                                              min(size * cut_off_mult, max_p_size)][\"index\"].min() + 1)\n        upper_index_cutoff = first_index_for_size.set_index(\"proteome_size_unique\")[\"cutoff\"]\n        proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Proteomes network construction...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix=\"%(index)d/%(max)d\")\n        stime = time.time()\n        edges, weights = [], []\n        for i in range(len(self.proteomes.index)):\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            clusters_i = self.annotation.iat[i, self.__col_to_ind[\"protein_clusters\"]]\n            size_i = self.annotation.iat[i, self.__col_to_ind[\"proteome_size_unique\"]]\n            counts_i = collections.defaultdict(int)\n            upper_i_cutoff = upper_index_cutoff.at[size_i]\n            for cl in clusters_i:\n                js = cluster_to_proteome_index[cl]\n                for j in js.copy():\n                    if i &lt; j &lt; upper_i_cutoff:\n                        counts_i[j] += 1\n                    elif j &lt;= i:\n                        js.popleft()\n                    else:\n                        break\n            weights_i = pd.Series(counts_i)\n            proteome_sizes_connected = proteome_sizes.iloc[weights_i.index]\n            norm_factor_i = pd.Series(\n                0.5 * (size_i + proteome_sizes_connected) / (size_i * proteome_sizes_connected), \\\n                index=weights_i.index)\n            weights_i = weights_i.mul(norm_factor_i)\n            weights_i = weights_i[weights_i &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n            for j, w in weights_i.items():\n                edges.append([i, j])\n                weights.append(round(w, 4))\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        etime = time.time()\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Network building elapsed time: {round(etime - stime, 2)} sec\")\n        graph = igraph.Graph(len(self.proteomes.index), edges, directed=False)\n        graph.vs[\"index\"] = self.annotation[\"index\"].to_list()\n        graph.vs[\"sequence_id\"] = self.annotation.index.to_list()\n        graph.es[\"weight\"] = weights\n        graph.save(os.path.join(self.prms.args[\"output_dir\"], \"proteome_network.gml\"))\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Proteomes network with {len(edges)} connections was built\\n\"\n                  f\"  \u29bf Network was saved as {os.path.join(self.prms.args['output_dir'], 'proteome_network.gml')}\",\n                  file=sys.stdout)\n        return graph\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to built proteome network.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.db_init","title":"<code>db_init(db_path, parameters)</code>  <code>classmethod</code>","text":"<p>Class method to load a Proteomes object from a database.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>path to the database.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code>        \u2013          <p>Proteomes object.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>@classmethod\ndef db_init(cls, db_path: str, parameters: ilund4u.manager.Parameters):\n    \"\"\"Class method to load a Proteomes object from a database.\n\n    Arguments:\n        db_path (str): path to the database.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        cls: Proteomes object.\n\n    \"\"\"\n    try:\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading cds objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"cds.ind.attributes.json\"), \"r\") as json_file:\n            cds_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(cds_ind_attributes), suffix='%(index)d/%(max)d')\n        cds_list = []\n        for cds_dict in cds_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            cds_list.append(CDS(**cds_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        cdss = pd.Series(cds_list, index=[cds.cds_id for cds in cds_list])\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading island objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"island.ind.attributes.json\"), \"r\") as json_file:\n            island_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(island_ind_attributes), suffix='%(index)d/%(max)d')\n        island_list = []\n        for proteome_dict in island_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            island_list.append(Island(**proteome_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        islands = pd.Series(island_list, index=[island.island_id for island in island_list])\n\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading proteome objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"proteome.ind.attributes.json\"), \"r\") as json_file:\n            proteome_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(proteome_ind_attributes), suffix='%(index)d/%(max)d')\n        proteome_list = []\n        for proteome_dict in proteome_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            proteome_dict[\"gff_file\"] = os.path.join(db_path, \"gff\", proteome_dict[\"gff_file\"])\n            proteome_dict[\"cdss\"] = cdss.loc[proteome_dict[\"cdss\"]]\n            proteome_dict[\"islands\"] = islands.loc[proteome_dict[\"islands\"]]\n            proteome_list.append(Proteome(**proteome_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        proteomes = pd.Series(proteome_list, index=[proteome.proteome_id for proteome in proteome_list])\n\n        cls_obj = cls(parameters)\n        cls_obj.proteomes = proteomes\n        with open(os.path.join(db_path, \"proteomes.attributes.json\"), \"r\") as json_file:\n            proteomes_attributes = json.load(json_file)\n        cls_obj.communities = {int(k): v for k, v in proteomes_attributes[\"communities\"].items()}\n\n        cls_obj.proteins_fasta_file = os.path.join(db_path, proteomes_attributes[\"proteins_fasta_file\"])\n\n        cls_obj.annotation = pd.read_table(os.path.join(db_path, \"proteomes.annotations.tsv\"),\n                                           sep=\"\\t\").set_index(\"proteome_id\")\n        cls_obj.__col_to_ind = {col: idx for idx, col in enumerate(cls_obj.annotation.columns)}\n        cls_obj.communities_annot = pd.read_table(os.path.join(db_path, \"proteomes.communities_annot.tsv\"),\n                                                  sep=\"\\t\").set_index(\"id\")\n        cls_obj.seq_to_ind = {sid: idx for idx, sid in enumerate(cls_obj.annotation.index)}\n\n        return cls_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to read Proteomes from the database.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.define_protein_classes","title":"<code>define_protein_classes()</code>","text":"<p>Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def define_protein_classes(self) -&gt; None:\n    \"\"\"Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Defining protein classes within each community...\")\n        number_of_communities = len(self.communities_annot.index)\n        protein_classes_trows = []\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix='%(index)d/%(max)d')\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_size = len(com_pr_ids)\n            com_annotation = self.annotation.loc[com_pr_ids]\n            com_protein_clusters = com_annotation[\"protein_clusters\"].sum()\n            com_protein_clusters_count = collections.Counter(com_protein_clusters)\n            com_protein_classes = dict()\n            for pc, counts in com_protein_clusters_count.items():\n                pc_fraction = counts / com_size\n                if pc_fraction &lt; self.prms.args[\"variable_protein_cluster_cutoff\"]:\n                    pc_class = \"variable\"\n                elif pc_fraction &gt; self.prms.args[\"conserved_protein_cluster_cutoff\"]:\n                    pc_class = \"conserved\"\n                else:\n                    pc_class = \"intermediate\"\n                com_protein_classes[pc] = pc_class\n                protein_classes_trows.append(dict(community=com_id, community_size=com_size, protein_group=pc,\n                                                  protein_group_class=pc_class, fraction=round(pc_fraction, 3),\n                                                  protein_group_counts=counts))\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            for com_proteome in com_proteomes:\n                for cds in com_proteome.cdss:\n                    cds.g_class = com_protein_classes[cds.group]\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        protein_classes_t = pd.DataFrame(protein_classes_trows)\n        protein_classes_t.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_classes.tsv\"), sep=\"\\t\",\n                                 index=False)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to define protein classes.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.find_hotspots","title":"<code>find_hotspots(networks)</code>","text":"<p>Find hotspots in an island network using Leiden algorithm.</p> <p>Parameters:</p> <ul> <li> <code>networks</code>             (<code>Graph</code>)         \u2013          <p>Island network obtained by build_islands_network() function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def find_hotspots(self, networks: igraph.Graph) -&gt; None:\n    \"\"\"Find hotspots in an island network using Leiden algorithm.\n\n    Args:\n        networks (igraph.Graph): Island network obtained by build_islands_network() function.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Searching for hotspots within each community...\")\n        number_of_communities = len(self.communities_annot.index)\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n        hotspots_l, h_annotation_rows = [], []\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_size = len(com_pr_ids)\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            com_island_network = networks[com_id]\n            partition_leiden = leidenalg.find_partition(com_island_network, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_i\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            com_island_network.vs[\"communities_Leiden\"] = partition_leiden.membership\n            islands_list = [island for proteome in com_proteomes for island in proteome.islands.to_list()]\n            for icom_ind, i_com in enumerate(partition_leiden):\n                subgraph = com_island_network.subgraph(i_com)\n                proteomes = subgraph.vs[\"proteome_id\"]\n                hotspot_uniq_size = len(set(proteomes))\n                hotspot_presence = hotspot_uniq_size / com_size\n                if hotspot_presence &gt; self.prms.args[\"hotspot_presence_cutoff\"]:\n                    island_indexes, island_ids = subgraph.vs[\"index\"], subgraph.vs[\"island_id\"]\n                    island_size, island_flanked = subgraph.vs[\"island_size\"], subgraph.vs[\"flanked\"]\n                    strength, degree = subgraph.strength(weights=\"weight\"), subgraph.degree()\n                    island_annotation = pd.DataFrame(dict(island=island_ids, island_index=island_indexes,\n                                                          island_size=island_size, proteome=proteomes,\n                                                          flanked=island_flanked, strength=strength,\n                                                          degree=degree)).set_index(\"island\")\n                    if self.prms.args[\"deduplicate_proteomes_within_hotspot\"]:  # To update usage wo it\n                        island_annotation = island_annotation.sort_values(by=\"strength\", ascending=False)\n                        island_annotation = island_annotation.drop_duplicates(subset=\"proteome\", keep=\"first\")\n                        island_annotation = island_annotation.sort_values(by=\"island_index\")\n                        nodes_to_remove = subgraph.vs.select(island_id_notin=island_annotation.index.to_list())\n                        subgraph.delete_vertices(nodes_to_remove)\n                    islands = [islands_list[ind] for ind in island_annotation[\"island_index\"].to_list()]\n                    unique_island_cds_groups = []\n                    conserved_island_groups_count = collections.defaultdict(int)\n                    flanked_count = 0\n                    for island in islands:\n                        flanked_count += island.flanked\n                        island_proteome_cdss = com_proteomes.at[island.proteome].cdss\n                        island_cds_groups = island_proteome_cdss.iloc[island.indexes].apply(\n                            lambda isl: isl.group).to_list()\n                        ic_indexes = island.left_cons_neighbours + island.right_cons_neighbours\n                        island_conserved_groups = island_proteome_cdss.iloc[ic_indexes].apply(\n                            lambda isl: isl.group).to_list()\n                        if set(island_cds_groups) not in unique_island_cds_groups:\n                            unique_island_cds_groups.append(set(island_cds_groups))\n                        for icg in set(island_conserved_groups):\n                            conserved_island_groups_count[icg] += 1\n                    if flanked_count / hotspot_uniq_size &gt;= self.prms.args[\"flanked_fraction_cutoff\"]:\n                        flanked_hotspot = 1\n                    else:\n                        flanked_hotspot = 0\n                    if not self.prms.args[\"report_not_flanked\"] and not flanked_hotspot:\n                        continue\n\n                    signature_cutoff = int(self.prms.args[\"hotspot_signature_presence_cutoff\"] * hotspot_uniq_size)\n                    hotspot_conserved_signature = [g for g, c in conserved_island_groups_count.items() if\n                                                   c &gt;= signature_cutoff]\n                    number_of_unique_islands = len(unique_island_cds_groups)\n                    hotspot = Hotspot(hotspot_id=f\"{com_id}-{icom_ind}\", proteome_community=com_id,\n                                      size=hotspot_uniq_size, islands=islands,\n                                      conserved_signature=hotspot_conserved_signature,\n                                      island_annotation=island_annotation, flanked=flanked_hotspot)\n                    for island in islands:\n                        island.hotspot_id = hotspot.hotspot_id\n                    h_annotation_row = dict(hotspot_id=f\"{com_id}-{icom_ind}\", size=hotspot_uniq_size,\n                                            uniqueness=round(number_of_unique_islands / hotspot_uniq_size, 3),\n                                            number_of_unique_islands=number_of_unique_islands,\n                                            proteome_community=com_id, flanked=flanked_hotspot,\n                                            flanked_fraction=round(flanked_count / hotspot_uniq_size, 3))\n                    h_annotation_rows.append(h_annotation_row)\n                    hotspots_l.append(hotspot)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        h_annotation = pd.DataFrame(h_annotation_rows).set_index(\"hotspot_id\")\n        hotspots_s = pd.Series(hotspots_l, index=[hotspot.hotspot_id for hotspot in hotspots_l])\n        hotspots_obj = Hotspots(hotspots_s, h_annotation, parameters=self.prms)\n        num_of_hotspots = len(hotspots_l)\n        num_of_flanked = sum([hotspot.flanked for hotspot in hotspots_l])\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {num_of_hotspots} hotspots were found in {number_of_communities} proteome communities\"\n                  f\"  (Avg: {round(num_of_hotspots / number_of_communities, 3)} per community)\\n\"\n                  f\"  {num_of_flanked}/{num_of_hotspots} hotspots are flanked (consist of islands that have \"\n                  f\"conserved genes on both sides)\",\n                  file=sys.stdout)\n        return hotspots_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to find communities in the island network.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.find_proteome_communities","title":"<code>find_proteome_communities(graph)</code>","text":"<p>Find proteome communities using Leiden algorithm and update communities attribute.</p> <p>Parameters:</p> <ul> <li> <code>graph</code>             (<code>Graph</code>)         \u2013          <p>network of proteomes obtained by build_proteome_network function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def find_proteome_communities(self, graph: igraph.Graph) -&gt; None:\n    \"\"\"Find proteome communities using Leiden algorithm and update communities attribute.\n\n    Arguments:\n        graph (igraph.Graph): network of proteomes obtained by build_proteome_network function.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Proteome network partitioning using the Leiden algorithm...\")\n        partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                    resolution_parameter=self.prms.args[\n                                                        \"leiden_resolution_parameter_p\"],\n                                                    weights=\"weight\", n_iterations=-1)\n        graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(set(partition_leiden.membership))} proteome communities were found\")\n        communities_annot_rows = []\n        sequences_to_drop = []\n        for community_index, community in enumerate(partition_leiden):\n            community_size = len(community)\n            subgraph = graph.subgraph(community)\n            proteomes = subgraph.vs[\"sequence_id\"]\n            if community_size &gt;= self.prms.args[\"min_proteome_community_size\"]:\n                self.communities[community_index] = proteomes\n            else:\n                sequences_to_drop += proteomes\n            if community_size &gt; 1:\n                subgraph_edges = subgraph.get_edgelist()\n                num_of_edges = len(subgraph_edges)\n                num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                weights = subgraph.es[\"weight\"]\n                avg_weight = round(np.mean(weights), 3)\n                max_identity = max(weights)\n            else:\n                num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n            communities_annot_rows.append([community_index, community_size, avg_weight, max_identity,\n                                           num_of_edges_fr, num_of_edges, \";\".join(proteomes)])\n        communities_annot = pd.DataFrame(communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\", \"max_weight\",\n                                                                          \"fr_edges\", \"n_edges\", \"proteomes\"])\n        communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"proteome_communities.tsv\")), sep=\"\\t\", index=False)\n        communities_annot = communities_annot[communities_annot[\"size\"] &gt;=\n                                              self.prms.args[\"min_proteome_community_size\"]]\n        self.communities_annot = communities_annot.set_index(\"id\")\n        self.annotation = self.annotation.drop(sequences_to_drop)\n        self.proteomes = self.proteomes.drop(sequences_to_drop)\n        self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n        self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(communities_annot.index)} proteomes communities with size &gt;= \"\n                  f\"{self.prms.args['min_proteome_community_size']} were taken for further analysis\",\n                  file=sys.stdout)\n            print(f\"  \u29bf {len(sequences_to_drop)} proteomes from smaller communities were excluded from the \"\n                  f\"analysis\", file=sys.stdout)\n        if len(communities_annot.index) == 0:\n            print(\"\u25cb Termination since no proteome community was taken for further analysis\")\n            sys.exit()\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to find proteome communities.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.load_sequences_from_extended_gff","title":"<code>load_sequences_from_extended_gff(input_f, genome_annotation=None)</code>","text":"<p>Load proteomes from gff files.</p> <p>Parameters:</p> <ul> <li> <code>input_f</code>             (<code>str | list</code>)         \u2013          <p>List of file paths or path to a folder with gff files.</p> </li> <li> <code>genome_annotation</code>             (<code>path</code>, default:                 <code>None</code> )         \u2013          <p>Path to a table with annotation of genome circularity. Format: two columns with names: id, circular; tab-separated, 1,0 values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def load_sequences_from_extended_gff(self, input_f: typing.Union[str, list], genome_annotation=None) -&gt; None:\n    \"\"\"Load proteomes from gff files.\n\n    Arguments:\n        input_f (str | list): List of file paths or path to a folder with gff files.\n        genome_annotation (path): Path to a table with annotation of genome circularity.\n            Format: two columns with names: id, circular; tab-separated, 1,0 values.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if isinstance(input_f, str):\n            input_folder = input_f\n            if not os.path.exists(input_folder):\n                raise ilund4u.manager.ilund4uError(f\"Folder {input_folder} does not exist.\")\n            gff_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder)]\n        elif isinstance(input_f, list):\n            gff_files = input_f\n        else:\n            raise ilund4u.manager.ilund4uError(f\"The input for the GFF parsing function must be either a folder or \"\n                                               f\"a list of files.\")\n        if not gff_files:\n            raise ilund4u.manager.ilund4uError(f\"Folder {input_f} does not contain files.\")\n        if not os.path.exists(self.prms.args[\"output_dir\"]):\n            os.mkdir(self.prms.args[\"output_dir\"])\n        else:\n            if os.path.exists(self.proteins_fasta_file):\n                os.remove(self.proteins_fasta_file)\n        genome_circularity_dict = dict()\n        if genome_annotation:\n            try:\n                genome_annotation_table = pd.read_table(genome_annotation, sep=\"\\t\").set_index(\"id\")\n                genome_circularity_dict = genome_annotation_table[\"circular\"].to_dict()\n            except:\n                raise ilund4u.manager.ilund4uError(\"\u25cb Warning: unable to read genome annotation table. \"\n                                                   \"Check the format.\")\n        num_of_gff_files = len(gff_files)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Reading gff file{'s' if len(gff_files) &gt; 1 else ''}...\", file=sys.stdout)\n        if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=num_of_gff_files, suffix='%(index)d/%(max)d')\n        proteome_list, annotation_rows = [], []\n        gff_records_batch = []\n        for gff_file_index, gff_file_path in enumerate(gff_files):\n            try:\n                if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                    bar.next()\n                gff_records = list(BCBio.GFF.parse(gff_file_path, limit_info=dict(gff_type=[\"CDS\"])))\n                if len(gff_records) != 1:\n                    print(f\"\\n\u25cb Warning: gff file {gff_file_path} contains information for more than 1 \"\n                          f\"sequence. File will be skipped.\")\n                    continue\n                current_gff_records = []\n                gff_record = gff_records[0]\n                try:\n                    record_locus_sequence = gff_record.seq\n                except Bio.Seq.UndefinedSequenceError as error:\n                    raise ilund4u.manager.ilund4uError(f\"gff file doesn't contain corresponding \"\n                                                       f\"sequences.\") from error\n                if self.prms.args[\"use_filename_as_contig_id\"]:\n                    gff_record.id = os.path.splitext(os.path.basename(gff_file_path))[0]\n                features_ids = [i.id for i in gff_record.features]\n                if len(features_ids) != len(set(features_ids)):\n                    raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains duplicated feature \"\n                                                       f\"ids while only unique are allowed.\")\n                if len(features_ids) &gt; self.prms.args[\"min_proteome_size\"]:\n                    if gff_record.id in genome_circularity_dict.keys():\n                        circular = int(genome_circularity_dict[gff_record.id])\n                    else:\n                        circular = int(self.prms.args[\"circular_genomes\"])\n                    record_proteome = Proteome(proteome_id=gff_record.id, gff_file=gff_file_path, cdss=pd.Series(),\n                                               circular=circular)\n                    record_cdss = []\n                    all_defined = True\n                    for gff_feature in gff_record.features:\n                        cds_id = gff_feature.id.replace(\";\", \",\")\n                        if gff_record.id not in cds_id:\n                            cds_id = f\"{gff_record.id}-{cds_id}\"  # Attention\n                        transl_table = self.prms.args[\"default_transl_table\"]\n                        if \"transl_table\" in gff_feature.qualifiers.keys():\n                            transl_table = int(gff_feature.qualifiers[\"transl_table\"][0])\n                        name = \"\"\n                        if self.prms.args[\"gff_CDS_name_source\"] in gff_feature.qualifiers:\n                            name = gff_feature.qualifiers[self.prms.args[\"gff_CDS_name_source\"]][0]\n\n                        sequence = gff_feature.translate(record_locus_sequence, table=transl_table, cds=False)[:-1]\n\n                        if not sequence.defined:\n                            all_defined = False\n                            continue\n\n                        current_gff_records.append(Bio.SeqRecord.SeqRecord(seq=sequence, id=cds_id, description=\"\"))\n                        cds = CDS(cds_id=cds_id, proteome_id=gff_record.id,\n                                  start=int(gff_feature.location.start) + 1, end=int(gff_feature.location.end),\n                                  strand=gff_feature.location.strand, name=name)\n                        record_cdss.append(cds)\n                    if all_defined:\n                        gff_records_batch += current_gff_records\n                        record_proteome.cdss = pd.Series(record_cdss, index=[cds.cds_id for cds in record_cdss])\n                        proteome_list.append(record_proteome)\n                        annotation_rows.append(dict(id=gff_record.id, length=len(gff_record.seq),\n                                                    proteome_size=len(features_ids),\n                                                    proteome_size_unique=\"\", protein_clusters=\"\"))\n                    else:\n                        raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains not defined feature\")\n                if gff_file_index % 1000 == 0 or gff_file_index == num_of_gff_files - 1:\n                    with open(self.proteins_fasta_file, \"a\") as handle:\n                        Bio.SeqIO.write(gff_records_batch, handle, \"fasta\")\n                    gff_records_batch = []\n            except:\n                print(f\"\u25cb Warning: gff file {gff_file_path} was not read properly and skipped\")\n                if self.prms.args[\"parsing_debug\"]:\n                    self.prms.args[\"debug\"] = True\n                    raise ilund4u.manager.ilund4uError(\"Gff file {gff_file_path} was not read properly\")\n        if len(gff_files) &gt; 1 and self.prms.args[\"verbose\"]:\n            bar.finish()\n        proteome_ids = [pr.proteome_id for pr in proteome_list]\n        if len(proteome_ids) != len(set(proteome_ids)):\n            raise ilund4u.manager.ilund4uError(f\"The input gff files have duplicated contig ids.\\n  \"\n                                               f\"You can use `--use-filename-as-id` parameter to use file name \"\n                                               f\"as contig id which can help to fix the problem.\")\n        self.proteomes = pd.Series(proteome_list, index=[pr.proteome_id for pr in proteome_list])\n        self.annotation = pd.DataFrame(annotation_rows).set_index(\"id\")\n        self.__col_to_ind = {col: idx for idx, col in enumerate(self.annotation.columns)}\n        self.annotation = self.annotation.sort_values(by=\"proteome_size\")\n        self.proteomes = self.proteomes.loc[self.annotation.index]\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(proteome_list)} {'locus was' if len(proteome_list) == 1 else 'loci were'} loaded from\"\n                  f\" the gff files folder\", file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to load proteomes from gff files.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.mmseqs_cluster","title":"<code>mmseqs_cluster()</code>","text":"<p>Cluster all proteins using mmseqs in order to define groups of homologues.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>protein id to cluster id dictionary.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def mmseqs_cluster(self) -&gt; dict:\n    \"\"\"Cluster all proteins using mmseqs in order to define groups of homologues.\n\n    Returns:\n        dict: protein id to cluster id dictionary.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Running mmseqs for protein clustering...\", file=sys.stdout)\n        mmseqs_input = self.proteins_fasta_file\n        mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n        if os.path.exists(mmseqs_output_folder):\n            shutil.rmtree(mmseqs_output_folder)\n        os.mkdir(mmseqs_output_folder)\n        mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DB\")\n        os.mkdir(mmseqs_output_folder_db)\n        mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n        mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", mmseqs_input,\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\")], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"cluster\",\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"tmp\"),\n                        \"--cluster-mode\", str(self.prms.args[\"mmseqs_cluster_mode\"]),\n                        \"--cov-mode\", str(self.prms.args[\"mmseqs_cov_mode\"]),\n                        \"--min-seq-id\", str(self.prms.args[\"mmseqs_min_seq_id\"]),\n                        \"-c\", str(self.prms.args[\"mmseqs_c\"]),\n                        \"-s\", str(self.prms.args[\"mmseqs_s\"])], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)  # threads!\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createtsv\",\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                        os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\")],\n                       stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        mmseqs_clustering_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\"),\n                                                  sep=\"\\t\", header=None, names=[\"cluster\", \"protein_id\"])\n        mmseqs_clustering_results = mmseqs_clustering_results.set_index(\"protein_id\")[\"cluster\"].to_dict()\n        num_of_unique_clusters = len(set(mmseqs_clustering_results.values()))\n        num_of_proteins = len(mmseqs_clustering_results.keys())\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {num_of_unique_clusters} clusters for {num_of_proteins} proteins were found with mmseqs\\n\"\n                  f\"  \u29bf mmseqs clustering results were saved to \"\n                  f\"{os.path.join(mmseqs_output_folder, 'mmseqs_clustering.tsv')}\", file=sys.stdout)\n        return mmseqs_clustering_results\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs clustering.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.process_mmseqs_results","title":"<code>process_mmseqs_results(mmseqs_results)</code>","text":"<p>Process results of mmseqs clustering run.</p> <p>Parameters:</p> <ul> <li> <code>mmseqs_results</code>             (<code>dict</code>)         \u2013          <p>results of mmseqs_cluster function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>dictionary with protein cluster id to list of protein ids items.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def process_mmseqs_results(self, mmseqs_results: dict) -&gt; dict:\n    \"\"\"Process results of mmseqs clustering run.\n\n    Arguments:\n        mmseqs_results (dict): results of mmseqs_cluster function.\n\n    Returns:\n        dict: dictionary with protein cluster id to list of protein ids items.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Processing mmseqs results ...\", file=sys.stdout)\n        sequences_to_drop, drop_reason = [], []\n        current_p_length, cpl_added_proteomes = None, None\n        cluster_to_sequences = collections.defaultdict(list)\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix='%(index)d/%(max)d')\n        for p_index, proteome in enumerate(self.proteomes.to_list()):\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            seq_p_size = self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size\"]]\n            if seq_p_size != current_p_length:\n                current_p_length = seq_p_size\n                cpl_added_proteomes = []\n            seq_protein_clusters = []\n            for cds in proteome.cdss.to_list():\n                cds.group = mmseqs_results[cds.cds_id]\n                seq_protein_clusters.append(cds.group)\n            seq_protein_clusters_set = set(seq_protein_clusters)\n            unique_p_size = len(seq_protein_clusters_set)\n            if seq_protein_clusters_set in cpl_added_proteomes:\n                sequences_to_drop.append(proteome.proteome_id)\n                drop_reason.append(f\"Duplicate\")\n                continue\n            if unique_p_size / seq_p_size &lt; self.prms.args[\"proteome_uniqueness_cutoff\"]:\n                sequences_to_drop.append(proteome.proteome_id)\n                drop_reason.append(\"Proteome uniqueness cutoff\")\n                continue\n            self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size_unique\"]] = unique_p_size\n            self.annotation.iat[p_index, self.__col_to_ind[\"protein_clusters\"]] = list(seq_protein_clusters_set)\n            cpl_added_proteomes.append(seq_protein_clusters_set)\n            for p_cluster in seq_protein_clusters_set:\n                cluster_to_sequences[p_cluster].append(proteome.proteome_id)\n        dropped_sequences = pd.DataFrame(dict(sequence=sequences_to_drop, reason=drop_reason))\n        dropped_sequences.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"dropped_sequences.tsv\"), sep=\"\\t\",\n                                 index=False)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n            print(f\"  \u29bf {len(sequences_to_drop)} proteomes were excluded after proteome\"\n                  f\" deduplication and filtering\", file=sys.stdout)\n        self.annotation = self.annotation.drop(sequences_to_drop)\n        self.proteomes = self.proteomes.drop(sequences_to_drop)\n        self.annotation = self.annotation.sort_values(by=\"proteome_size_unique\")\n        self.proteomes = self.proteomes.loc[self.annotation.index]\n        self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n        self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n        return cluster_to_sequences\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to process mmseqs output.\") from error\n</code></pre>"},{"location":"API/data_processing/#ilund4u.data_processing.Proteomes.save_as_db","title":"<code>save_as_db(db_folder)</code>","text":"<p>Save Proteomes to the iLnd4u database.</p> <p>Parameters:</p> <ul> <li> <code>db_folder</code>             (<code>str</code>)         \u2013          <p>Database folder path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def save_as_db(self, db_folder: str) -&gt; None:\n    \"\"\"Save Proteomes to the iLnd4u database.\n\n    Arguments:\n        db_folder (str): Database folder path.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        attributes_to_ignore = [\"proteomes\", \"annotation\", \"communities_annot\", \"prms\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"proteins_fasta_file\"] = os.path.basename(attributes[\"proteins_fasta_file\"])\n        with open(os.path.join(db_folder, \"proteomes.attributes.json\"), 'w') as json_file:\n            json.dump(attributes, json_file)\n        self.communities_annot.to_csv(os.path.join(db_folder, \"proteomes.communities_annot.tsv\"), sep=\"\\t\",\n                                      index_label=\"id\")\n        self.annotation[\"protein_clusters\"] = self.annotation[\"protein_clusters\"].apply(lambda x: \";\".join(x))\n        self.annotation.to_csv(os.path.join(db_folder, \"proteomes.annotations.tsv\"), sep=\"\\t\",\n                               index_label=\"proteome_id\")\n        os.mkdir(os.path.join(db_folder, \"gff\"))\n        proteome_db_ind, cdss_db_ind, islands_db_ind, cds_ids, repr_cds_ids = [], [], [], [], set()\n        for community, proteomes in self.communities.items():\n            for proteome_id in proteomes:\n                proteome = self.proteomes.at[proteome_id]\n                proteome_db_ind.append(proteome.get_proteome_db_row())\n                os.system(f\"cp '{proteome.gff_file}' {os.path.join(db_folder, 'gff')}/\")\n                for cds in proteome.cdss.to_list():\n                    cds_ids.append(cds.cds_id)\n                    cdss_db_ind.append(cds.get_cds_db_row())\n                    repr_cds_ids.add(cds.group)\n                for island in proteome.islands.to_list():\n                    islands_db_ind.append(island.get_island_db_row())\n\n        with open(os.path.join(db_folder, \"proteome.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(proteome_db_ind, json_file)\n        with open(os.path.join(db_folder, \"cds.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(cdss_db_ind, json_file)\n        with open(os.path.join(db_folder, \"island.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(islands_db_ind, json_file)\n\n        initial_fasta_file = Bio.SeqIO.index(self.proteins_fasta_file, \"fasta\")\n        with open(os.path.join(db_folder, attributes[\"proteins_fasta_file\"]), \"wb\") as out_handle:\n            for acc in cds_ids:\n                out_handle.write(initial_fasta_file.get_raw(acc))\n\n        with open(os.path.join(db_folder, \"representative_seqs.fa\"), \"wb\") as out_handle:\n            for acc in repr_cds_ids:\n                out_handle.write(initial_fasta_file.get_raw(acc))\n\n        mmseqs_db_folder = os.path.join(db_folder, \"mmseqs_db\")\n        if os.path.exists(mmseqs_db_folder):\n            shutil.rmtree(mmseqs_db_folder)\n        os.mkdir(mmseqs_db_folder)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\",\n                        os.path.join(db_folder, attributes[\"proteins_fasta_file\"]),\n                        os.path.join(mmseqs_db_folder, \"all_proteins\")],\n                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to write Proteomes to the database.\") from error\n</code></pre>"},{"location":"API/drawing/","title":"Drawing","text":"<p>This module provides visualisation classes and methods for the tool.</p>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager","title":"<code>DrawingManager</code>","text":"<p>Manager for data visualisation using LoVis4u library.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>class DrawingManager:\n    \"\"\"Manager for data visualisation using LoVis4u library.\n\n    Attributes:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n\n    \"\"\"\n\n    def __init__(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                 parameters: ilund4u.manager.Parameters):\n        \"\"\"DrawingManager class constructor\n\n        Arguments:\n            proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n            hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = proteomes\n        self.hotspots = hotspots\n        self.prms = parameters\n\n    def plot_hotspot_communities(self, communities: typing.Union[None, list] = None,\n                                 shortest_labels=\"auto\") -&gt; None:\n        \"\"\"Run visualisation of hotspot list for each hotspot community.\n\n        Arguments:\n            communities (None | list): list of communities to be plotted.\n            shortest_labels (bool | auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n                proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n            if not communities:\n                communities = self.hotspots.communities.values()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Visualisation of hotspot communities using lovis4u...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(communities), suffix='%(index)d/%(max)d')\n            for hc in communities:\n                self.plot_hotspots(hc, vis_output_folder, shortest_labels=shortest_labels)\n                bar.next()\n            bar.finish()\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot hotspot communities.\") from error\n\n    def plot_hotspots(self, hotspot_ids: list, output_folder: str = \"default\",\n                      island_ids: typing.Union[None, list] = None,\n                      proteome_ids: typing.Union[None, list] = None,\n                      additional_annotation: typing.Union[None, dict] = None, keep_while_deduplication: list = [],\n                      shortest_labels: typing.Union[str, bool] = \"auto\", compact_mode: bool = False,\n                      keep_temp_data=True):\n        \"\"\"Visualise set of hotspots using Lovis4u.\n\n        Arguments:\n            hotspot_ids (list): List of hotspot ids to be plotted.\n            output_folder (str): Output folder to save pdf file.\n            island_ids (None | list): List of island ids. In case it's specified only listed islands will be plotted.\n            proteome_ids (None | list): List of proteome ids. In case it's specifiedd only listed proteome will\n                be plotted.\n            additional_annotation (dict): Additional LoVis4u feature annotation dict.\n            keep_while_deduplication (list): List of island ids to be kept during deduplication.\n            shortest_labels (bool| auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n                proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if output_folder == \"default\":\n                output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n            if not os.path.exists(output_folder):\n                os.mkdir(output_folder)\n            locus_annotation_rows, feature_annotation_rows, mmseqs_results_rows, gff_files = [], [], [], []\n            cds_table_rows = []\n            already_added_groups = []\n            hotspot_subset = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n            added_proteomes = []\n            for hotspot in hotspot_subset:\n                for h_island in hotspot.islands:\n                    if island_ids:\n                        if h_island.island_id not in island_ids:\n                            continue\n                    proteome = self.proteomes.proteomes.at[h_island.proteome]\n                    if proteome_ids:\n                        if proteome.proteome_id not in proteome_ids:\n                            continue\n                    if proteome.proteome_id in added_proteomes:\n                        continue\n                    added_proteomes.append(proteome.proteome_id)\n                    proteome_annotation = self.proteomes.annotation.loc[h_island.proteome]\n                    proteome_cdss = proteome.cdss.to_list()\n                    locus_indexes = h_island.get_all_locus_indexes(proteome.cdss)\n                    locus_groups = h_island.get_locus_groups(proteome.cdss)\n                    if locus_groups not in already_added_groups:\n                        already_added_groups.append(locus_groups)\n                    else:\n                        if h_island.island_id not in keep_while_deduplication:\n                            continue\n                    gff_files.append(proteome.gff_file)\n                    start_coordinate = proteome_cdss[locus_indexes[0]].start\n                    end_coordinate = proteome_cdss[locus_indexes[-1]].end\n                    if compact_mode:\n                        nf = 1\n                        start_coordinate = proteome_cdss[h_island.indexes[0] - nf].start  #\n                        end_coordinate = proteome_cdss[h_island.indexes[-1] + nf].end  #\n                    if end_coordinate &gt; start_coordinate:\n                        sequence_coordinate = f\"{start_coordinate}:{end_coordinate}:1\"\n                    else:\n                        sequence_coordinate = f\"{start_coordinate}:{proteome_annotation['length']}:1,1:{end_coordinate}:1\"\n                    locus_annotation_row = dict(sequence_id=h_island.proteome, coordinates=sequence_coordinate,\n                                                circular=proteome.circular, group=hotspot.proteome_community)\n                    if len(hotspot_ids) &gt; 1:\n                        locus_annotation_row[\"description\"] = f\"proteome community: {hotspot.proteome_community}\"\n                    locus_annotation_rows.append(locus_annotation_row)\n                    for cds_ind, cds in enumerate(proteome_cdss):\n                        if cds_ind in locus_indexes:\n                            short_id = cds.cds_id.replace(proteome.proteome_id, \"\").strip().strip(\"_\").strip(\"-\")\n                            if cds_ind not in h_island.indexes:\n                                if cds.g_class == \"conserved\":\n                                    group_type = \"conserved\"\n                                    fcolour = \"#8B9697\"  # attention\n                                else:\n                                    group_type = \"conserved\"\n                                    fcolour = \"#D3D5D6\"\n                                scolour = \"#000000\"\n                            else:\n                                fcolour = \"default\"\n                                scolour = \"default\"\n                                group_type = \"variable\"\n                                cds_table_row = dict(hotspot=hotspot.hotspot_id, sequence=h_island.proteome,\n                                                     island_index=h_island.indexes.index(cds_ind),\n                                                     group=cds.group,\n                                                     coordinates=f\"{cds.start}:{cds.end}:{cds.strand}\",\n                                                     cds_id=cds.cds_id, cds_type=cds.g_class,\n                                                     name=cds.name)\n                                if cds.hmmscan_results:\n                                    cds_table_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                    if \"db_name\" in cds.hmmscan_results.keys():\n                                        cds_table_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                    else:\n                                        cds_table_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                                cds_table_rows.append(cds_table_row)\n                            feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type,\n                                                          fill_colour=fcolour, stroke_colour=scolour,\n                                                          name=cds.name)\n                            if feature_annotation_row[\"name\"] == \"hypothetical protein\":\n                                feature_annotation_row[\"name\"] = \"\"\n                            if cds.hmmscan_results:\n                                feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                if \"db_name\" in cds.hmmscan_results.keys():\n                                    feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                else:\n                                    feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                            if additional_annotation:\n                                if cds.cds_id in additional_annotation.keys():\n                                    feature_annotation_row.update(additional_annotation[cds.cds_id])\n                            if feature_annotation_row[\"name\"]:\n                                if not compact_mode:\n                                    feature_annotation_row[\"name\"] += f\" ({short_id})\"\n                            else:\n                                if shortest_labels == True or \\\n                                        (shortest_labels == \"auto\" and len(h_island.indexes) &gt;= self.prms.args[\n                                            \"island_size_cutoff_to_show_index_only\"]):\n                                    feature_annotation_row[\"name\"] = str(cds_ind + 1)\n                                    if compact_mode:\n                                        feature_annotation_row[\"name\"] = \"\"\n                                else:\n                                    feature_annotation_row[\"name\"] = str(short_id)\n                            feature_annotation_rows.append(feature_annotation_row)\n                        mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n            cds_tables_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots_annotation\")\n            if not os.path.exists(cds_tables_folder):\n                os.mkdir(cds_tables_folder)\n            cds_table = pd.DataFrame(cds_table_rows)\n            table_name = f\"{'_'.join(hotspot_ids)}\"\n            if len(table_name) &gt; 200:\n                table_name = f\"{table_name[:200]}..._{hotspot_ids[-1]}\"\n            cds_table.to_csv(os.path.join(cds_tables_folder, f\"{table_name}.tsv\"), sep=\"\\t\", index=False)\n\n            locus_annotation_t = pd.DataFrame(locus_annotation_rows)\n            feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n            temp_input_f = tempfile.NamedTemporaryFile()\n            temp_input_l = tempfile.NamedTemporaryFile()\n            locus_annotation_t.to_csv(temp_input_l.name, sep=\"\\t\", index=False)\n            feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n\n            l_parameters = lovis4u.Manager.Parameters()\n            if compact_mode:\n                l_parameters.load_config(\"A4p1\")\n            else:\n                l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n            # l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n            l_parameters.args[\"cluster_all_proteins\"] = False\n            l_parameters.args[\"locus_label_style\"] = \"id\"\n            l_parameters.args[\"locus_label_position\"] = \"bottom\"\n            l_parameters.args[\"verbose\"] = False\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n\n            l_parameters.args[\"draw_middle_line\"] = True\n            l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n            l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n\n            if compact_mode:\n                l_parameters.args[\"feature_group_types_to_show_label\"] = []\n                l_parameters.args[\"feature_group_types_to_show_label_on_first_occurrence\"] = [\"conserved\", \"variable\"]\n                l_parameters.args[\"draw_individual_x_axis\"] = False\n\n            loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n            loci.load_feature_annotation_file(temp_input_f.name)\n            loci.load_locus_annotation_file(temp_input_l.name)\n\n            mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n            loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n            loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n            loci.reorient_loci(ilund4u_mode=True)\n            loci.set_feature_colours_based_on_groups()\n            loci.set_category_colours()\n            loci.define_labels_to_be_shown()\n\n            loci.save_feature_annotation_table()\n            loci.save_locus_annotation_table()\n\n            canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n            canvas_manager.define_layout(loci)\n            canvas_manager.add_loci_tracks(loci)\n            canvas_manager.add_scale_line_track()\n            canvas_manager.add_categories_colour_legend_track(loci)\n            canvas_manager.add_homology_track()\n            pdf_name = f\"{'_'.join(hotspot_ids)}\"\n            if len(pdf_name) &gt; 200:\n                pdf_name = f\"{pdf_name[:200]}..._{hotspot_ids[-1]}\"\n            canvas_manager.plot(f\"{pdf_name}.pdf\")\n            os.system(f\"mv {l_parameters.args['output_dir']}/{pdf_name}.pdf {output_folder}/\")\n            if keep_temp_data:\n                if not os.path.exists(os.path.join(output_folder, \"lovis4u_output\")):\n                    os.mkdir(os.path.join(output_folder, \"lovis4u_output\"))\n                os.system(f\"mv {l_parameters.args['output_dir']} \"\n                          f\"{os.path.join(output_folder, 'lovis4u_output', pdf_name)}\")\n                os.mkdir(os.path.join(output_folder, \"lovis4u_output\", pdf_name, \"gff_files\"))\n                for gff_file in gff_files:\n                    os.system(f\"cp '{gff_file}' {os.path.join(output_folder, 'lovis4u_output', pdf_name, 'gff_files')}/\")\n            else:\n                shutil.rmtree(l_parameters.args[\"output_dir\"])\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot set of hotspots using LoVis4u.\") from error\n\n    def plot_proteome_communities(self):\n        \"\"\"Run visualisation of proteome list for each proteome community.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Visualisation of proteome communities with corresponding hotspots using lovis4u...\",\n                      file=sys.stdout)\n                vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_proteome_communities\")\n                if os.path.exists(vis_output_folder):\n                    shutil.rmtree(vis_output_folder)\n                os.mkdir(vis_output_folder)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.communities.keys()),\n                                                     suffix='%(index)d/%(max)d')\n                for com_id, com_pr_ids in self.proteomes.communities.items():\n                    bar.next()\n                    self.plot_proteome_community(com_id, vis_output_folder)\n                bar.finish()\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot proteome communities.\") from error\n\n    def plot_proteome_community(self, community: int, output_folder: str, mode: str = \"hotspot\",\n                                proteome_ids: typing.Union[None, list] = None,\n                                additional_annotation: typing.Union[None, dict] = None,\n                                filename: typing.Union[None, str] = None):\n        \"\"\"Visualise proteome community using LoVis4u.\n\n        Arguments:\n            community (int): Id of proteome community to be plotted.\n            output_folder (str): Output folder to save pdf file.\n            mode (str): Mode of visualisation.\n            proteome_ids: (None | list): List of proteome ids. In case it's specified only listed proteomes will\n                be plotted.\n            additional_annotation (dict): Additional LoVis4u feature annotation dict.\n            filename (None | str): Pdf file name. If not specified id of community will be used.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            community_proteomes = self.proteomes.communities[community]\n            if mode == \"hotspot\":\n                hotspot_annotation_com = self.hotspots.annotation[\n                    self.hotspots.annotation[\"proteome_community\"] == community]\n                num_of_hotspots = len(hotspot_annotation_com.index)\n                colours_rgb = seaborn.color_palette(\"husl\", num_of_hotspots, desat=1)\n                colours = list(map(lambda x: matplotlib.colors.rgb2hex(x), colours_rgb))\n                colours_dict = ({g: c for g, c in zip(list(hotspot_annotation_com.index.to_list()), colours)})\n                com_hotspots = self.hotspots.hotspots.loc[hotspot_annotation_com.index]\n                island_proteins_d = dict()\n                for hotspot in com_hotspots.to_list():\n                    for island in hotspot.islands:\n                        proteome = self.proteomes.proteomes.at[island.proteome]\n                        island_indexes = island.indexes\n                        island_cds_ids = proteome.cdss.iloc[island_indexes].apply(lambda cds: cds.cds_id).to_list()\n                        for ic_id in island_cds_ids:\n                            island_proteins_d[ic_id] = hotspot.hotspot_id\n            gff_files = []\n            feature_annotation_rows = []\n            mmseqs_results_rows = []\n            n_of_added_proteomes = 0\n            for proteome_id in community_proteomes:\n                if proteome_ids:\n                    if proteome_id not in proteome_ids:\n                        continue\n                n_of_added_proteomes += 1\n                proteome = self.proteomes.proteomes.at[proteome_id]\n                gff_files.append(proteome.gff_file)\n                for cds_ind, cds in enumerate(proteome.cdss.to_list()):\n                    if cds.g_class == \"conserved\":\n                        group_type = \"conserved\"\n                    else:\n                        group_type = \"variable\"\n                    if mode == \"hotspot\":\n                        if cds.cds_id in island_proteins_d.keys():\n                            fcolour = colours_dict[island_proteins_d[cds.cds_id]]\n                        else:\n                            if cds.g_class == \"conserved\":\n                                fcolour = \"#BDC6CA\"\n                            else:\n                                fcolour = \"#8C9295\"\n                    feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type)\n                    if mode == \"hotspot\":\n                        feature_annotation_row[\"show_label\"] = 0\n                        feature_annotation_row[\"stroke_colour\"] = \"#000000\"\n                        feature_annotation_row[\"fill_colour\"] = fcolour\n                    if cds.hmmscan_results and self.prms.args[\"show_hmmscan_hits_on_full_proteomes\"]:\n                        feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                        if \"db_name\" in cds.hmmscan_results.keys():\n                            feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                        else:\n                            feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"].lower()\n                    if additional_annotation:\n                        if cds.cds_id in additional_annotation.keys():\n                            feature_annotation_row.update(additional_annotation[cds.cds_id])\n                    feature_annotation_rows.append(feature_annotation_row)\n                    mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n            l_parameters = lovis4u.Manager.Parameters()\n            l_parameters.load_config(self.prms.args[\"lovis4u_proteome_config_filename\"])\n            l_parameters.args[\"cluster_all_proteins\"] = False\n            if n_of_added_proteomes &gt; 1:\n                l_parameters.args[\"draw_individual_x_axis\"] = False\n            else:\n                l_parameters.args[\"draw_individual_x_axis\"] = True\n            l_parameters.args[\"verbose\"] = False\n            l_parameters.args[\"locus_label_style\"] = \"id\"\n            if mode == \"hotspot\" and n_of_added_proteomes != 1:\n                l_parameters.args[\"gff_CDS_category_source\"] = \"-\"\n            l_parameters.args[\"draw_middle_line\"] = False\n            l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n            l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n            if os.path.exists(l_parameters.args[\"output_dir\"]):\n                shutil.rmtree(l_parameters.args[\"output_dir\"])\n            os.mkdir(l_parameters.args[\"output_dir\"])\n            loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n            feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n            temp_input_f = tempfile.NamedTemporaryFile()\n            feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n            loci.load_feature_annotation_file(temp_input_f.name)\n            mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n            loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n            if len(gff_files) &lt;= self.prms.args[\"max_number_of_seqs_to_redefine_order\"]:\n                loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n            loci.reorient_loci(ilund4u_mode=True)\n            if mode == \"regular\" or n_of_added_proteomes == 1:\n                loci.define_labels_to_be_shown()\n            loci.set_feature_colours_based_on_groups()\n            loci.set_category_colours()\n            loci.save_feature_annotation_table()\n            canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n            canvas_manager.define_layout(loci)\n            canvas_manager.add_loci_tracks(loci)\n            if n_of_added_proteomes &gt; 1:\n                canvas_manager.add_scale_line_track()\n            canvas_manager.add_categories_colour_legend_track(loci)\n            canvas_manager.add_homology_track()\n            if not filename:\n                filename = f\"{community}.pdf\"\n            canvas_manager.plot(filename)\n            os.system(f\"mv {l_parameters.args['output_dir']}/{filename} {output_folder}/\")\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to plot proteome community {community}.\") from error\n</code></pre>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager.__init__","title":"<code>__init__(proteomes, hotspots, parameters)</code>","text":"<p>DrawingManager class constructor</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def __init__(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n             parameters: ilund4u.manager.Parameters):\n    \"\"\"DrawingManager class constructor\n\n    Arguments:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = proteomes\n    self.hotspots = hotspots\n    self.prms = parameters\n</code></pre>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager.plot_hotspot_communities","title":"<code>plot_hotspot_communities(communities=None, shortest_labels='auto')</code>","text":"<p>Run visualisation of hotspot list for each hotspot community.</p> <p>Parameters:</p> <ul> <li> <code>communities</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>list of communities to be plotted.</p> </li> <li> <code>shortest_labels</code>             (<code>bool | auto</code>, default:                 <code>'auto'</code> )         \u2013          <p>Whether to put 1-based cds index only instead of CDS id for hypothetical proteins.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_hotspot_communities(self, communities: typing.Union[None, list] = None,\n                             shortest_labels=\"auto\") -&gt; None:\n    \"\"\"Run visualisation of hotspot list for each hotspot community.\n\n    Arguments:\n        communities (None | list): list of communities to be plotted.\n        shortest_labels (bool | auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n            proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n        if os.path.exists(vis_output_folder):\n            shutil.rmtree(vis_output_folder)\n        os.mkdir(vis_output_folder)\n        if not communities:\n            communities = self.hotspots.communities.values()\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Visualisation of hotspot communities using lovis4u...\", file=sys.stdout)\n        bar = progress.bar.FillingCirclesBar(\" \", max=len(communities), suffix='%(index)d/%(max)d')\n        for hc in communities:\n            self.plot_hotspots(hc, vis_output_folder, shortest_labels=shortest_labels)\n            bar.next()\n        bar.finish()\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot hotspot communities.\") from error\n</code></pre>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager.plot_hotspots","title":"<code>plot_hotspots(hotspot_ids, output_folder='default', island_ids=None, proteome_ids=None, additional_annotation=None, keep_while_deduplication=[], shortest_labels='auto', compact_mode=False, keep_temp_data=True)</code>","text":"<p>Visualise set of hotspots using Lovis4u.</p> <p>Parameters:</p> <ul> <li> <code>hotspot_ids</code>             (<code>list</code>)         \u2013          <p>List of hotspot ids to be plotted.</p> </li> <li> <code>output_folder</code>             (<code>str</code>, default:                 <code>'default'</code> )         \u2013          <p>Output folder to save pdf file.</p> </li> <li> <code>island_ids</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>List of island ids. In case it's specified only listed islands will be plotted.</p> </li> <li> <code>proteome_ids</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>List of proteome ids. In case it's specifiedd only listed proteome will be plotted.</p> </li> <li> <code>additional_annotation</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Additional LoVis4u feature annotation dict.</p> </li> <li> <code>keep_while_deduplication</code>             (<code>list</code>, default:                 <code>[]</code> )         \u2013          <p>List of island ids to be kept during deduplication.</p> </li> <li> <code>shortest_labels</code>             (<code>bool | auto</code>, default:                 <code>'auto'</code> )         \u2013          <p>Whether to put 1-based cds index only instead of CDS id for hypothetical proteins.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_hotspots(self, hotspot_ids: list, output_folder: str = \"default\",\n                  island_ids: typing.Union[None, list] = None,\n                  proteome_ids: typing.Union[None, list] = None,\n                  additional_annotation: typing.Union[None, dict] = None, keep_while_deduplication: list = [],\n                  shortest_labels: typing.Union[str, bool] = \"auto\", compact_mode: bool = False,\n                  keep_temp_data=True):\n    \"\"\"Visualise set of hotspots using Lovis4u.\n\n    Arguments:\n        hotspot_ids (list): List of hotspot ids to be plotted.\n        output_folder (str): Output folder to save pdf file.\n        island_ids (None | list): List of island ids. In case it's specified only listed islands will be plotted.\n        proteome_ids (None | list): List of proteome ids. In case it's specifiedd only listed proteome will\n            be plotted.\n        additional_annotation (dict): Additional LoVis4u feature annotation dict.\n        keep_while_deduplication (list): List of island ids to be kept during deduplication.\n        shortest_labels (bool| auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n            proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if output_folder == \"default\":\n            output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n        if not os.path.exists(output_folder):\n            os.mkdir(output_folder)\n        locus_annotation_rows, feature_annotation_rows, mmseqs_results_rows, gff_files = [], [], [], []\n        cds_table_rows = []\n        already_added_groups = []\n        hotspot_subset = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n        added_proteomes = []\n        for hotspot in hotspot_subset:\n            for h_island in hotspot.islands:\n                if island_ids:\n                    if h_island.island_id not in island_ids:\n                        continue\n                proteome = self.proteomes.proteomes.at[h_island.proteome]\n                if proteome_ids:\n                    if proteome.proteome_id not in proteome_ids:\n                        continue\n                if proteome.proteome_id in added_proteomes:\n                    continue\n                added_proteomes.append(proteome.proteome_id)\n                proteome_annotation = self.proteomes.annotation.loc[h_island.proteome]\n                proteome_cdss = proteome.cdss.to_list()\n                locus_indexes = h_island.get_all_locus_indexes(proteome.cdss)\n                locus_groups = h_island.get_locus_groups(proteome.cdss)\n                if locus_groups not in already_added_groups:\n                    already_added_groups.append(locus_groups)\n                else:\n                    if h_island.island_id not in keep_while_deduplication:\n                        continue\n                gff_files.append(proteome.gff_file)\n                start_coordinate = proteome_cdss[locus_indexes[0]].start\n                end_coordinate = proteome_cdss[locus_indexes[-1]].end\n                if compact_mode:\n                    nf = 1\n                    start_coordinate = proteome_cdss[h_island.indexes[0] - nf].start  #\n                    end_coordinate = proteome_cdss[h_island.indexes[-1] + nf].end  #\n                if end_coordinate &gt; start_coordinate:\n                    sequence_coordinate = f\"{start_coordinate}:{end_coordinate}:1\"\n                else:\n                    sequence_coordinate = f\"{start_coordinate}:{proteome_annotation['length']}:1,1:{end_coordinate}:1\"\n                locus_annotation_row = dict(sequence_id=h_island.proteome, coordinates=sequence_coordinate,\n                                            circular=proteome.circular, group=hotspot.proteome_community)\n                if len(hotspot_ids) &gt; 1:\n                    locus_annotation_row[\"description\"] = f\"proteome community: {hotspot.proteome_community}\"\n                locus_annotation_rows.append(locus_annotation_row)\n                for cds_ind, cds in enumerate(proteome_cdss):\n                    if cds_ind in locus_indexes:\n                        short_id = cds.cds_id.replace(proteome.proteome_id, \"\").strip().strip(\"_\").strip(\"-\")\n                        if cds_ind not in h_island.indexes:\n                            if cds.g_class == \"conserved\":\n                                group_type = \"conserved\"\n                                fcolour = \"#8B9697\"  # attention\n                            else:\n                                group_type = \"conserved\"\n                                fcolour = \"#D3D5D6\"\n                            scolour = \"#000000\"\n                        else:\n                            fcolour = \"default\"\n                            scolour = \"default\"\n                            group_type = \"variable\"\n                            cds_table_row = dict(hotspot=hotspot.hotspot_id, sequence=h_island.proteome,\n                                                 island_index=h_island.indexes.index(cds_ind),\n                                                 group=cds.group,\n                                                 coordinates=f\"{cds.start}:{cds.end}:{cds.strand}\",\n                                                 cds_id=cds.cds_id, cds_type=cds.g_class,\n                                                 name=cds.name)\n                            if cds.hmmscan_results:\n                                cds_table_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                if \"db_name\" in cds.hmmscan_results.keys():\n                                    cds_table_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                else:\n                                    cds_table_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                            cds_table_rows.append(cds_table_row)\n                        feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type,\n                                                      fill_colour=fcolour, stroke_colour=scolour,\n                                                      name=cds.name)\n                        if feature_annotation_row[\"name\"] == \"hypothetical protein\":\n                            feature_annotation_row[\"name\"] = \"\"\n                        if cds.hmmscan_results:\n                            feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                            if \"db_name\" in cds.hmmscan_results.keys():\n                                feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                            else:\n                                feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                        if additional_annotation:\n                            if cds.cds_id in additional_annotation.keys():\n                                feature_annotation_row.update(additional_annotation[cds.cds_id])\n                        if feature_annotation_row[\"name\"]:\n                            if not compact_mode:\n                                feature_annotation_row[\"name\"] += f\" ({short_id})\"\n                        else:\n                            if shortest_labels == True or \\\n                                    (shortest_labels == \"auto\" and len(h_island.indexes) &gt;= self.prms.args[\n                                        \"island_size_cutoff_to_show_index_only\"]):\n                                feature_annotation_row[\"name\"] = str(cds_ind + 1)\n                                if compact_mode:\n                                    feature_annotation_row[\"name\"] = \"\"\n                            else:\n                                feature_annotation_row[\"name\"] = str(short_id)\n                        feature_annotation_rows.append(feature_annotation_row)\n                    mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n        cds_tables_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots_annotation\")\n        if not os.path.exists(cds_tables_folder):\n            os.mkdir(cds_tables_folder)\n        cds_table = pd.DataFrame(cds_table_rows)\n        table_name = f\"{'_'.join(hotspot_ids)}\"\n        if len(table_name) &gt; 200:\n            table_name = f\"{table_name[:200]}..._{hotspot_ids[-1]}\"\n        cds_table.to_csv(os.path.join(cds_tables_folder, f\"{table_name}.tsv\"), sep=\"\\t\", index=False)\n\n        locus_annotation_t = pd.DataFrame(locus_annotation_rows)\n        feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n        temp_input_f = tempfile.NamedTemporaryFile()\n        temp_input_l = tempfile.NamedTemporaryFile()\n        locus_annotation_t.to_csv(temp_input_l.name, sep=\"\\t\", index=False)\n        feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n\n        l_parameters = lovis4u.Manager.Parameters()\n        if compact_mode:\n            l_parameters.load_config(\"A4p1\")\n        else:\n            l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n        # l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n        l_parameters.args[\"cluster_all_proteins\"] = False\n        l_parameters.args[\"locus_label_style\"] = \"id\"\n        l_parameters.args[\"locus_label_position\"] = \"bottom\"\n        l_parameters.args[\"verbose\"] = False\n        l_parameters.args[\"draw_individual_x_axis\"] = False\n\n        l_parameters.args[\"draw_middle_line\"] = True\n        l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n        l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n\n        if compact_mode:\n            l_parameters.args[\"feature_group_types_to_show_label\"] = []\n            l_parameters.args[\"feature_group_types_to_show_label_on_first_occurrence\"] = [\"conserved\", \"variable\"]\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n\n        loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n        loci.load_feature_annotation_file(temp_input_f.name)\n        loci.load_locus_annotation_file(temp_input_l.name)\n\n        mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n        loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n        loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n        loci.reorient_loci(ilund4u_mode=True)\n        loci.set_feature_colours_based_on_groups()\n        loci.set_category_colours()\n        loci.define_labels_to_be_shown()\n\n        loci.save_feature_annotation_table()\n        loci.save_locus_annotation_table()\n\n        canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n        canvas_manager.define_layout(loci)\n        canvas_manager.add_loci_tracks(loci)\n        canvas_manager.add_scale_line_track()\n        canvas_manager.add_categories_colour_legend_track(loci)\n        canvas_manager.add_homology_track()\n        pdf_name = f\"{'_'.join(hotspot_ids)}\"\n        if len(pdf_name) &gt; 200:\n            pdf_name = f\"{pdf_name[:200]}..._{hotspot_ids[-1]}\"\n        canvas_manager.plot(f\"{pdf_name}.pdf\")\n        os.system(f\"mv {l_parameters.args['output_dir']}/{pdf_name}.pdf {output_folder}/\")\n        if keep_temp_data:\n            if not os.path.exists(os.path.join(output_folder, \"lovis4u_output\")):\n                os.mkdir(os.path.join(output_folder, \"lovis4u_output\"))\n            os.system(f\"mv {l_parameters.args['output_dir']} \"\n                      f\"{os.path.join(output_folder, 'lovis4u_output', pdf_name)}\")\n            os.mkdir(os.path.join(output_folder, \"lovis4u_output\", pdf_name, \"gff_files\"))\n            for gff_file in gff_files:\n                os.system(f\"cp '{gff_file}' {os.path.join(output_folder, 'lovis4u_output', pdf_name, 'gff_files')}/\")\n        else:\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot set of hotspots using LoVis4u.\") from error\n</code></pre>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager.plot_proteome_communities","title":"<code>plot_proteome_communities()</code>","text":"<p>Run visualisation of proteome list for each proteome community.</p> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_proteome_communities(self):\n    \"\"\"Run visualisation of proteome list for each proteome community.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Visualisation of proteome communities with corresponding hotspots using lovis4u...\",\n                  file=sys.stdout)\n            vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_proteome_communities\")\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.communities.keys()),\n                                                 suffix='%(index)d/%(max)d')\n            for com_id, com_pr_ids in self.proteomes.communities.items():\n                bar.next()\n                self.plot_proteome_community(com_id, vis_output_folder)\n            bar.finish()\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot proteome communities.\") from error\n</code></pre>"},{"location":"API/drawing/#ilund4u.drawing.DrawingManager.plot_proteome_community","title":"<code>plot_proteome_community(community, output_folder, mode='hotspot', proteome_ids=None, additional_annotation=None, filename=None)</code>","text":"<p>Visualise proteome community using LoVis4u.</p> <p>Parameters:</p> <ul> <li> <code>community</code>             (<code>int</code>)         \u2013          <p>Id of proteome community to be plotted.</p> </li> <li> <code>output_folder</code>             (<code>str</code>)         \u2013          <p>Output folder to save pdf file.</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'hotspot'</code> )         \u2013          <p>Mode of visualisation.</p> </li> <li> <code>proteome_ids</code>             (<code>Union[None, list]</code>, default:                 <code>None</code> )         \u2013          <p>(None | list): List of proteome ids. In case it's specified only listed proteomes will be plotted.</p> </li> <li> <code>additional_annotation</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Additional LoVis4u feature annotation dict.</p> </li> <li> <code>filename</code>             (<code>None | str</code>, default:                 <code>None</code> )         \u2013          <p>Pdf file name. If not specified id of community will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_proteome_community(self, community: int, output_folder: str, mode: str = \"hotspot\",\n                            proteome_ids: typing.Union[None, list] = None,\n                            additional_annotation: typing.Union[None, dict] = None,\n                            filename: typing.Union[None, str] = None):\n    \"\"\"Visualise proteome community using LoVis4u.\n\n    Arguments:\n        community (int): Id of proteome community to be plotted.\n        output_folder (str): Output folder to save pdf file.\n        mode (str): Mode of visualisation.\n        proteome_ids: (None | list): List of proteome ids. In case it's specified only listed proteomes will\n            be plotted.\n        additional_annotation (dict): Additional LoVis4u feature annotation dict.\n        filename (None | str): Pdf file name. If not specified id of community will be used.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        community_proteomes = self.proteomes.communities[community]\n        if mode == \"hotspot\":\n            hotspot_annotation_com = self.hotspots.annotation[\n                self.hotspots.annotation[\"proteome_community\"] == community]\n            num_of_hotspots = len(hotspot_annotation_com.index)\n            colours_rgb = seaborn.color_palette(\"husl\", num_of_hotspots, desat=1)\n            colours = list(map(lambda x: matplotlib.colors.rgb2hex(x), colours_rgb))\n            colours_dict = ({g: c for g, c in zip(list(hotspot_annotation_com.index.to_list()), colours)})\n            com_hotspots = self.hotspots.hotspots.loc[hotspot_annotation_com.index]\n            island_proteins_d = dict()\n            for hotspot in com_hotspots.to_list():\n                for island in hotspot.islands:\n                    proteome = self.proteomes.proteomes.at[island.proteome]\n                    island_indexes = island.indexes\n                    island_cds_ids = proteome.cdss.iloc[island_indexes].apply(lambda cds: cds.cds_id).to_list()\n                    for ic_id in island_cds_ids:\n                        island_proteins_d[ic_id] = hotspot.hotspot_id\n        gff_files = []\n        feature_annotation_rows = []\n        mmseqs_results_rows = []\n        n_of_added_proteomes = 0\n        for proteome_id in community_proteomes:\n            if proteome_ids:\n                if proteome_id not in proteome_ids:\n                    continue\n            n_of_added_proteomes += 1\n            proteome = self.proteomes.proteomes.at[proteome_id]\n            gff_files.append(proteome.gff_file)\n            for cds_ind, cds in enumerate(proteome.cdss.to_list()):\n                if cds.g_class == \"conserved\":\n                    group_type = \"conserved\"\n                else:\n                    group_type = \"variable\"\n                if mode == \"hotspot\":\n                    if cds.cds_id in island_proteins_d.keys():\n                        fcolour = colours_dict[island_proteins_d[cds.cds_id]]\n                    else:\n                        if cds.g_class == \"conserved\":\n                            fcolour = \"#BDC6CA\"\n                        else:\n                            fcolour = \"#8C9295\"\n                feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type)\n                if mode == \"hotspot\":\n                    feature_annotation_row[\"show_label\"] = 0\n                    feature_annotation_row[\"stroke_colour\"] = \"#000000\"\n                    feature_annotation_row[\"fill_colour\"] = fcolour\n                if cds.hmmscan_results and self.prms.args[\"show_hmmscan_hits_on_full_proteomes\"]:\n                    feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                    if \"db_name\" in cds.hmmscan_results.keys():\n                        feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                    else:\n                        feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"].lower()\n                if additional_annotation:\n                    if cds.cds_id in additional_annotation.keys():\n                        feature_annotation_row.update(additional_annotation[cds.cds_id])\n                feature_annotation_rows.append(feature_annotation_row)\n                mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n        l_parameters = lovis4u.Manager.Parameters()\n        l_parameters.load_config(self.prms.args[\"lovis4u_proteome_config_filename\"])\n        l_parameters.args[\"cluster_all_proteins\"] = False\n        if n_of_added_proteomes &gt; 1:\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n        else:\n            l_parameters.args[\"draw_individual_x_axis\"] = True\n        l_parameters.args[\"verbose\"] = False\n        l_parameters.args[\"locus_label_style\"] = \"id\"\n        if mode == \"hotspot\" and n_of_added_proteomes != 1:\n            l_parameters.args[\"gff_CDS_category_source\"] = \"-\"\n        l_parameters.args[\"draw_middle_line\"] = False\n        l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n        l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n        if os.path.exists(l_parameters.args[\"output_dir\"]):\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n        os.mkdir(l_parameters.args[\"output_dir\"])\n        loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n        feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n        temp_input_f = tempfile.NamedTemporaryFile()\n        feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n        loci.load_feature_annotation_file(temp_input_f.name)\n        mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n        loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n        if len(gff_files) &lt;= self.prms.args[\"max_number_of_seqs_to_redefine_order\"]:\n            loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n        loci.reorient_loci(ilund4u_mode=True)\n        if mode == \"regular\" or n_of_added_proteomes == 1:\n            loci.define_labels_to_be_shown()\n        loci.set_feature_colours_based_on_groups()\n        loci.set_category_colours()\n        loci.save_feature_annotation_table()\n        canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n        canvas_manager.define_layout(loci)\n        canvas_manager.add_loci_tracks(loci)\n        if n_of_added_proteomes &gt; 1:\n            canvas_manager.add_scale_line_track()\n        canvas_manager.add_categories_colour_legend_track(loci)\n        canvas_manager.add_homology_track()\n        if not filename:\n            filename = f\"{community}.pdf\"\n        canvas_manager.plot(filename)\n        os.system(f\"mv {l_parameters.args['output_dir']}/{filename} {output_folder}/\")\n        shutil.rmtree(l_parameters.args[\"output_dir\"])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to plot proteome community {community}.\") from error\n</code></pre>"},{"location":"API/manager/","title":"Manager","text":"<p>This module provides managing classes and methods for the tool.</p>"},{"location":"API/manager/#ilund4u.manager.Parameters","title":"<code>Parameters</code>","text":"<p>A Parameters object holds and parse command line and config arguments.</p> <p>A Parameters object have to be created in each script since it's used almost by each     class of the tool as a mandatory argument.</p> <p>Attributes:</p> <ul> <li> <code>args</code>             (<code>dict</code>)         \u2013          <p>dictionary that holds all arguments.</p> </li> <li> <code>cmd_arguments</code>             (<code>dict</code>)         \u2013          <p>dictionary wich command-line arguments.</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>class Parameters:\n    \"\"\"A Parameters object holds and parse command line and config arguments.\n\n    A Parameters object have to be created in each script since it's used almost by each\n        class of the tool as a mandatory argument.\n\n    Attributes:\n        args (dict): dictionary that holds all arguments.\n        cmd_arguments (dict): dictionary wich command-line arguments.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Parameters class constructor.\n\n        \"\"\"\n        self.args = dict(debug=True)\n        self.cmd_arguments = dict()\n\n    def parse_cmd_arguments(self) -&gt; None:\n        \"\"\"Parse command-line arguments.\n\n        Returns:\n            None\n\n        \"\"\"\n        parser = argparse.ArgumentParser(prog=\"ilund4u\", add_help=False)\n        parser.add_argument(\"-data\", \"--data\", dest=\"ilund4u_data\", action=\"store_true\")\n        parser.add_argument(\"-get-hmms\", \"--get-hmms\", dest=\"get_hmms\", action=\"store_true\")\n        parser.add_argument(\"-database\", \"--database\", dest=\"get_database\", default=None, type=str,\n                            choices=[\"phages\", \"plasmids\"])\n\n        parser.add_argument(\"-linux\", \"--linux\", dest=\"linux\", action=\"store_true\", default=None)\n        parser.add_argument(\"-mac\", \"--mac\", dest=\"mac\", action=\"store_true\", default=None)\n        parser.add_argument(\"-h\", \"--help\", dest=\"help\", action=\"store_true\")\n        parser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 0.0.8\")\n\n        subparsers = parser.add_subparsers(dest=\"mode\")\n\n        parser_hm = subparsers.add_parser(\"hotspots\")\n        parser_hm.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n        parser_hm.add_argument(\"-ufid\", \"--use-filename-as-id\", dest=\"use_filename_as_contig_id\", action=\"store_true\",\n                               default=None)\n        parser_hm.add_argument(\"-mps\", \"--min-proteome-size\", dest=\"min_proteome_size\", type=int, default=None)\n        parser_hm.add_argument(\"-gct\", \"--genome-circularity-table\", dest=\"genome_annotation\", type=str, default=None)\n        parser_hm.add_argument(\"-psc\", \"--proteome-sim-cutoff\", dest=\"proteome_similarity_cutoff\", type=float,\n                               default=None)\n        parser_hm.add_argument(\"-mpcs\", \"--min-proteome-community-size\", dest=\"min_proteome_community_size\", type=int,\n                               default=None)\n        parser_hm.add_argument(\"-vpc\", \"--variable-protein-cutoff\", dest=\"variable_protein_cluster_cutoff\", type=float,\n                               default=None)\n        parser_hm.add_argument(\"-cpc\", \"--conserved-protein-cutoff\", dest=\"conserved_protein_cluster_cutoff\",\n                               type=float, default=None)\n        parser_hm.add_argument(\"-cg\", \"--circular-genomes\", dest=\"circular_genomes\", default=None,\n                               action=\"store_true\")\n        parser_hm.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=None,\n                               action=\"store_false\")\n        parser_hm.add_argument(\"-hpc\", \"--hotspot-presence-cutoff\", dest=\"hotspot_presence_cutoff\",\n                               type=float, default=None)\n        parser_hm.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_hm.add_argument(\"-o-db\", \"--output-database\", dest=\"output_database\", type=str, default=None)\n        parser_hm.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_hm.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_hm.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_hm.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n        parser_hm.add_argument(\"--parsing-debug\", \"-parsing-debug\", dest=\"parsing_debug\", action=\"store_true\")\n\n        parser_ps = subparsers.add_parser(\"protein\")\n        parser_ps.add_argument(\"-fa\", \"--fa\", dest=\"fa\", type=str, default=None)\n        parser_ps.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n        parser_ps.add_argument(\"-ql\", \"--query-label\", dest=\"query_label\", type=str, default=None)\n        parser_ps.add_argument(\"-hsm\", \"--homology-search-mode\", dest=\"protein_search_target_mode\", type=str,\n                               choices=[\"group\", \"proteins\"], default=\"group\")\n        parser_ps.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n        parser_ps.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n        parser_ps.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n        parser_ps.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n        parser_ps.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                               default=None)\n        parser_ps.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_ps.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_ps.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_ps.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_ps.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n        parser_pa = subparsers.add_parser(\"proteome\")\n        parser_pa.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n        parser_pa.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n        parser_pa.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=True,\n                               action=\"store_false\")\n        parser_pa.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n        parser_pa.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n        parser_pa.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n        parser_pa.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n        parser_pa.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                               default=None)\n        parser_pa.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_pa.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_pa.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_pa.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_pa.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n        args = vars(parser.parse_args())\n        if len(sys.argv[1:]) == 0:\n            args[\"help\"] = True\n        if args[\"ilund4u_data\"]:\n            ilund4u.methods.copy_package_data()\n            sys.exit()\n        if args[\"linux\"]:\n            ilund4u.methods.adjust_paths(\"linux\")\n            sys.exit()\n        if args[\"mac\"]:\n            ilund4u.methods.adjust_paths(\"mac\")\n            sys.exit()\n        if args[\"get_hmms\"]:\n            self.load_config()\n            ilund4u.methods.get_HMM_models(self.args)\n            sys.exit()\n        if args[\"get_database\"]:\n            self.load_config()\n            ilund4u.methods.get_ilund4u_db(self.args, args[\"get_database\"])\n            sys.exit()\n        if args[\"help\"]:\n            if not args[\"mode\"]:\n                help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"help_main.txt\")\n            else:\n                help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", f\"help_{args['mode']}.txt\")\n            with open(help_message_path, \"r\") as help_message:\n                print(help_message.read(), file=sys.stdout)\n                sys.exit()\n        if args[\"mode\"] == \"hotspots\":\n            if not args[\"gff\"]:\n                raise ilund4uError(\"-gff argument is required for hotspots mode.\")\n        elif args[\"mode\"] == \"protein\":\n            if not args[\"fa\"] or not args[\"database\"]:\n                raise ilund4uError(\"Both -fa/--fa and -db/--database arguments are required for protein mode.\")\n        elif args[\"mode\"] == \"proteome\":\n            if not args[\"gff\"] or not args[\"database\"]:\n                raise ilund4uError(\"Both -gff/--gff and -db/--database arguments are required for protein mode.\")\n\n        args_to_keep = [\"gff\", \"output_database\", \"query_label\", \"genome_annotation\"]\n        filtered_args = {k: v for k, v in args.items() if v is not None or k in args_to_keep}\n        self.cmd_arguments = filtered_args\n        return None\n\n    def load_config(self, path: str = \"standard\") -&gt; None:\n        \"\"\"Load configuration file.\n\n        Arguments\n            path (str): path to a config file or name (only standard available at this moment).\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if path == \"standard\":\n                path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"standard.cfg\")\n            config = configs.load(path).get_config()\n            internal_dir = os.path.dirname(__file__)\n            for key in config[\"root\"].keys():\n                if type(config[\"root\"][key]) is str and \"{internal}\" in config[\"root\"][key]:\n                    config[\"root\"][key] = config[\"root\"][key].replace(\"{internal}\",\n                                                                      os.path.join(internal_dir, \"ilund4u_data\"))\n            config[\"root\"][\"output_dir\"] = config[\"root\"][\"output_dir\"].replace(\"{current_date}\",\n                                                                                time.strftime(\"%Y_%m_%d-%H_%M\"))\n            keys_to_transform_to_list = []\n            for ktl in keys_to_transform_to_list:\n                if isinstance(config[\"root\"][ktl], str):\n                    if config[\"root\"][ktl] != \"None\":\n                        config[\"root\"][ktl] = [config[\"root\"][ktl]]\n                    else:\n                        config[\"root\"][ktl] = []\n            self.args.update(config[\"root\"])\n            if self.cmd_arguments:\n                self.args.update(self.cmd_arguments)\n            return None\n        except Exception as error:\n            raise ilund4uError(\"Unable to parse the specified config file. Please check your config file \"\n                               \"or provided name.\") from error\n</code></pre>"},{"location":"API/manager/#ilund4u.manager.Parameters.__init__","title":"<code>__init__()</code>","text":"<p>Parameters class constructor.</p> Source code in <code>ilund4u/manager.py</code> <pre><code>def __init__(self):\n    \"\"\"Parameters class constructor.\n\n    \"\"\"\n    self.args = dict(debug=True)\n    self.cmd_arguments = dict()\n</code></pre>"},{"location":"API/manager/#ilund4u.manager.Parameters.load_config","title":"<code>load_config(path='standard')</code>","text":"<p>Load configuration file.</p> <p>Arguments     path (str): path to a config file or name (only standard available at this moment).</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>def load_config(self, path: str = \"standard\") -&gt; None:\n    \"\"\"Load configuration file.\n\n    Arguments\n        path (str): path to a config file or name (only standard available at this moment).\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if path == \"standard\":\n            path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"standard.cfg\")\n        config = configs.load(path).get_config()\n        internal_dir = os.path.dirname(__file__)\n        for key in config[\"root\"].keys():\n            if type(config[\"root\"][key]) is str and \"{internal}\" in config[\"root\"][key]:\n                config[\"root\"][key] = config[\"root\"][key].replace(\"{internal}\",\n                                                                  os.path.join(internal_dir, \"ilund4u_data\"))\n        config[\"root\"][\"output_dir\"] = config[\"root\"][\"output_dir\"].replace(\"{current_date}\",\n                                                                            time.strftime(\"%Y_%m_%d-%H_%M\"))\n        keys_to_transform_to_list = []\n        for ktl in keys_to_transform_to_list:\n            if isinstance(config[\"root\"][ktl], str):\n                if config[\"root\"][ktl] != \"None\":\n                    config[\"root\"][ktl] = [config[\"root\"][ktl]]\n                else:\n                    config[\"root\"][ktl] = []\n        self.args.update(config[\"root\"])\n        if self.cmd_arguments:\n            self.args.update(self.cmd_arguments)\n        return None\n    except Exception as error:\n        raise ilund4uError(\"Unable to parse the specified config file. Please check your config file \"\n                           \"or provided name.\") from error\n</code></pre>"},{"location":"API/manager/#ilund4u.manager.Parameters.parse_cmd_arguments","title":"<code>parse_cmd_arguments()</code>","text":"<p>Parse command-line arguments.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>def parse_cmd_arguments(self) -&gt; None:\n    \"\"\"Parse command-line arguments.\n\n    Returns:\n        None\n\n    \"\"\"\n    parser = argparse.ArgumentParser(prog=\"ilund4u\", add_help=False)\n    parser.add_argument(\"-data\", \"--data\", dest=\"ilund4u_data\", action=\"store_true\")\n    parser.add_argument(\"-get-hmms\", \"--get-hmms\", dest=\"get_hmms\", action=\"store_true\")\n    parser.add_argument(\"-database\", \"--database\", dest=\"get_database\", default=None, type=str,\n                        choices=[\"phages\", \"plasmids\"])\n\n    parser.add_argument(\"-linux\", \"--linux\", dest=\"linux\", action=\"store_true\", default=None)\n    parser.add_argument(\"-mac\", \"--mac\", dest=\"mac\", action=\"store_true\", default=None)\n    parser.add_argument(\"-h\", \"--help\", dest=\"help\", action=\"store_true\")\n    parser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 0.0.8\")\n\n    subparsers = parser.add_subparsers(dest=\"mode\")\n\n    parser_hm = subparsers.add_parser(\"hotspots\")\n    parser_hm.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n    parser_hm.add_argument(\"-ufid\", \"--use-filename-as-id\", dest=\"use_filename_as_contig_id\", action=\"store_true\",\n                           default=None)\n    parser_hm.add_argument(\"-mps\", \"--min-proteome-size\", dest=\"min_proteome_size\", type=int, default=None)\n    parser_hm.add_argument(\"-gct\", \"--genome-circularity-table\", dest=\"genome_annotation\", type=str, default=None)\n    parser_hm.add_argument(\"-psc\", \"--proteome-sim-cutoff\", dest=\"proteome_similarity_cutoff\", type=float,\n                           default=None)\n    parser_hm.add_argument(\"-mpcs\", \"--min-proteome-community-size\", dest=\"min_proteome_community_size\", type=int,\n                           default=None)\n    parser_hm.add_argument(\"-vpc\", \"--variable-protein-cutoff\", dest=\"variable_protein_cluster_cutoff\", type=float,\n                           default=None)\n    parser_hm.add_argument(\"-cpc\", \"--conserved-protein-cutoff\", dest=\"conserved_protein_cluster_cutoff\",\n                           type=float, default=None)\n    parser_hm.add_argument(\"-cg\", \"--circular-genomes\", dest=\"circular_genomes\", default=None,\n                           action=\"store_true\")\n    parser_hm.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=None,\n                           action=\"store_false\")\n    parser_hm.add_argument(\"-hpc\", \"--hotspot-presence-cutoff\", dest=\"hotspot_presence_cutoff\",\n                           type=float, default=None)\n    parser_hm.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_hm.add_argument(\"-o-db\", \"--output-database\", dest=\"output_database\", type=str, default=None)\n    parser_hm.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_hm.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_hm.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_hm.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n    parser_hm.add_argument(\"--parsing-debug\", \"-parsing-debug\", dest=\"parsing_debug\", action=\"store_true\")\n\n    parser_ps = subparsers.add_parser(\"protein\")\n    parser_ps.add_argument(\"-fa\", \"--fa\", dest=\"fa\", type=str, default=None)\n    parser_ps.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n    parser_ps.add_argument(\"-ql\", \"--query-label\", dest=\"query_label\", type=str, default=None)\n    parser_ps.add_argument(\"-hsm\", \"--homology-search-mode\", dest=\"protein_search_target_mode\", type=str,\n                           choices=[\"group\", \"proteins\"], default=\"group\")\n    parser_ps.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n    parser_ps.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n    parser_ps.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n    parser_ps.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n    parser_ps.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                           default=None)\n    parser_ps.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_ps.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_ps.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_ps.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_ps.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n    parser_pa = subparsers.add_parser(\"proteome\")\n    parser_pa.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n    parser_pa.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n    parser_pa.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=True,\n                           action=\"store_false\")\n    parser_pa.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n    parser_pa.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n    parser_pa.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n    parser_pa.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n    parser_pa.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                           default=None)\n    parser_pa.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_pa.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_pa.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_pa.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_pa.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n    args = vars(parser.parse_args())\n    if len(sys.argv[1:]) == 0:\n        args[\"help\"] = True\n    if args[\"ilund4u_data\"]:\n        ilund4u.methods.copy_package_data()\n        sys.exit()\n    if args[\"linux\"]:\n        ilund4u.methods.adjust_paths(\"linux\")\n        sys.exit()\n    if args[\"mac\"]:\n        ilund4u.methods.adjust_paths(\"mac\")\n        sys.exit()\n    if args[\"get_hmms\"]:\n        self.load_config()\n        ilund4u.methods.get_HMM_models(self.args)\n        sys.exit()\n    if args[\"get_database\"]:\n        self.load_config()\n        ilund4u.methods.get_ilund4u_db(self.args, args[\"get_database\"])\n        sys.exit()\n    if args[\"help\"]:\n        if not args[\"mode\"]:\n            help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"help_main.txt\")\n        else:\n            help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", f\"help_{args['mode']}.txt\")\n        with open(help_message_path, \"r\") as help_message:\n            print(help_message.read(), file=sys.stdout)\n            sys.exit()\n    if args[\"mode\"] == \"hotspots\":\n        if not args[\"gff\"]:\n            raise ilund4uError(\"-gff argument is required for hotspots mode.\")\n    elif args[\"mode\"] == \"protein\":\n        if not args[\"fa\"] or not args[\"database\"]:\n            raise ilund4uError(\"Both -fa/--fa and -db/--database arguments are required for protein mode.\")\n    elif args[\"mode\"] == \"proteome\":\n        if not args[\"gff\"] or not args[\"database\"]:\n            raise ilund4uError(\"Both -gff/--gff and -db/--database arguments are required for protein mode.\")\n\n    args_to_keep = [\"gff\", \"output_database\", \"query_label\", \"genome_annotation\"]\n    filtered_args = {k: v for k, v in args.items() if v is not None or k in args_to_keep}\n    self.cmd_arguments = filtered_args\n    return None\n</code></pre>"},{"location":"API/manager/#ilund4u.manager.ilund4uError","title":"<code>ilund4uError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>A class for exceptions parsing inherited from the Exception class.</p> Source code in <code>ilund4u/manager.py</code> <pre><code>class ilund4uError(Exception):\n    \"\"\"A class for exceptions parsing inherited from the Exception class.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API/methods/","title":"Methods","text":"<p>This module provides some methods (e.g. colour transformation, data copying, etc.) used by the tool.</p>"},{"location":"API/methods/#ilund4u.methods.adjust_paths","title":"<code>adjust_paths(to)</code>","text":"<p>Change paths in the internal config files for linux or mac.</p> <p>Parameters:</p> <ul> <li> <code>to</code>             (<code>str</code>)         \u2013          <p>mac | linux</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def adjust_paths(to: str) -&gt; None:\n    \"\"\"Change paths in the internal config files for linux or mac.\n\n    Arguments:\n        to (str): mac | linux\n\n    Returns:\n        None\n\n    \"\"\"\n    internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n    config_files = [\"standard.cfg\"]\n    for config_file in config_files:\n        config_file_path = os.path.join(internal_dir, config_file)\n        with open(config_file_path, \"r+\") as config:\n            if to == \"linux\":\n                if not os.path.exists(os.path.join(internal_dir, \"bin/mmseqs_linux\")):\n                    os.system(f\"unzip -q -d {os.path.join(internal_dir, 'bin/')} \"\n                              f\"{os.path.join(internal_dir, 'bin/mmseqs_linux.zip')}\")\n                config_txt = re.sub(r\"mmseqs_mac/bin/mmseqs\", \"mmseqs_linux/bin/mmseqs\", config.read())\n                os.system(\"msa4u --linux &gt;&gt; /dev/null\")\n            else:\n                config_txt = re.sub(r\"mmseqs_linux/bin/mmseqs\", \"mmseqs_mac/bin/mmseqs\", config.read())\n            config.seek(0)\n            config.truncate()\n            config.write(config_txt)\n    print(f\"\u29bf mmseqs path was adjusted to {to}\", file=sys.stdout)\n    return None\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.copy_package_data","title":"<code>copy_package_data()</code>","text":"<p>Copy the ilund4u package data folder to your current dir.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def copy_package_data() -&gt; None:\n    \"\"\"Copy the ilund4u package data folder to your current dir.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        users_dir = os.path.join(os.getcwd(), \"ilund4u_data\")\n        internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n        if os.path.exists(users_dir):\n            print(\"Warning: ilund4u_data folder already exists. Remove it or change its name first before \"\n                  \"updating with default.\")\n            return None\n        shutil.copytree(internal_dir, users_dir, ignore=shutil.ignore_patterns(\"help*\", \".*\", \"HMMs*\", \"bin\"))\n        print(\"\u29bf ilund4u_data folder was copied to the current working directory\", file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to copy ilund4u folder in your working dir.\") from error\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.download_file_with_progress","title":"<code>download_file_with_progress(url, local_folder)</code>","text":"<p>Function for downloading a particular file from a web server.</p> <p>Parameters:</p> <ul> <li> <code>url</code>             (<code>str</code>)         \u2013          <p>Link to the file.</p> </li> <li> <code>local_folder</code>             (<code>str</code>)         \u2013          <p>Path to a folder where file will be saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def download_file_with_progress(url: str, local_folder: str) -&gt; None:\n    \"\"\"Function for downloading a particular file from a web server.\n\n    Arguments:\n        url (str): Link to the file.\n        local_folder (str): Path to a folder where file will be saved.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        response = requests.head(url)\n        file_size = int(response.headers.get('content-length', 0))\n        # Extract the original file name from the URL\n        file_name = os.path.basename(url)\n        local_path = os.path.join(local_folder, file_name)\n        # Stream the file download and show progress bar\n        with requests.get(url, stream=True) as r, open(local_path, 'wb') as f:\n            bar = progress.bar.FillingCirclesBar(\" \", max=file_size // 8192, suffix='%(percent)d%%')\n            downloaded_size = 0\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    downloaded_size += len(chunk)\n                    f.write(chunk)\n                    bar.next()\n            bar.finish()\n        # Verify that the file was fully downloaded\n        if downloaded_size != file_size:\n            raise ilund4u.manager.ilund4uError(f\"Downloaded file size ({downloaded_size} bytes) does not match \"\n                                               f\"expected size ({file_size} bytes).\")\n        print(f\"\u29bf File was saved to {local_path}\")\n        if file_name.endswith('.tar.gz'):\n            with tarfile.open(local_path, 'r:gz') as tar:\n                tar.extractall(path=local_folder)\n            print(f\"\u29bf Folder was successfully unarchived\")\n            os.remove(local_path)\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to get file from the {url}.\") from error\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.get_HMM_models","title":"<code>get_HMM_models(parameters)</code>","text":"<p>Download HMM models</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_HMM_models(parameters) -&gt; None:\n    \"\"\"Download HMM models\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        url =  parameters[\"hmm_models\"]\n        internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n        if os.path.exists(os.path.join(internal_dir, \"HMMs\")):\n            print(f\"\u25cb HMMs folder already exists and will be rewritten...\", file=sys.stdout)\n        # Add checking if it's already downloaded\n        print(f\"\u25cb Downloading HMM models...\\n\"\n              f\"  Source: {url}\", file=sys.stdout)\n        download_file_with_progress(url, internal_dir)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to download HMM models.\") from error\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.get_color","title":"<code>get_color(name, parameters)</code>","text":"<p>Get HEX color by its name</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>name of a color.</p> </li> <li> <code>parameters</code>             (<code>dict</code>)         \u2013          <p>Parameters' object dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>HEX color.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_color(name: str, parameters: dict) -&gt; str:\n    \"\"\"Get HEX color by its name\n\n    Arguments:\n        name (str): name of a color.\n        parameters (dict): Parameters' object dict.\n\n    Returns:\n        str: HEX color.\n\n    \"\"\"\n    hex_c = parameters.args[\"palette\"][parameters.args[name]]\n    return hex_c\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.get_colour_rgba","title":"<code>get_colour_rgba(name, parameters)</code>","text":"<p>Get rgba colour by its name</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>name of a colour.</p> </li> <li> <code>parameters</code>             (<code>dict</code>)         \u2013          <p>Parameters' object dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (            <code>tuple</code> )        \u2013          <p>RGBA colour</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_colour_rgba(name: str, parameters: dict) -&gt; tuple:\n    \"\"\"Get rgba colour by its name\n\n    Arguments:\n        name (str): name of a colour.\n        parameters (dict): Parameters' object dict.\n\n    Returns:\n        tuple: RGBA colour\n\n    \"\"\"\n    return *matplotlib.colors.hex2color(get_color(name, parameters)), parameters.args[f\"{name}_alpha\"]\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.get_ilund4u_db","title":"<code>get_ilund4u_db(parameters, db, path='./')</code>","text":"<p>Download ilund4u database</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_ilund4u_db(parameters, db, path = \"./\") -&gt; None:\n    \"\"\"Download ilund4u database\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        url = parameters[f\"{db}_db\"]\n        # Add checking if it's already downloaded\n        print(f\"\u25cb Downloading iLund4u {db} database...\\n\"\n              f\"  Source: {url}\", file=sys.stdout)\n        download_file_with_progress(url, path)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to download {db} database.\") from error\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.run_pyhmmer","title":"<code>run_pyhmmer(query_fasta, query_size, prms)</code>","text":"<p>Run pyhmmer hmmscan for a set of query proteins</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>Path to a query fasta file.</p> </li> <li> <code>query_size</code>             (<code>int</code>)         \u2013          <p>Number of query proteins.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: Table with hmmscan search results.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def run_pyhmmer(query_fasta: str, query_size: int, prms: ilund4u.manager.Parameters) -&gt; pd.DataFrame:\n    \"\"\"Run pyhmmer hmmscan for a set of query proteins\n\n    Arguments:\n        query_fasta (str): Path to a query fasta file.\n        query_size (int): Number of query proteins.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        pd.DataFrame: Table with hmmscan search results.\n\n    \"\"\"\n    with pyhmmer.easel.SequenceFile(query_fasta, digital=True) as seqs_file:\n        query_proteins = seqs_file.read_block()\n    num_of_query_proteins = query_size\n\n    hmmscan_output_folder = os.path.join(prms.args[\"output_dir\"], \"hmmscan\")\n    if os.path.exists(hmmscan_output_folder):\n        shutil.rmtree(hmmscan_output_folder)\n    os.mkdir(hmmscan_output_folder)\n\n    databases_cname = prms.args[\"hmm_config_names\"]\n    databases_short_names = prms.args[\"database_names\"]\n\n    databases_names = {\"hmm_defence_df\": \"DefenceFinder and CasFinder databases\",\n                       \"hmm_defence_padloc\": \"PADLOC database\", \"hmm_virulence\": \"virulence factor database (VFDB)\",\n                       \"hmm_anti_defence\": \"anti-prokaryotic immune systems database (dbAPIS)\",\n                       \"hmm_amr\": \"AMRFinderPlus Database\"}\n    databases_class = {\"hmm_defence_df\": \"defence\", \"hmm_defence_padloc\": \"defence\", \"hmm_virulence\": \"virulence\",\n                       \"hmm_anti_defence\": \"anti-defence\",\n                       \"hmm_amr\": \"AMR\"}\n\n    alignment_table_rows = []\n    for db_ind, db_name in enumerate(databases_cname):\n        if prms.args[\"defence_models\"] == \"DefenseFinder\" and db_name == \"hmm_defence_padloc\":\n            continue\n        if prms.args[\"defence_models\"] == \"PADLOC\" and db_name == \"hmm_defence_df\":\n            continue\n        db_alignment_table_rows = []\n        db_shortname = databases_short_names[db_ind]\n        db_path = prms.args[db_name]\n        db_full_name = databases_names[db_name]\n        db_class = databases_class[db_name]\n        if not os.path.exists(db_path):\n            print(f\"  \u29bf Database {db_full_name} was not found.\", file=sys.stdout)\n            continue\n        hmm_files = [fp for fp in os.listdir(db_path) if os.path.splitext(fp)[1].lower() == \".hmm\" and fp[0] != \".\"]\n        hmms = []\n        for hmm_file in hmm_files:\n            hmms.append(pyhmmer.plan7.HMMFile(os.path.join(db_path, hmm_file)).read())\n        if prms.args[\"verbose\"]:\n            print(f\"  \u29bf Running pyhmmer hmmscan versus {db_full_name}...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\"   \", max=num_of_query_proteins, suffix=\"%(index)d/%(max)d\")\n        for hits in pyhmmer.hmmscan(query_proteins, hmms, E=prms.args[\"hmmscan_evalue\"], cpus=0):\n            if prms.args[\"verbose\"]:\n                bar.next()\n            for hit in hits:\n                if hit.included:\n                    for domain in hit.domains.reported:\n                        if domain.i_evalue &lt; prms.args[\"hmmscan_evalue\"]:\n                            alignment = domain.alignment\n                            hit_name = hit.name.decode()\n                            hit_description = hit.description\n                            if hit.description:\n                                hit_description = hit_description.decode()\n                                if hit_description == \"NA\":\n                                    hit_description = \"\"\n                            else:\n                                hit_description = \"\"\n                            if hit_name != hit_description and hit_name not in hit_description and hit_description:\n                                hname = f\"{hit_name} {hit_description}\"\n                            elif hit_description:\n                                hname = hit_description\n                            else:\n                                hname = hit_name\n                            alignment_row = dict(query=alignment.target_name.decode(),  db_class = db_class,\n                                                 target_db=db_shortname, target=hname,t_name=hit_name,\n                                                 t_description=hit_description,\n                                                 hit_evalue=hit.evalue, di_evalue=domain.i_evalue,\n                                                 q_from=alignment.target_from, q_to=alignment.target_to,\n                                                 qlen=alignment.target_length, t_from=alignment.hmm_from,\n                                                 t_to=alignment.hmm_to, tlen=alignment.hmm_length)\n                            alignment_row[\"q_cov\"] = round((alignment_row[\"q_to\"] - alignment_row[\"q_from\"]) / \\\n                                                           alignment_row[\"qlen\"], 2)\n                            alignment_row[\"t_cov\"] = round((alignment_row[\"t_to\"] - alignment_row[\"t_from\"]) / \\\n                                                           alignment_row[\"tlen\"], 2)\n                            if alignment_row[\"q_cov\"] &gt;= prms.args[\"hmmscan_query_coverage_cutoff\"] and \\\n                                    alignment_row[\"t_cov\"] &gt;= prms.args[\"hmmscan_hmm_coverage_cutoff\"]:\n                                if hit.description:\n                                    alignment_row[\"t_description\"] = hit.description.decode()\n                                else:\n                                    alignment_row[\"t_description\"] = hit.name.decode()\n                                db_alignment_table_rows.append(alignment_row)\n        if prms.args[\"verbose\"]:\n            bar.finish()\n        alignment_table_rows += db_alignment_table_rows\n        db_alignment_table = pd.DataFrame(db_alignment_table_rows)\n        if not db_alignment_table.empty:\n            db_alignment_table = db_alignment_table.sort_values(by=\"hit_evalue\", ascending=True)\n            db_alignment_table = db_alignment_table.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n            db_alignment_table.to_csv(os.path.join(hmmscan_output_folder, f\"{db_shortname}.tsv\"), sep=\"\\t\",\n                                      index_label=\"query\")\n        n_hits = len(db_alignment_table.index)\n        if prms.args[\"verbose\"]:\n            print(f\"    Number of hits: {n_hits}\", file=sys.stdout)\n    alignment_table = pd.DataFrame(alignment_table_rows)\n    if not alignment_table.empty:\n        alignment_table = alignment_table.sort_values(by=\"hit_evalue\", ascending=True)\n        alignment_table = alignment_table.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n\n    return alignment_table\n</code></pre>"},{"location":"API/methods/#ilund4u.methods.update_path_extension","title":"<code>update_path_extension(path, new_extension)</code>","text":"<p>Get path basename and replace its extension</p> <p>Parameters:</p> <ul> <li> <code>path</code>             (<code>str</code>)         \u2013          <p>path to a file</p> </li> <li> <code>new_extension</code>             (<code>str</code>)         \u2013          <p>new extension</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>basename of a file with new extension.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def update_path_extension(path: str, new_extension: str) -&gt; str:\n    \"\"\"Get path basename and replace its extension\n\n    Arguments:\n        path (str): path to a file\n        new_extension (str): new extension\n\n    Returns:\n        str: basename of a file with new extension.\n\n    \"\"\"\n    updated_filename = f\"{os.path.splitext(os.path.basename(path))[0]}.{new_extension}\"\n    return updated_filename\n</code></pre>"},{"location":"API/package/","title":"Library","text":""},{"location":"API/package/#data-processing-module","title":"data processing module","text":"<p>This module contains all key data processing classes and methods of the tool.</p>"},{"location":"API/package/#ilund4u.data_processing.CDS","title":"<code>CDS</code>","text":"<p>CDS object represents an annotated protein.</p> <p>Attributes:</p> <ul> <li> <code>cds_id</code>             (<code>str</code>)         \u2013          <p>CDS identifier.</p> </li> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where CDS is encoded.</p> </li> <li> <code>start</code>             (<code>int</code>)         \u2013          <p>1-based start genomic coordinate.</p> </li> <li> <code>end</code>             (<code>int</code>)         \u2013          <p>1-based end genomic coordinates.</p> </li> <li> <code>length</code>             (<code>int</code>)         \u2013          <p>length of the CDS.</p> </li> <li> <code>strand</code>             (<code>int</code>)         \u2013          <p>Genomic strand (1: plus strand, -1: minus strand).</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the feature which will be used as a label.</p> </li> <li> <code>group</code>             (<code>str</code>)         \u2013          <p>CDS group that represents a set of homologous proteins.</p> </li> <li> <code>g_class</code>             (<code>str</code>)         \u2013          <p>Class of CDS group (variable, intermediate, conserved).</p> </li> <li> <code>hmmscan_results</code>             (<code>dict</code>)         \u2013          <p>Results of pyhmmer hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class CDS:\n    \"\"\"CDS object represents an annotated protein.\n\n    Attributes:\n        cds_id (str): CDS identifier.\n        proteome_id (str): Proteome identifier where CDS is encoded.\n        start (int): 1-based start genomic coordinate.\n        end (int): 1-based end genomic coordinates.\n        length (int): length of the CDS.\n        strand (int): Genomic strand (1: plus strand, -1: minus strand).\n        name (str): Name of the feature which will be used as a label.\n        group (str): CDS group that represents a set of homologous proteins.\n        g_class (str): Class of CDS group (variable, intermediate, conserved).\n        hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n    \"\"\"\n\n    def __init__(self, cds_id: str, proteome_id: str, start: int, end: int, strand: int, name: str,\n                 group: typing.Union[None, str] = None, g_class: typing.Union[None, str] = None,\n                 hmmscan_results: typing.Union[None, dict] = None):\n        \"\"\"CDS class constructor.\n\n        Arguments:\n            cds_id (str): CDS identifier.\n            proteome_id (str): Proteome identifier where CDS is encoded.\n            start (int): 1-based start genomic coordinate.\n            end (int): 1-based end genomic coordinates.\n            strand (int): Genomic strand (1: plus strand, -1: minus strand).\n            name (str): Name of the feature which will be used as a label.\n            group (str): CDS group that represents a set of homologous proteins.\n            g_class (str): Class of CDS group (variable, intermediate, conserved).\n            hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n        \"\"\"\n        self.cds_id = cds_id\n        self.proteome_id = proteome_id\n        self.start = start\n        self.end = end\n        self.length = int((end - start + 1) / 3)\n        self.strand = strand\n        self.name = name\n        self.group = group\n        self.g_class = g_class\n        self.hmmscan_results = hmmscan_results\n\n    def get_cds_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"length\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.CDS.__init__","title":"<code>__init__(cds_id, proteome_id, start, end, strand, name, group=None, g_class=None, hmmscan_results=None)</code>","text":"<p>CDS class constructor.</p> <p>Parameters:</p> <ul> <li> <code>cds_id</code>             (<code>str</code>)         \u2013          <p>CDS identifier.</p> </li> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where CDS is encoded.</p> </li> <li> <code>start</code>             (<code>int</code>)         \u2013          <p>1-based start genomic coordinate.</p> </li> <li> <code>end</code>             (<code>int</code>)         \u2013          <p>1-based end genomic coordinates.</p> </li> <li> <code>strand</code>             (<code>int</code>)         \u2013          <p>Genomic strand (1: plus strand, -1: minus strand).</p> </li> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the feature which will be used as a label.</p> </li> <li> <code>group</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>CDS group that represents a set of homologous proteins.</p> </li> <li> <code>g_class</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Class of CDS group (variable, intermediate, conserved).</p> </li> <li> <code>hmmscan_results</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Results of pyhmmer hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, cds_id: str, proteome_id: str, start: int, end: int, strand: int, name: str,\n             group: typing.Union[None, str] = None, g_class: typing.Union[None, str] = None,\n             hmmscan_results: typing.Union[None, dict] = None):\n    \"\"\"CDS class constructor.\n\n    Arguments:\n        cds_id (str): CDS identifier.\n        proteome_id (str): Proteome identifier where CDS is encoded.\n        start (int): 1-based start genomic coordinate.\n        end (int): 1-based end genomic coordinates.\n        strand (int): Genomic strand (1: plus strand, -1: minus strand).\n        name (str): Name of the feature which will be used as a label.\n        group (str): CDS group that represents a set of homologous proteins.\n        g_class (str): Class of CDS group (variable, intermediate, conserved).\n        hmmscan_results (dict): Results of pyhmmer hmmscan annotation.\n\n    \"\"\"\n    self.cds_id = cds_id\n    self.proteome_id = proteome_id\n    self.start = start\n    self.end = end\n    self.length = int((end - start + 1) / 3)\n    self.strand = strand\n    self.name = name\n    self.group = group\n    self.g_class = g_class\n    self.hmmscan_results = hmmscan_results\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.CDS.get_cds_db_row","title":"<code>get_cds_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_cds_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"length\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Database","title":"<code>Database</code>","text":"<p>Database object represents iLund4u database with proteomes and hotspots objects.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Database proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Database hotspots object.</p> </li> <li> <code>db_paths</code>             (<code>dict</code>)         \u2013          <p>Dictionary of database paths.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Database:\n    \"\"\"Database object represents iLund4u database with proteomes and hotspots objects.\n\n    Attributes:\n        proteomes (Proteomes): Database proteomes object.\n        hotspots (Hotspots): Database hotspots object.\n        db_paths (dict): Dictionary of database paths.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, proteomes: Proteomes, hotspots: Hotspots, db_paths: dict,\n                 parameters: ilund4u.manager.Parameters):\n        \"\"\"Database class constructor.\n\n        Args:\n            proteomes (Proteomes): Database proteomes object.\n            hotspots (Hotspots): Database hotspots object.\n            db_paths (dict): Dictionary of database paths.\n            prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = proteomes\n        self.hotspots = hotspots\n        self.db_paths = db_paths\n        self.prms = parameters\n\n    def mmseqs_search_versus_protein_database(self, query_fasta: str, fast=False) -&gt; pd.DataFrame:\n        \"\"\"Run mmseqs search versus protein database.\n\n        Arguments:\n            query_fasta (str): path to a query fasta file with protein sequence(s).\n            fast (bool): if true, then search will be performed only against representative sequences.\n\n        Returns:\n            pd.DataFrame: mmseqs search results table.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Running mmseqs for protein search versus the {'representative' if fast else 'full'}\"\n                      f\" database of proteins...\",\n                      file=sys.stdout)\n            if not os.path.exists(self.prms.args[\"output_dir\"]):\n                os.mkdir(self.prms.args[\"output_dir\"])\n            mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n            if os.path.exists(mmseqs_output_folder):\n                shutil.rmtree(mmseqs_output_folder)\n            os.mkdir(mmseqs_output_folder)\n            mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DBs\")\n            os.mkdir(mmseqs_output_folder_db)\n            mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n            mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n            query_length = len(list(Bio.SeqIO.parse(query_fasta, \"fasta\")))\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", query_fasta,\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\")], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)\n            target_db = self.db_paths[\"proteins_db\"]\n            if fast:\n                if not os.path.exists(os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")):\n                    subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", self.db_paths[\"rep_fasta\"],\n                                    os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")],\n                                   stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n                target_db = os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"search\",\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\"), target_db,\n                            os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                            os.path.join(mmseqs_output_folder, \"tmp\"), \"-e\",\n                            str(self.prms.args[\"mmseqs_search_evalue\"]),\n                            \"-s\", str(self.prms.args[\"mmseqs_search_s\"])], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"convertalis\",\n                            os.path.join(mmseqs_output_folder_db, \"query_seq_db\"),\n                            self.db_paths[\"proteins_db\"],\n                            os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                            os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"), \"--format-output\",\n                            \"query,target,qlen,tlen,alnlen,fident,qstart,qend,tstart,tend,evalue\",\n                            \"--format-mode\", \"4\"], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            mmseqs_search_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"),\n                                                  sep=\"\\t\")\n            mmseqs_search_results[\"qcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"qlen\"], axis=1)\n            mmseqs_search_results[\"tcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"tlen\"], axis=1)\n            mmseqs_search_results = mmseqs_search_results[\n                (mmseqs_search_results[\"qcov\"] &gt;= self.prms.args[\"mmseqs_search_qcov\"]) &amp;\n                (mmseqs_search_results[\"tcov\"] &gt;= self.prms.args[\"mmseqs_search_tcov\"]) &amp;\n                (mmseqs_search_results[\"fident\"] &gt;= self.prms.args[\"mmseqs_search_fident\"])]\n            queries_with_res = len(set(mmseqs_search_results[\"query\"].to_list()))\n            target_to_group = dict()\n            for proteome in self.proteomes.proteomes.to_list():\n                for cds in proteome.cdss.to_list():\n                    target_to_group[cds.cds_id] = cds.group\n            mmseqs_search_results[\"group\"] = mmseqs_search_results[\"target\"].apply(lambda t: target_to_group[t])\n            mmseqs_search_results.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"mmseqs_homology_search_full.tsv\"),\n                                         sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                if queries_with_res &gt; 0:\n                    print(f\"  \u29bf A homologous group was found for {queries_with_res}/{query_length} query protein\"\n                          f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n                else:\n                    print(f\"  \u29bf No homologous group was found for {query_length} query protein\"\n                          f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n            return mmseqs_search_results\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs search versus protein database.\") from error\n\n    def protein_search_mode(self, query_fasta: str, query_label: typing.Union[None, str] = None,\n                            predefined_protein_group: typing.Union[None, str] = None) -&gt; None:\n        \"\"\"Run protein search mode which finds homologues of your query proteins in the database and returns\n            comprehensive output including visualisation and hotspot annotation.\n\n        Arguments:\n            query_fasta (str): Fasta with query protein sequence.\n            query_label (str): Label to be shown on lovis4u visualisation.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Load fasta\n            if not predefined_protein_group:\n                query_records = list(Bio.SeqIO.parse(query_fasta, \"fasta\"))\n                if len(query_records) &gt; 1:\n                    raise ilund4u.manager.ilund4uError(\"Only single query protein is allowed for protein mode\")\n                query_record = query_records[0]\n                # Run mmseqs for homology search\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\" and self.prms.args[\n                    \"fast_mmseqs_search_mode\"]:\n                    print(\"\u25cb Fast mode is not available with 'proteins' search modea and was deactivated.\",\n                          file=sys.stdout)\n                    self.prms.args[\"fast_mmseqs_search_mode\"] = False\n                mmseqs_results = self.mmseqs_search_versus_protein_database(query_fasta,\n                                                                            self.prms.args[\"fast_mmseqs_search_mode\"])\n                if len(mmseqs_results.index) == 0:\n                    print(\"\u25cb Termination since no homology to hotspot db proteins was found\", file=sys.stdout)\n                    return None\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                    homologous_protein_ids = mmseqs_results[\"target\"].to_list()\n                    homologous_groups = mmseqs_results[\"group\"].to_list()\n                elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                    mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"],\n                                               ascending=[True, False, False, False], inplace=True)\n                    mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n                    homologous_protein_ids = []\n                    homologous_group = mmseqs_results.at[query_record.id, \"group\"]\n                    homologous_groups = [homologous_group]\n            else:\n                homologous_group = predefined_protein_group\n                homologous_groups = [homologous_group]\n                homologous_protein_ids = []\n                self.prms.args[\"protein_search_target_mode\"] = \"group\"\n            if \"protein_group_stat\" in self.db_paths.keys():\n                protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                    \"representative_protein\")\n                groups_to_select = list(protein_group_stat_table.index.intersection(homologous_groups))\n                if groups_to_select:\n                    protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                    protein_group_stat_table.to_csv(\n                        os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                        sep=\"\\t\", index=True, index_label=\"representative_protein\")\n\n            # Searching for hotspots\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Searching for hotspots with your query protein homologues...\", file=sys.stdout)\n            found_hotspots = collections.defaultdict(list)\n            island_annotations = []\n            location_stat = dict(flanking=0, cargo=0)\n            n_flanked = 0\n            for hotspot in self.hotspots.hotspots.to_list():\n                if not self.prms.args[\"report_not_flanked\"] and not hotspot.flanked:\n                    continue\n                for island in hotspot.islands:\n                    proteome = self.proteomes.proteomes.at[island.proteome]\n                    if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                        locus_proteins = island.get_locus_proteins(proteome.cdss)\n                        overlapping = list(set(homologous_protein_ids) &amp; set(locus_proteins))\n                    elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                        locus_groups = island.get_locus_groups(proteome.cdss)\n                        overlapping = homologous_group in locus_groups\n                    if overlapping:\n                        if self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                            homologous_protein_ids_island = []\n                            locus_proteins = island.get_locus_proteins(proteome.cdss)\n                            for lp, lpg in zip(locus_proteins, locus_groups):\n                                if lpg == homologous_group:\n                                    homologous_protein_ids_island.append(lp)\n                                    homologous_protein_ids.append(lp)\n                            overlapping = homologous_protein_ids_island\n                        isl_groups = island.get_island_groups(proteome.cdss)\n                        isl_proteins = island.get_island_proteins(proteome.cdss)\n                        for op in overlapping:\n                            if op in isl_proteins:\n                                location_stat[\"cargo\"] += 1\n                                n_flanked += 1\n                            else:\n                                location_stat[\"flanking\"] += 1\n                        island_annotation = hotspot.island_annotation.loc[island.island_id].copy()\n                        island_annotation.drop(labels=[\"island_index\", \"strength\", \"degree\"], inplace=True)\n                        island_annotation.at[\"indexes\"] = \",\".join(map(str, island.indexes))\n                        island_annotation.at[\"size\"] = island.size\n                        island_annotation.at[\"island_proteins\"] = \",\".join(island.get_island_proteins(proteome.cdss))\n                        island_annotation.at[\"island_protein_groups\"] = \",\".join(isl_groups)\n                        island_annotation.at[\"query_homologues\"] = \",\".join(overlapping)\n                        island_annotations.append(island_annotation)\n                        found_hotspots[hotspot.hotspot_id].append(island)\n            if sum(location_stat.values()) == 0:\n                print(\"\u25cb Termination since no homologous protein was found in hotspots (neither as flanking or cargo)\",\n                      file=sys.stdout)\n                return None\n            found_islands = [island.island_id for islands in found_hotspots.values() for island in islands]\n            island_annotations = pd.DataFrame(island_annotations)\n            island_annotations.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                                \"found_island_annotation.tsv\")),\n                                      sep=\"\\t\", index_label=\"island_id\")\n            found_hotspots_annotation = self.hotspots.annotation.loc[found_hotspots.keys()]\n            found_hotspots_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"found_hotspot_annotation.tsv\"),\n                                             sep=\"\\t\", index_label=\"hotspot_id\")\n            found_hotspot_communities = list(set(found_hotspots_annotation[\"hotspot_community\"].to_list()))\n            # Get hotspot community stat\n            hotspot_community_annot_rows = []\n            r_types = [\"cargo\", \"flanking\"]\n            for h_com, hotspot_ids in self.hotspots.communities.items():\n                if h_com not in found_hotspot_communities:\n                    continue\n                h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n                hotspot_com_groups = dict(cargo=set(), flanking=set())\n                hotspots = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n                n_islands, n_flanked = 0, 0\n                for hotspot in hotspots:\n                    n_islands += hotspot.size\n                    n_flanked += hotspot.flanked\n                    hotspot_groups = hotspot.get_hotspot_groups(self.proteomes)\n                    db_stat = hotspot.calculate_database_hits_stats(self.proteomes, self.prms, protein_mode=True)\n                    for r_type in r_types:\n                        hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        for r_type in r_types:\n                            h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                    N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                    pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n                hotspot_community_annot_rows.append(hc_annot_row)\n            hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n            for db_name in self.prms.args[\"databases_classes\"]:\n                hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                    hotspot_community_annot.apply(\n                        lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n            hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                        \"found_hotspot_community_annotation.tsv\"),\n                                           sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Query protein homologues were found in {len(found_hotspot_communities)} hotspot \"\n                      f\"communit{'y' if len(found_hotspot_communities) == 1 else 'ies'} \"\n                      f\"({len(found_hotspots.keys())} hotspot{'s' if len(found_hotspots.keys()) &gt; 1 else ''}) on \"\n                      f\"{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''}\\n\"\n                      f\"    Found as cargo: {location_stat['cargo']}, as flanking gene: {location_stat['flanking']}\"\n                      f\"\\n    {n_flanked}/{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''} where found\"\n                      f\" as cargo are both side flanked (have conserved genes on both sides)\",\n                      file=sys.stdout)\n\n            homologous_protein_fasta = os.path.join(self.prms.args[\"output_dir\"], \"homologous_proteins.fa\")\n            full_fasta_file = Bio.SeqIO.index(self.proteomes.proteins_fasta_file, \"fasta\")\n            with open(homologous_protein_fasta, \"w\") as out_handle:\n                if not predefined_protein_group:\n                    Bio.SeqIO.write(query_record, out_handle, \"fasta\")\n                for acc in homologous_protein_ids:\n                    out_handle.write(full_fasta_file.get_raw(acc).decode())\n            # MSA visualisation\n            if len(homologous_protein_ids) &gt; 1 or not predefined_protein_group:\n                msa4u_p = msa4u.manager.Parameters()\n                msa4u_p.arguments[\"label\"] = \"id\"\n                msa4u_p.arguments[\"verbose\"] = False\n                msa4u_p.arguments[\"output_filename\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                    \"msa4u_homologous_proteines.pdf\")\n                msa4u_p.arguments[\"output_filename_aln\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                        \"homologous_proteins_aln.fa\")\n                fasta = msa4u.manager.Fasta(fasta=homologous_protein_fasta, parameters=msa4u_p)\n                mafft_output = fasta.run_mafft()\n                msa = msa4u.manager.MSA(mafft_output, msa4u_p)\n                msa.plot()\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf Homologous proteins were saved to {homologous_protein_fasta} and the MSA was \"\n                          f\"visualised with MSA4u\")\n            print(f\"\u25cb Visualisation of the hotspot(s) with your query protein homologues using lovis4u...\",\n                  file=sys.stdout)\n            # lovis4u visualisation\n            vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                  os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_with_query\")]\n            for vis_output_folder in vis_output_folders:\n                if os.path.exists(vis_output_folder):\n                    shutil.rmtree(vis_output_folder)\n                os.mkdir(vis_output_folder)\n            additional_annotation = dict()\n            for hpid in homologous_protein_ids:\n                additional_annotation[hpid] = dict(stroke_colour=\"#000000\", fill_colour=\"#000000\")\n                if query_label:\n                    additional_annotation[hpid][\"name\"] = query_label\n            drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n            for community in found_hotspot_communities:\n                drawing_manager.plot_hotspots(self.hotspots.communities[community],\n                                              output_folder=os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                              additional_annotation=additional_annotation)\n            drawing_manager.plot_hotspots(list(found_hotspots.keys()),\n                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                     \"lovis4u_with_query\"),\n                                          island_ids=found_islands,\n                                          additional_annotation=additional_annotation)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Done!\")\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to perform protein search versus the database.\") from error\n\n    def proteome_annotation_mode(self, query_gff: str) -&gt; None:\n        \"\"\"Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and\n            variable proteins in the query proteome in case a community with similar proteomes was found in the database.\n\n        Arguments:f\n            query_gff (str): GFF with query proteome.\n\n        Returns:\n            None\n        \"\"\"\n        try:\n            # Load query gff\n            proteomes_helper_obj = ilund4u.data_processing.Proteomes(parameters=self.prms)\n            proteomes_helper_obj.load_sequences_from_extended_gff(input_f=[query_gff])\n            query_proteome = proteomes_helper_obj.proteomes.iat[0]\n            # Get and parse mmseqs search results\n            mmseqs_results = self.mmseqs_search_versus_protein_database(proteomes_helper_obj.proteins_fasta_file,\n                                                                        self.prms.args[\"fast_mmseqs_search_mode\"])\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Searching for similar proteomes in the database network\", file=sys.stdout)\n            mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"], ascending=[True, False, False, False],\n                                       inplace=True)\n            mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n            proteins_wo_hits = []\n            for cds in query_proteome.cdss.to_list():\n                if cds.cds_id in mmseqs_results.index:\n                    cds.group = mmseqs_results.at[cds.cds_id, \"group\"]\n                else:\n                    cds.group = f\"{cds.cds_id}\"\n                    proteins_wo_hits.append(cds.group)\n            if \"protein_group_stat\" in self.db_paths.keys():\n                protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                    \"representative_protein\")\n                groups_to_select = list(protein_group_stat_table.index.intersection(mmseqs_results[\"group\"].tolist()))\n                if groups_to_select:\n                    protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                    protein_group_stat_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                                                    sep=\"\\t\", index=True, index_label=\"representative_protein\")\n            # Running pyhmmer annotation\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Preparing data for protein annotation with pyhmmer hmmscan...\", file=sys.stdout)\n            alignment_table = ilund4u.methods.run_pyhmmer(proteomes_helper_obj.proteins_fasta_file,\n                                                          len(query_proteome.cdss.index), self.prms)\n            if not alignment_table.empty:\n                found_hits_for = alignment_table.index.to_list()\n                proteome_cdss = query_proteome.cdss.to_list()\n                proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.cds_id in found_hits_for]\n                if proteome_cdss_with_hits:\n                    cdss_with_hits = query_proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                    for cds in cdss_with_hits:\n                        alignment_table_row = alignment_table.loc[cds.cds_id]\n                        cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                   db_name=alignment_table_row[\"target_db\"],\n                                                   target=alignment_table_row[\"target\"],\n                                                   evalue=alignment_table_row[\"hit_evalue\"])\n            # Connect to the database proteome network\n            proteome_names = pd.Series({idx: sid for idx, sid in enumerate(self.proteomes.annotation.index)})\n            proteome_sizes = self.proteomes.annotation[[\"proteome_size_unique\", \"index\"]]\n            proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n            cluster_to_sequences = collections.defaultdict(list)\n            for p_index, proteome in enumerate(self.proteomes.proteomes.to_list()):\n                cds_groups = set(proteome.cdss.apply(lambda cds: cds.group).to_list())\n                for cds_g in cds_groups:\n                    cluster_to_sequences[cds_g].append(proteome.proteome_id)\n            cluster_to_proteome_index = dict()\n            for cluster, sequences in cluster_to_sequences.items():\n                indexes = sorted([self.proteomes.seq_to_ind[seq_id] for seq_id in sequences])\n                cluster_to_proteome_index[cluster] = indexes\n            query_clusters = set(query_proteome.cdss.apply(lambda cds: cds.group).to_list())\n            query_size = len(query_clusters)\n            counts = collections.defaultdict(int)\n            for cl in query_clusters:\n                if cl not in proteins_wo_hits:\n                    js = cluster_to_proteome_index[cl]\n                    for j in js:\n                        counts[j] += 1\n            weights = pd.Series(counts)\n            proteome_sizes_connected = proteome_sizes.iloc[weights.index]\n            norm_factor = pd.Series(\n                0.5 * (query_size + proteome_sizes_connected) / (query_size * proteome_sizes_connected), \\\n                index=weights.index)\n            weights = weights.mul(norm_factor)\n            weights = weights[weights &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n            query_network_df = pd.DataFrame(dict(weight=weights, seq_id=proteome_names.iloc[weights.index]))\n            query_network_df.sort_values(by=\"weight\", inplace=True, ascending=False)\n            query_network_df.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_proteome_network.tsv\"),\n                                    sep=\"\\t\", index_label=\"t_index\")\n            query_network_df = query_network_df.set_index(\"seq_id\")\n            max_weight = round(query_network_df[\"weight\"].max(), 2)\n            if len(weights.index) == 0:\n                print(\"\u25cb Termination since no similar proteome was found in the database\", file=sys.stdout)\n                sys.exit()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(weights.index)} similar proteomes were found in the database network with \"\n                      f\"max proteome similarity = {max_weight}\", file=sys.stdout)\n            similar_proteoms_ids = query_network_df.index.to_list()\n            # Assign the closest community\n            similar_communities_rows = list()\n            for pcom_id, pcom_pr_ids in self.proteomes.communities.items():\n                com_size = len(pcom_pr_ids)\n                overlapping = list(set(pcom_pr_ids) &amp; set(similar_proteoms_ids))\n                if overlapping:\n                    weights_subset = query_network_df.loc[overlapping][\"weight\"].to_list()\n                    similar_communities_rows.append(dict(com_id=pcom_id, com_size=com_size,\n                                                         connection_fr=len(overlapping) / com_size,\n                                                         avg_weight=np.mean(weights_subset)))\n            similar_communities = pd.DataFrame(similar_communities_rows)\n            similar_communities.sort_values(by=[\"avg_weight\", \"connection_fr\", \"com_size\"], inplace=True,\n                                            ascending=[False, False, False])\n            similar_communities.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"similar_proteome_communities.tsv\"),\n                                       sep=\"\\t\", index=False)\n            selected_community_dict = similar_communities.iloc[0].to_dict()\n            selected_community_dict[\"com_id\"] = int(selected_community_dict[\"com_id\"])\n            query_proteome_community = selected_community_dict[\"com_id\"]\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf The query proteome was assigned to a community (id: {int(selected_community_dict['com_id'])})\"\n                      f\" with connection to {round(selected_community_dict['connection_fr'], 2)} of its members with \"\n                      f\"avg weight = {round(selected_community_dict['avg_weight'], 2)}\", file=sys.stdout)\n            # Define cds classes\n            com_protein_classes = dict()\n            for com_proteome in self.proteomes.communities[selected_community_dict[\"com_id\"]]:\n                for cds in self.proteomes.proteomes.at[com_proteome].cdss:\n                    com_protein_classes[cds.group] = cds.g_class\n            defined_classes_rows = []\n            for cds in query_proteome.cdss:\n                if cds.group in com_protein_classes.keys():\n                    cds.g_class = com_protein_classes[cds.group]\n                else:\n                    cds.g_class = \"variable\"\n                defined_classes_rows.append(dict(cds_id=cds.cds_id, cds_class=cds.g_class))\n            defined_classes = pd.DataFrame(defined_classes_rows)\n            defined_classes.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_protein_clusters.tsv\"),\n                                   sep=\"\\t\", index=False)\n            defined_classes_c = collections.Counter(defined_classes[\"cds_class\"].to_list())\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Protein class distribution in query proteome: \"\n                      f\"{', '.join(f'{v} {k}' for k, v in defined_classes_c.items())}\"\n                      , file=sys.stdout)\n            # Annotate variable islands\n            query_proteome.annotate_variable_islands(self.prms)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(query_proteome.islands.index)} variable island\"\n                      f\"{'s were' if len(query_proteome.islands.index) &gt; 1 else ' was'} annotated in the \"\n                      f\"query proteome\", file=sys.stdout)\n            # Connect query proteome islands to the island network\n            community_hotspots = [h for h in self.hotspots.hotspots.to_list() if\n                                  h.proteome_community == query_proteome_community]\n            if not self.prms.args[\"report_not_flanked\"]:\n                community_hotspots_flanked = [h for h in community_hotspots if h.flanked == 1]\n                community_hotspots = community_hotspots_flanked\n            if community_hotspots:\n                hotspots_islands = [island for hotspot in community_hotspots for island in hotspot.islands]\n                island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(hotspots_islands)}\n                com_island_n_sizes = pd.Series()\n                com_neighbours = pd.Series()\n                cluster_to_island = collections.defaultdict(list)\n                for island in hotspots_islands:\n                    island_proteome = self.proteomes.proteomes.at[island.proteome]\n                    island_id = island.island_id\n                    island_index = island_id_to_index[island_id]\n                    conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(island_proteome.cdss))\n                    com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                    com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                    for cing in conserved_island_neighbours_groups:\n                        cluster_to_island[cing].append(island_index)\n                hotspot_hits_statistic_rows = []\n                for q_island in query_proteome.islands.to_list():\n                    q_isl_neighbours = list(set(q_island.get_cons_neighbours_groups(query_proteome.cdss)))\n                    q_isl_size = len(q_isl_neighbours)\n                    q_isl_counts = collections.defaultdict(int)\n                    for qin in q_isl_neighbours:\n                        js = cluster_to_island[qin]\n                        for j in js:\n                            q_isl_counts[j] += 1\n                    q_isl_weights = pd.Series(q_isl_counts)\n                    q_isl_connected_n_sizes = com_island_n_sizes.iloc[q_isl_weights.index]\n                    q_isl_norm_factors = pd.Series(\n                        0.5 * (q_isl_size + q_isl_connected_n_sizes) / (q_isl_size * q_isl_connected_n_sizes),\n                        index=q_isl_weights.index)\n                    q_isl_weights = q_isl_weights.mul(q_isl_norm_factors)\n                    q_isl_weights = q_isl_weights[\n                        q_isl_weights &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                    q_isl_similar_islands = q_isl_weights.index.to_list()\n                    similar_hotspots_rows = list()\n                    for c_hotspot in community_hotspots:\n                        hotspot_islands = c_hotspot.islands\n                        hotspot_islsnds_idx = [island_id_to_index[isl.island_id] for isl in hotspot_islands]\n                        overlapping = list(set(q_isl_similar_islands) &amp; set(hotspot_islsnds_idx))\n                        if overlapping:\n                            weights_subset = q_isl_weights.loc[overlapping].to_list()\n                            connection_fr = len(overlapping) / c_hotspot.size\n                            if connection_fr &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]:\n                                similar_hotspots_rows.append(dict(hotspot_id=c_hotspot.hotspot_id,\n                                                                  hotspot_size=c_hotspot.size,\n                                                                  connection_fr=connection_fr,\n                                                                  avg_weight=np.mean(weights_subset)))\n                    if similar_hotspots_rows:\n                        similar_hotspots_rows = pd.DataFrame(similar_hotspots_rows)\n                        similar_hotspots_rows.sort_values(by=[\"avg_weight\", \"connection_fr\", \"hotspot_size\"],\n                                                          inplace=True,\n                                                          ascending=[False, False, False])\n                        best_hit_for_q_isl = similar_hotspots_rows.iloc[0].to_dict()\n                        hotspot_hits_statistic_rows.append(dict(query_island_id=q_island.island_id,\n                                                                query_island_proteins=\",\".join(\n                                                                    q_island.get_island_proteins(query_proteome.cdss)),\n                                                                closest_hotspot=best_hit_for_q_isl[\"hotspot_id\"],\n                                                                closest_hotspot_size=best_hit_for_q_isl[\"hotspot_size\"],\n                                                                closest_hotspot_avg_weight=best_hit_for_q_isl[\n                                                                    \"avg_weight\"],\n                                                                closest_hotspot_connection_fr=best_hit_for_q_isl[\n                                                                    \"connection_fr\"]))\n                hotspot_hits_statistic = pd.DataFrame(hotspot_hits_statistic_rows)\n                hotspot_hits_statistic.sort_values(by=[\"closest_hotspot_avg_weight\", \"closest_hotspot_connection_fr\"],\n                                                   inplace=True, ascending=[False, False])\n                hotspot_hits_statistic.to_csv(\n                    os.path.join(self.prms.args[\"output_dir\"], \"island_to_hotspot_mapping.tsv\"),\n                    sep=\"\\t\", index=False)\n                hotspot_hits_statistic = hotspot_hits_statistic.drop_duplicates(subset=\"closest_hotspot\", keep=\"first\")\n                hotspot_hits_statistic.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"island_to_hotspot_mapping_deduplicated.tsv\"),\n                                              sep=\"\\t\", index=False)\n                subset_hotspot_annotation = self.hotspots.annotation.loc[hotspot_hits_statistic[\"closest_hotspot\"]]\n                subset_hotspot_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                              \"annotation_of_mapped_hotspots.tsv\"), sep=\"\\t\",\n                                                 index=False)\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf {len(hotspot_hits_statistic.index)} of annotated variable island\"\n                          f\"{'s were' if len(hotspot_hits_statistic.index) &gt; 1 else ' was'} mapped to \"\n                          f\"the database hotspots\", file=sys.stdout)\n            else:\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u29bf The proteome community does not have any annotated hotspots\", file=sys.stdout)\n            # Update proteome and hotspots objects\n            self.proteomes.proteomes.at[query_proteome.proteome_id] = query_proteome\n            self.proteomes.annotation.loc[query_proteome.proteome_id] = proteomes_helper_obj.annotation.loc[\n                query_proteome.proteome_id]\n            self.proteomes.communities[query_proteome_community].append(query_proteome.proteome_id)\n            if community_hotspots:\n                for index, row in hotspot_hits_statistic.iterrows():\n                    hotspot_id = row[\"closest_hotspot\"]\n                    query_island_id = row[\"query_island_id\"]\n                    query_island = query_proteome.islands.at[query_island_id]\n                    closest_hotspot = self.hotspots.hotspots.at[hotspot_id]\n                    closest_hotspot.islands.append(query_island)\n            # Visualisation\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Lovis4u visualisation of communities and proteomes...\", file=sys.stdout)\n            drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"hotspot\",\n                                                    filename=\"lovis4u_proteome_community_hotspots.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"regular\",\n                                                    filename=\"lovis4u_proteome_community.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"regular\",\n                                                    proteome_ids=[query_proteome.proteome_id],\n                                                    filename=\"lovis4u_query_proteome_variable.pdf\")\n            drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                    output_folder=self.prms.args[\"output_dir\"],\n                                                    mode=\"hotspot\",\n                                                    proteome_ids=[query_proteome.proteome_id],\n                                                    filename=\"lovis4u_query_proteome_hotspot.pdf\")\n            if community_hotspots:\n                if self.prms.args[\"verbose\"]:\n                    print(f\"\u25cb Lovis4u visualisation found hotspots..\", file=sys.stdout)\n                if len(hotspot_hits_statistic.index) &gt; 0:\n                    vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_full\"),\n                                          os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_with_query\")]\n                    for vis_output_folder in vis_output_folders:\n                        if os.path.exists(vis_output_folder):\n                            shutil.rmtree(vis_output_folder)\n                        os.mkdir(vis_output_folder)\n                for index, row in hotspot_hits_statistic.iterrows():\n                    hotspot_id = row[\"closest_hotspot\"]\n                    query_island = row[\"query_island_id\"]\n                    for hc, c_hotspots in self.hotspots.communities.items():\n                        if hotspot_id in c_hotspots:\n                            drawing_manager.plot_hotspots([hotspot_id],\n                                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                     \"lovis4u_hotspot_with_query\"),\n                                                          island_ids=[query_island])\n                            drawing_manager.plot_hotspots(c_hotspots,\n                                                          output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                     \"lovis4u_hotspot_full\"),\n                                                          keep_while_deduplication=[query_island])\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Done!\")\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run proteome annotation mode versus the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Database.__init__","title":"<code>__init__(proteomes, hotspots, db_paths, parameters)</code>","text":"<p>Database class constructor.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Database proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Database hotspots object.</p> </li> <li> <code>db_paths</code>             (<code>dict</code>)         \u2013          <p>Dictionary of database paths.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, proteomes: Proteomes, hotspots: Hotspots, db_paths: dict,\n             parameters: ilund4u.manager.Parameters):\n    \"\"\"Database class constructor.\n\n    Args:\n        proteomes (Proteomes): Database proteomes object.\n        hotspots (Hotspots): Database hotspots object.\n        db_paths (dict): Dictionary of database paths.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = proteomes\n    self.hotspots = hotspots\n    self.db_paths = db_paths\n    self.prms = parameters\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Database.mmseqs_search_versus_protein_database","title":"<code>mmseqs_search_versus_protein_database(query_fasta, fast=False)</code>","text":"<p>Run mmseqs search versus protein database.</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>path to a query fasta file with protein sequence(s).</p> </li> <li> <code>fast</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if true, then search will be performed only against representative sequences.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: mmseqs search results table.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def mmseqs_search_versus_protein_database(self, query_fasta: str, fast=False) -&gt; pd.DataFrame:\n    \"\"\"Run mmseqs search versus protein database.\n\n    Arguments:\n        query_fasta (str): path to a query fasta file with protein sequence(s).\n        fast (bool): if true, then search will be performed only against representative sequences.\n\n    Returns:\n        pd.DataFrame: mmseqs search results table.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Running mmseqs for protein search versus the {'representative' if fast else 'full'}\"\n                  f\" database of proteins...\",\n                  file=sys.stdout)\n        if not os.path.exists(self.prms.args[\"output_dir\"]):\n            os.mkdir(self.prms.args[\"output_dir\"])\n        mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n        if os.path.exists(mmseqs_output_folder):\n            shutil.rmtree(mmseqs_output_folder)\n        os.mkdir(mmseqs_output_folder)\n        mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DBs\")\n        os.mkdir(mmseqs_output_folder_db)\n        mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n        mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n        query_length = len(list(Bio.SeqIO.parse(query_fasta, \"fasta\")))\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", query_fasta,\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\")], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)\n        target_db = self.db_paths[\"proteins_db\"]\n        if fast:\n            if not os.path.exists(os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")):\n                subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", self.db_paths[\"rep_fasta\"],\n                                os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")],\n                               stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            target_db = os.path.join(self.db_paths[\"db_path\"], \"mmseqs_db\", \"rep_proteins\")\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"search\",\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\"), target_db,\n                        os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                        os.path.join(mmseqs_output_folder, \"tmp\"), \"-e\",\n                        str(self.prms.args[\"mmseqs_search_evalue\"]),\n                        \"-s\", str(self.prms.args[\"mmseqs_search_s\"])], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"convertalis\",\n                        os.path.join(mmseqs_output_folder_db, \"query_seq_db\"),\n                        self.db_paths[\"proteins_db\"],\n                        os.path.join(mmseqs_output_folder_db, \"search_res_db\"),\n                        os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"), \"--format-output\",\n                        \"query,target,qlen,tlen,alnlen,fident,qstart,qend,tstart,tend,evalue\",\n                        \"--format-mode\", \"4\"], stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        mmseqs_search_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_search_results.tsv\"),\n                                              sep=\"\\t\")\n        mmseqs_search_results[\"qcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"qlen\"], axis=1)\n        mmseqs_search_results[\"tcov\"] = mmseqs_search_results.apply(lambda row: row[\"alnlen\"] / row[\"tlen\"], axis=1)\n        mmseqs_search_results = mmseqs_search_results[\n            (mmseqs_search_results[\"qcov\"] &gt;= self.prms.args[\"mmseqs_search_qcov\"]) &amp;\n            (mmseqs_search_results[\"tcov\"] &gt;= self.prms.args[\"mmseqs_search_tcov\"]) &amp;\n            (mmseqs_search_results[\"fident\"] &gt;= self.prms.args[\"mmseqs_search_fident\"])]\n        queries_with_res = len(set(mmseqs_search_results[\"query\"].to_list()))\n        target_to_group = dict()\n        for proteome in self.proteomes.proteomes.to_list():\n            for cds in proteome.cdss.to_list():\n                target_to_group[cds.cds_id] = cds.group\n        mmseqs_search_results[\"group\"] = mmseqs_search_results[\"target\"].apply(lambda t: target_to_group[t])\n        mmseqs_search_results.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"mmseqs_homology_search_full.tsv\"),\n                                     sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            if queries_with_res &gt; 0:\n                print(f\"  \u29bf A homologous group was found for {queries_with_res}/{query_length} query protein\"\n                      f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n            else:\n                print(f\"  \u29bf No homologous group was found for {query_length} query protein\"\n                      f\"{'s' if query_length &gt; 1 else ''}\", file=sys.stdout)\n        return mmseqs_search_results\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs search versus protein database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Database.protein_search_mode","title":"<code>protein_search_mode(query_fasta, query_label=None, predefined_protein_group=None)</code>","text":"<p>Run protein search mode which finds homologues of your query proteins in the database and returns     comprehensive output including visualisation and hotspot annotation.</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>Fasta with query protein sequence.</p> </li> <li> <code>query_label</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>Label to be shown on lovis4u visualisation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def protein_search_mode(self, query_fasta: str, query_label: typing.Union[None, str] = None,\n                        predefined_protein_group: typing.Union[None, str] = None) -&gt; None:\n    \"\"\"Run protein search mode which finds homologues of your query proteins in the database and returns\n        comprehensive output including visualisation and hotspot annotation.\n\n    Arguments:\n        query_fasta (str): Fasta with query protein sequence.\n        query_label (str): Label to be shown on lovis4u visualisation.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Load fasta\n        if not predefined_protein_group:\n            query_records = list(Bio.SeqIO.parse(query_fasta, \"fasta\"))\n            if len(query_records) &gt; 1:\n                raise ilund4u.manager.ilund4uError(\"Only single query protein is allowed for protein mode\")\n            query_record = query_records[0]\n            # Run mmseqs for homology search\n            if self.prms.args[\"protein_search_target_mode\"] == \"proteins\" and self.prms.args[\n                \"fast_mmseqs_search_mode\"]:\n                print(\"\u25cb Fast mode is not available with 'proteins' search modea and was deactivated.\",\n                      file=sys.stdout)\n                self.prms.args[\"fast_mmseqs_search_mode\"] = False\n            mmseqs_results = self.mmseqs_search_versus_protein_database(query_fasta,\n                                                                        self.prms.args[\"fast_mmseqs_search_mode\"])\n            if len(mmseqs_results.index) == 0:\n                print(\"\u25cb Termination since no homology to hotspot db proteins was found\", file=sys.stdout)\n                return None\n            if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                homologous_protein_ids = mmseqs_results[\"target\"].to_list()\n                homologous_groups = mmseqs_results[\"group\"].to_list()\n            elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"],\n                                           ascending=[True, False, False, False], inplace=True)\n                mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n                homologous_protein_ids = []\n                homologous_group = mmseqs_results.at[query_record.id, \"group\"]\n                homologous_groups = [homologous_group]\n        else:\n            homologous_group = predefined_protein_group\n            homologous_groups = [homologous_group]\n            homologous_protein_ids = []\n            self.prms.args[\"protein_search_target_mode\"] = \"group\"\n        if \"protein_group_stat\" in self.db_paths.keys():\n            protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                \"representative_protein\")\n            groups_to_select = list(protein_group_stat_table.index.intersection(homologous_groups))\n            if groups_to_select:\n                protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                protein_group_stat_table.to_csv(\n                    os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                    sep=\"\\t\", index=True, index_label=\"representative_protein\")\n\n        # Searching for hotspots\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Searching for hotspots with your query protein homologues...\", file=sys.stdout)\n        found_hotspots = collections.defaultdict(list)\n        island_annotations = []\n        location_stat = dict(flanking=0, cargo=0)\n        n_flanked = 0\n        for hotspot in self.hotspots.hotspots.to_list():\n            if not self.prms.args[\"report_not_flanked\"] and not hotspot.flanked:\n                continue\n            for island in hotspot.islands:\n                proteome = self.proteomes.proteomes.at[island.proteome]\n                if self.prms.args[\"protein_search_target_mode\"] == \"proteins\":\n                    locus_proteins = island.get_locus_proteins(proteome.cdss)\n                    overlapping = list(set(homologous_protein_ids) &amp; set(locus_proteins))\n                elif self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                    locus_groups = island.get_locus_groups(proteome.cdss)\n                    overlapping = homologous_group in locus_groups\n                if overlapping:\n                    if self.prms.args[\"protein_search_target_mode\"] == \"group\":\n                        homologous_protein_ids_island = []\n                        locus_proteins = island.get_locus_proteins(proteome.cdss)\n                        for lp, lpg in zip(locus_proteins, locus_groups):\n                            if lpg == homologous_group:\n                                homologous_protein_ids_island.append(lp)\n                                homologous_protein_ids.append(lp)\n                        overlapping = homologous_protein_ids_island\n                    isl_groups = island.get_island_groups(proteome.cdss)\n                    isl_proteins = island.get_island_proteins(proteome.cdss)\n                    for op in overlapping:\n                        if op in isl_proteins:\n                            location_stat[\"cargo\"] += 1\n                            n_flanked += 1\n                        else:\n                            location_stat[\"flanking\"] += 1\n                    island_annotation = hotspot.island_annotation.loc[island.island_id].copy()\n                    island_annotation.drop(labels=[\"island_index\", \"strength\", \"degree\"], inplace=True)\n                    island_annotation.at[\"indexes\"] = \",\".join(map(str, island.indexes))\n                    island_annotation.at[\"size\"] = island.size\n                    island_annotation.at[\"island_proteins\"] = \",\".join(island.get_island_proteins(proteome.cdss))\n                    island_annotation.at[\"island_protein_groups\"] = \",\".join(isl_groups)\n                    island_annotation.at[\"query_homologues\"] = \",\".join(overlapping)\n                    island_annotations.append(island_annotation)\n                    found_hotspots[hotspot.hotspot_id].append(island)\n        if sum(location_stat.values()) == 0:\n            print(\"\u25cb Termination since no homologous protein was found in hotspots (neither as flanking or cargo)\",\n                  file=sys.stdout)\n            return None\n        found_islands = [island.island_id for islands in found_hotspots.values() for island in islands]\n        island_annotations = pd.DataFrame(island_annotations)\n        island_annotations.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                            \"found_island_annotation.tsv\")),\n                                  sep=\"\\t\", index_label=\"island_id\")\n        found_hotspots_annotation = self.hotspots.annotation.loc[found_hotspots.keys()]\n        found_hotspots_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"found_hotspot_annotation.tsv\"),\n                                         sep=\"\\t\", index_label=\"hotspot_id\")\n        found_hotspot_communities = list(set(found_hotspots_annotation[\"hotspot_community\"].to_list()))\n        # Get hotspot community stat\n        hotspot_community_annot_rows = []\n        r_types = [\"cargo\", \"flanking\"]\n        for h_com, hotspot_ids in self.hotspots.communities.items():\n            if h_com not in found_hotspot_communities:\n                continue\n            h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n            hotspot_com_groups = dict(cargo=set(), flanking=set())\n            hotspots = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n            n_islands, n_flanked = 0, 0\n            for hotspot in hotspots:\n                n_islands += hotspot.size\n                n_flanked += hotspot.flanked\n                hotspot_groups = hotspot.get_hotspot_groups(self.proteomes)\n                db_stat = hotspot.calculate_database_hits_stats(self.proteomes, self.prms, protein_mode=True)\n                for r_type in r_types:\n                    hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n            hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n            for r_type in r_types:\n                hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n            for db_name in self.prms.args[\"databases_classes\"]:\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n            hotspot_community_annot_rows.append(hc_annot_row)\n        hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n        for db_name in self.prms.args[\"databases_classes\"]:\n            hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                hotspot_community_annot.apply(\n                    lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n        hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                    \"found_hotspot_community_annotation.tsv\"),\n                                       sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Query protein homologues were found in {len(found_hotspot_communities)} hotspot \"\n                  f\"communit{'y' if len(found_hotspot_communities) == 1 else 'ies'} \"\n                  f\"({len(found_hotspots.keys())} hotspot{'s' if len(found_hotspots.keys()) &gt; 1 else ''}) on \"\n                  f\"{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''}\\n\"\n                  f\"    Found as cargo: {location_stat['cargo']}, as flanking gene: {location_stat['flanking']}\"\n                  f\"\\n    {n_flanked}/{len(found_islands)} island{'s' if len(found_islands) &gt; 1 else ''} where found\"\n                  f\" as cargo are both side flanked (have conserved genes on both sides)\",\n                  file=sys.stdout)\n\n        homologous_protein_fasta = os.path.join(self.prms.args[\"output_dir\"], \"homologous_proteins.fa\")\n        full_fasta_file = Bio.SeqIO.index(self.proteomes.proteins_fasta_file, \"fasta\")\n        with open(homologous_protein_fasta, \"w\") as out_handle:\n            if not predefined_protein_group:\n                Bio.SeqIO.write(query_record, out_handle, \"fasta\")\n            for acc in homologous_protein_ids:\n                out_handle.write(full_fasta_file.get_raw(acc).decode())\n        # MSA visualisation\n        if len(homologous_protein_ids) &gt; 1 or not predefined_protein_group:\n            msa4u_p = msa4u.manager.Parameters()\n            msa4u_p.arguments[\"label\"] = \"id\"\n            msa4u_p.arguments[\"verbose\"] = False\n            msa4u_p.arguments[\"output_filename\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                \"msa4u_homologous_proteines.pdf\")\n            msa4u_p.arguments[\"output_filename_aln\"] = os.path.join(self.prms.args[\"output_dir\"],\n                                                                    \"homologous_proteins_aln.fa\")\n            fasta = msa4u.manager.Fasta(fasta=homologous_protein_fasta, parameters=msa4u_p)\n            mafft_output = fasta.run_mafft()\n            msa = msa4u.manager.MSA(mafft_output, msa4u_p)\n            msa.plot()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf Homologous proteins were saved to {homologous_protein_fasta} and the MSA was \"\n                      f\"visualised with MSA4u\")\n        print(f\"\u25cb Visualisation of the hotspot(s) with your query protein homologues using lovis4u...\",\n              file=sys.stdout)\n        # lovis4u visualisation\n        vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                              os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_with_query\")]\n        for vis_output_folder in vis_output_folders:\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n        additional_annotation = dict()\n        for hpid in homologous_protein_ids:\n            additional_annotation[hpid] = dict(stroke_colour=\"#000000\", fill_colour=\"#000000\")\n            if query_label:\n                additional_annotation[hpid][\"name\"] = query_label\n        drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n        for community in found_hotspot_communities:\n            drawing_manager.plot_hotspots(self.hotspots.communities[community],\n                                          output_folder=os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_full\"),\n                                          additional_annotation=additional_annotation)\n        drawing_manager.plot_hotspots(list(found_hotspots.keys()),\n                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                 \"lovis4u_with_query\"),\n                                      island_ids=found_islands,\n                                      additional_annotation=additional_annotation)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Done!\")\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to perform protein search versus the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Database.proteome_annotation_mode","title":"<code>proteome_annotation_mode(query_gff)</code>","text":"<p>Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and     variable proteins in the query proteome in case a community with similar proteomes was found in the database.</p> <p>Arguments:f     query_gff (str): GFF with query proteome.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def proteome_annotation_mode(self, query_gff: str) -&gt; None:\n    \"\"\"Run proteome annotation mode which searches for similar proteomes in the database and annotate hotspots and\n        variable proteins in the query proteome in case a community with similar proteomes was found in the database.\n\n    Arguments:f\n        query_gff (str): GFF with query proteome.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # Load query gff\n        proteomes_helper_obj = ilund4u.data_processing.Proteomes(parameters=self.prms)\n        proteomes_helper_obj.load_sequences_from_extended_gff(input_f=[query_gff])\n        query_proteome = proteomes_helper_obj.proteomes.iat[0]\n        # Get and parse mmseqs search results\n        mmseqs_results = self.mmseqs_search_versus_protein_database(proteomes_helper_obj.proteins_fasta_file,\n                                                                    self.prms.args[\"fast_mmseqs_search_mode\"])\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Searching for similar proteomes in the database network\", file=sys.stdout)\n        mmseqs_results.sort_values(by=[\"evalue\", \"qcov\", \"tcov\", \"fident\"], ascending=[True, False, False, False],\n                                   inplace=True)\n        mmseqs_results = mmseqs_results.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n        proteins_wo_hits = []\n        for cds in query_proteome.cdss.to_list():\n            if cds.cds_id in mmseqs_results.index:\n                cds.group = mmseqs_results.at[cds.cds_id, \"group\"]\n            else:\n                cds.group = f\"{cds.cds_id}\"\n                proteins_wo_hits.append(cds.group)\n        if \"protein_group_stat\" in self.db_paths.keys():\n            protein_group_stat_table = pd.read_table(self.db_paths[\"protein_group_stat\"], sep=\"\\t\").set_index(\n                \"representative_protein\")\n            groups_to_select = list(protein_group_stat_table.index.intersection(mmseqs_results[\"group\"].tolist()))\n            if groups_to_select:\n                protein_group_stat_table = protein_group_stat_table.loc[groups_to_select]\n                protein_group_stat_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_stat.tsv\"),\n                                                sep=\"\\t\", index=True, index_label=\"representative_protein\")\n        # Running pyhmmer annotation\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Preparing data for protein annotation with pyhmmer hmmscan...\", file=sys.stdout)\n        alignment_table = ilund4u.methods.run_pyhmmer(proteomes_helper_obj.proteins_fasta_file,\n                                                      len(query_proteome.cdss.index), self.prms)\n        if not alignment_table.empty:\n            found_hits_for = alignment_table.index.to_list()\n            proteome_cdss = query_proteome.cdss.to_list()\n            proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.cds_id in found_hits_for]\n            if proteome_cdss_with_hits:\n                cdss_with_hits = query_proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                for cds in cdss_with_hits:\n                    alignment_table_row = alignment_table.loc[cds.cds_id]\n                    cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                               db_name=alignment_table_row[\"target_db\"],\n                                               target=alignment_table_row[\"target\"],\n                                               evalue=alignment_table_row[\"hit_evalue\"])\n        # Connect to the database proteome network\n        proteome_names = pd.Series({idx: sid for idx, sid in enumerate(self.proteomes.annotation.index)})\n        proteome_sizes = self.proteomes.annotation[[\"proteome_size_unique\", \"index\"]]\n        proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n        cluster_to_sequences = collections.defaultdict(list)\n        for p_index, proteome in enumerate(self.proteomes.proteomes.to_list()):\n            cds_groups = set(proteome.cdss.apply(lambda cds: cds.group).to_list())\n            for cds_g in cds_groups:\n                cluster_to_sequences[cds_g].append(proteome.proteome_id)\n        cluster_to_proteome_index = dict()\n        for cluster, sequences in cluster_to_sequences.items():\n            indexes = sorted([self.proteomes.seq_to_ind[seq_id] for seq_id in sequences])\n            cluster_to_proteome_index[cluster] = indexes\n        query_clusters = set(query_proteome.cdss.apply(lambda cds: cds.group).to_list())\n        query_size = len(query_clusters)\n        counts = collections.defaultdict(int)\n        for cl in query_clusters:\n            if cl not in proteins_wo_hits:\n                js = cluster_to_proteome_index[cl]\n                for j in js:\n                    counts[j] += 1\n        weights = pd.Series(counts)\n        proteome_sizes_connected = proteome_sizes.iloc[weights.index]\n        norm_factor = pd.Series(\n            0.5 * (query_size + proteome_sizes_connected) / (query_size * proteome_sizes_connected), \\\n            index=weights.index)\n        weights = weights.mul(norm_factor)\n        weights = weights[weights &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n        query_network_df = pd.DataFrame(dict(weight=weights, seq_id=proteome_names.iloc[weights.index]))\n        query_network_df.sort_values(by=\"weight\", inplace=True, ascending=False)\n        query_network_df.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_proteome_network.tsv\"),\n                                sep=\"\\t\", index_label=\"t_index\")\n        query_network_df = query_network_df.set_index(\"seq_id\")\n        max_weight = round(query_network_df[\"weight\"].max(), 2)\n        if len(weights.index) == 0:\n            print(\"\u25cb Termination since no similar proteome was found in the database\", file=sys.stdout)\n            sys.exit()\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf {len(weights.index)} similar proteomes were found in the database network with \"\n                  f\"max proteome similarity = {max_weight}\", file=sys.stdout)\n        similar_proteoms_ids = query_network_df.index.to_list()\n        # Assign the closest community\n        similar_communities_rows = list()\n        for pcom_id, pcom_pr_ids in self.proteomes.communities.items():\n            com_size = len(pcom_pr_ids)\n            overlapping = list(set(pcom_pr_ids) &amp; set(similar_proteoms_ids))\n            if overlapping:\n                weights_subset = query_network_df.loc[overlapping][\"weight\"].to_list()\n                similar_communities_rows.append(dict(com_id=pcom_id, com_size=com_size,\n                                                     connection_fr=len(overlapping) / com_size,\n                                                     avg_weight=np.mean(weights_subset)))\n        similar_communities = pd.DataFrame(similar_communities_rows)\n        similar_communities.sort_values(by=[\"avg_weight\", \"connection_fr\", \"com_size\"], inplace=True,\n                                        ascending=[False, False, False])\n        similar_communities.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"similar_proteome_communities.tsv\"),\n                                   sep=\"\\t\", index=False)\n        selected_community_dict = similar_communities.iloc[0].to_dict()\n        selected_community_dict[\"com_id\"] = int(selected_community_dict[\"com_id\"])\n        query_proteome_community = selected_community_dict[\"com_id\"]\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf The query proteome was assigned to a community (id: {int(selected_community_dict['com_id'])})\"\n                  f\" with connection to {round(selected_community_dict['connection_fr'], 2)} of its members with \"\n                  f\"avg weight = {round(selected_community_dict['avg_weight'], 2)}\", file=sys.stdout)\n        # Define cds classes\n        com_protein_classes = dict()\n        for com_proteome in self.proteomes.communities[selected_community_dict[\"com_id\"]]:\n            for cds in self.proteomes.proteomes.at[com_proteome].cdss:\n                com_protein_classes[cds.group] = cds.g_class\n        defined_classes_rows = []\n        for cds in query_proteome.cdss:\n            if cds.group in com_protein_classes.keys():\n                cds.g_class = com_protein_classes[cds.group]\n            else:\n                cds.g_class = \"variable\"\n            defined_classes_rows.append(dict(cds_id=cds.cds_id, cds_class=cds.g_class))\n        defined_classes = pd.DataFrame(defined_classes_rows)\n        defined_classes.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"query_protein_clusters.tsv\"),\n                               sep=\"\\t\", index=False)\n        defined_classes_c = collections.Counter(defined_classes[\"cds_class\"].to_list())\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Protein class distribution in query proteome: \"\n                  f\"{', '.join(f'{v} {k}' for k, v in defined_classes_c.items())}\"\n                  , file=sys.stdout)\n        # Annotate variable islands\n        query_proteome.annotate_variable_islands(self.prms)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf {len(query_proteome.islands.index)} variable island\"\n                  f\"{'s were' if len(query_proteome.islands.index) &gt; 1 else ' was'} annotated in the \"\n                  f\"query proteome\", file=sys.stdout)\n        # Connect query proteome islands to the island network\n        community_hotspots = [h for h in self.hotspots.hotspots.to_list() if\n                              h.proteome_community == query_proteome_community]\n        if not self.prms.args[\"report_not_flanked\"]:\n            community_hotspots_flanked = [h for h in community_hotspots if h.flanked == 1]\n            community_hotspots = community_hotspots_flanked\n        if community_hotspots:\n            hotspots_islands = [island for hotspot in community_hotspots for island in hotspot.islands]\n            island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(hotspots_islands)}\n            com_island_n_sizes = pd.Series()\n            com_neighbours = pd.Series()\n            cluster_to_island = collections.defaultdict(list)\n            for island in hotspots_islands:\n                island_proteome = self.proteomes.proteomes.at[island.proteome]\n                island_id = island.island_id\n                island_index = island_id_to_index[island_id]\n                conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(island_proteome.cdss))\n                com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                for cing in conserved_island_neighbours_groups:\n                    cluster_to_island[cing].append(island_index)\n            hotspot_hits_statistic_rows = []\n            for q_island in query_proteome.islands.to_list():\n                q_isl_neighbours = list(set(q_island.get_cons_neighbours_groups(query_proteome.cdss)))\n                q_isl_size = len(q_isl_neighbours)\n                q_isl_counts = collections.defaultdict(int)\n                for qin in q_isl_neighbours:\n                    js = cluster_to_island[qin]\n                    for j in js:\n                        q_isl_counts[j] += 1\n                q_isl_weights = pd.Series(q_isl_counts)\n                q_isl_connected_n_sizes = com_island_n_sizes.iloc[q_isl_weights.index]\n                q_isl_norm_factors = pd.Series(\n                    0.5 * (q_isl_size + q_isl_connected_n_sizes) / (q_isl_size * q_isl_connected_n_sizes),\n                    index=q_isl_weights.index)\n                q_isl_weights = q_isl_weights.mul(q_isl_norm_factors)\n                q_isl_weights = q_isl_weights[\n                    q_isl_weights &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                q_isl_similar_islands = q_isl_weights.index.to_list()\n                similar_hotspots_rows = list()\n                for c_hotspot in community_hotspots:\n                    hotspot_islands = c_hotspot.islands\n                    hotspot_islsnds_idx = [island_id_to_index[isl.island_id] for isl in hotspot_islands]\n                    overlapping = list(set(q_isl_similar_islands) &amp; set(hotspot_islsnds_idx))\n                    if overlapping:\n                        weights_subset = q_isl_weights.loc[overlapping].to_list()\n                        connection_fr = len(overlapping) / c_hotspot.size\n                        if connection_fr &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]:\n                            similar_hotspots_rows.append(dict(hotspot_id=c_hotspot.hotspot_id,\n                                                              hotspot_size=c_hotspot.size,\n                                                              connection_fr=connection_fr,\n                                                              avg_weight=np.mean(weights_subset)))\n                if similar_hotspots_rows:\n                    similar_hotspots_rows = pd.DataFrame(similar_hotspots_rows)\n                    similar_hotspots_rows.sort_values(by=[\"avg_weight\", \"connection_fr\", \"hotspot_size\"],\n                                                      inplace=True,\n                                                      ascending=[False, False, False])\n                    best_hit_for_q_isl = similar_hotspots_rows.iloc[0].to_dict()\n                    hotspot_hits_statistic_rows.append(dict(query_island_id=q_island.island_id,\n                                                            query_island_proteins=\",\".join(\n                                                                q_island.get_island_proteins(query_proteome.cdss)),\n                                                            closest_hotspot=best_hit_for_q_isl[\"hotspot_id\"],\n                                                            closest_hotspot_size=best_hit_for_q_isl[\"hotspot_size\"],\n                                                            closest_hotspot_avg_weight=best_hit_for_q_isl[\n                                                                \"avg_weight\"],\n                                                            closest_hotspot_connection_fr=best_hit_for_q_isl[\n                                                                \"connection_fr\"]))\n            hotspot_hits_statistic = pd.DataFrame(hotspot_hits_statistic_rows)\n            hotspot_hits_statistic.sort_values(by=[\"closest_hotspot_avg_weight\", \"closest_hotspot_connection_fr\"],\n                                               inplace=True, ascending=[False, False])\n            hotspot_hits_statistic.to_csv(\n                os.path.join(self.prms.args[\"output_dir\"], \"island_to_hotspot_mapping.tsv\"),\n                sep=\"\\t\", index=False)\n            hotspot_hits_statistic = hotspot_hits_statistic.drop_duplicates(subset=\"closest_hotspot\", keep=\"first\")\n            hotspot_hits_statistic.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                       \"island_to_hotspot_mapping_deduplicated.tsv\"),\n                                          sep=\"\\t\", index=False)\n            subset_hotspot_annotation = self.hotspots.annotation.loc[hotspot_hits_statistic[\"closest_hotspot\"]]\n            subset_hotspot_annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                          \"annotation_of_mapped_hotspots.tsv\"), sep=\"\\t\",\n                                             index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf {len(hotspot_hits_statistic.index)} of annotated variable island\"\n                      f\"{'s were' if len(hotspot_hits_statistic.index) &gt; 1 else ' was'} mapped to \"\n                      f\"the database hotspots\", file=sys.stdout)\n        else:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u29bf The proteome community does not have any annotated hotspots\", file=sys.stdout)\n        # Update proteome and hotspots objects\n        self.proteomes.proteomes.at[query_proteome.proteome_id] = query_proteome\n        self.proteomes.annotation.loc[query_proteome.proteome_id] = proteomes_helper_obj.annotation.loc[\n            query_proteome.proteome_id]\n        self.proteomes.communities[query_proteome_community].append(query_proteome.proteome_id)\n        if community_hotspots:\n            for index, row in hotspot_hits_statistic.iterrows():\n                hotspot_id = row[\"closest_hotspot\"]\n                query_island_id = row[\"query_island_id\"]\n                query_island = query_proteome.islands.at[query_island_id]\n                closest_hotspot = self.hotspots.hotspots.at[hotspot_id]\n                closest_hotspot.islands.append(query_island)\n        # Visualisation\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Lovis4u visualisation of communities and proteomes...\", file=sys.stdout)\n        drawing_manager = ilund4u.drawing.DrawingManager(self.proteomes, self.hotspots, self.prms)\n\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"hotspot\",\n                                                filename=\"lovis4u_proteome_community_hotspots.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"regular\",\n                                                filename=\"lovis4u_proteome_community.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"regular\",\n                                                proteome_ids=[query_proteome.proteome_id],\n                                                filename=\"lovis4u_query_proteome_variable.pdf\")\n        drawing_manager.plot_proteome_community(community=query_proteome_community,\n                                                output_folder=self.prms.args[\"output_dir\"],\n                                                mode=\"hotspot\",\n                                                proteome_ids=[query_proteome.proteome_id],\n                                                filename=\"lovis4u_query_proteome_hotspot.pdf\")\n        if community_hotspots:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Lovis4u visualisation found hotspots..\", file=sys.stdout)\n            if len(hotspot_hits_statistic.index) &gt; 0:\n                vis_output_folders = [os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_full\"),\n                                      os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspot_with_query\")]\n                for vis_output_folder in vis_output_folders:\n                    if os.path.exists(vis_output_folder):\n                        shutil.rmtree(vis_output_folder)\n                    os.mkdir(vis_output_folder)\n            for index, row in hotspot_hits_statistic.iterrows():\n                hotspot_id = row[\"closest_hotspot\"]\n                query_island = row[\"query_island_id\"]\n                for hc, c_hotspots in self.hotspots.communities.items():\n                    if hotspot_id in c_hotspots:\n                        drawing_manager.plot_hotspots([hotspot_id],\n                                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                 \"lovis4u_hotspot_with_query\"),\n                                                      island_ids=[query_island])\n                        drawing_manager.plot_hotspots(c_hotspots,\n                                                      output_folder=os.path.join(self.prms.args[\"output_dir\"],\n                                                                                 \"lovis4u_hotspot_full\"),\n                                                      keep_while_deduplication=[query_island])\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf Done!\")\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run proteome annotation mode versus the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspot","title":"<code>Hotspot</code>","text":"<p>Hotspot object represent a hotspot as a set of islands.</p> <p>Attributes:</p> <ul> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot identifier.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Number of islands.</p> </li> <li> <code>proteome_community</code>             (<code>int</code>)         \u2013          <p>Identifier of proteome community where hotspot is annotated.</p> </li> <li> <code>islands</code>             (<code>list</code>)         \u2013          <p>List of islands.</p> </li> <li> <code>conserved_signature</code>             (<code>list</code>)         \u2013          <p>Conserved flanking proteins that are usually found in islands.</p> </li> <li> <code>island_annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of islands.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether hotspot consists of flanked islands or not (that have conserved genes on both sides) [int: 1 or 0]</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Hotspot:\n    \"\"\"Hotspot object represent a hotspot as a set of islands.\n\n    Attributes:\n        hotspot_id (str): Hotspot identifier.\n        size (int): Number of islands.\n        proteome_community (int): Identifier of proteome community where hotspot is annotated.\n        islands (list): List of islands.\n        conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n        island_annotation (pd.DataFrame): Annotation table of islands.\n        flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n            [int: 1 or 0]\n\n    \"\"\"\n\n    def __init__(self, hotspot_id: str, size: int, proteome_community: int, islands: list,\n                 conserved_signature: list, island_annotation: pd.DataFrame, flanked: int):\n        \"\"\"Hotspot class constructor.\n\n        Arguments:\n            hotspot_id (str): Hotspot identifier.\n            size (int): Number of islands.\n            proteome_community (int): Identifier of proteome community where hotspot is annotated.\n            islands (list): List of islands.\n            conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n            island_annotation (pd.DataFrame): Annotation table of islands.\n            flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n                [int: 1 or 0]\n\n        \"\"\"\n        self.hotspot_id = hotspot_id\n        self.size = size\n        self.proteome_community = proteome_community\n        self.islands = islands\n        self.island_annotation = island_annotation\n        self.conserved_signature = conserved_signature\n        self.flanked = flanked\n\n    def get_hotspot_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"island_annotation\", \"islands\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n\n    def calculate_database_hits_stats(self, proteomes: Proteomes, prms: ilund4u.manager.Parameters,\n                                      protein_mode=False) -&gt; collections.defaultdict:\n        \"\"\"Calculate statistics of pyhmmer annotation for island proteins.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n            prms (ilund4u.manager.Parameters): Parameters object.\n\n        Returns:\n            collections.defaultdict: hits to the databases.\n\n        \"\"\"\n        hotspot_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n        for island in self.islands:\n            proteome = proteomes.proteomes.at[island.proteome]\n            island.calculate_database_hits_stat(proteome.cdss)\n            island_dbstat = island.databases_hits_stat\n            db_names = prms.args[\"databases_classes\"]\n            for db_name in db_names:\n                r_types = [\"cargo\", \"flanking\"]\n                for r_type in r_types:\n                    if not protein_mode:\n                        self.island_annotation.at[island.island_id, f\"{db_name}_{r_type}\"] = \\\n                            \",\".join(island_dbstat[db_name][r_type].values())\n                    try:\n                        hotspot_stat[db_name][r_type].update(island_dbstat[db_name][r_type])\n                    except:\n                        # ! For old db version | to remove later\n                        db_name_transform_dict = {\"defence\": \"Defence\", \"AMR\": \"AMR\",\n                                                  \"virulence\": \"Virulence\", \"anti-defence\": \"Anti-defence\"}\n                        hotspot_stat[db_name][r_type].update(island_dbstat[db_name_transform_dict[db_name]][r_type])\n\n        return hotspot_stat\n\n    def get_hotspot_groups(self, proteomes: Proteomes) -&gt; dict:\n        \"\"\"Get protein groups found on island cargo or as flanking genes\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            dict: cargo and flanking gene protein groups.\n\n        \"\"\"\n        groups = dict(cargo=set(), flanking=set())\n        for island in self.islands:\n            proteome = proteomes.proteomes.at[island.proteome]\n            groups[\"cargo\"].update(set(island.get_island_groups(proteome.cdss)))\n            groups[\"flanking\"].update(set(island.get_flanking_groups(proteome.cdss)))\n        return groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspot.__init__","title":"<code>__init__(hotspot_id, size, proteome_community, islands, conserved_signature, island_annotation, flanked)</code>","text":"<p>Hotspot class constructor.</p> <p>Parameters:</p> <ul> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot identifier.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Number of islands.</p> </li> <li> <code>proteome_community</code>             (<code>int</code>)         \u2013          <p>Identifier of proteome community where hotspot is annotated.</p> </li> <li> <code>islands</code>             (<code>list</code>)         \u2013          <p>List of islands.</p> </li> <li> <code>conserved_signature</code>             (<code>list</code>)         \u2013          <p>Conserved flanking proteins that are usually found in islands.</p> </li> <li> <code>island_annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of islands.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether hotspot consists of flanked islands or not (that have conserved genes on both sides) [int: 1 or 0]</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, hotspot_id: str, size: int, proteome_community: int, islands: list,\n             conserved_signature: list, island_annotation: pd.DataFrame, flanked: int):\n    \"\"\"Hotspot class constructor.\n\n    Arguments:\n        hotspot_id (str): Hotspot identifier.\n        size (int): Number of islands.\n        proteome_community (int): Identifier of proteome community where hotspot is annotated.\n        islands (list): List of islands.\n        conserved_signature (list): Conserved flanking proteins that are usually found in islands.\n        island_annotation (pd.DataFrame): Annotation table of islands.\n        flanked (int): Whether hotspot consists of flanked islands or not (that have conserved genes on both sides)\n            [int: 1 or 0]\n\n    \"\"\"\n    self.hotspot_id = hotspot_id\n    self.size = size\n    self.proteome_community = proteome_community\n    self.islands = islands\n    self.island_annotation = island_annotation\n    self.conserved_signature = conserved_signature\n    self.flanked = flanked\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspot.calculate_database_hits_stats","title":"<code>calculate_database_hits_stats(proteomes, prms, protein_mode=False)</code>","text":"<p>Calculate statistics of pyhmmer annotation for island proteins.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>defaultdict</code>         \u2013          <p>collections.defaultdict: hits to the databases.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_database_hits_stats(self, proteomes: Proteomes, prms: ilund4u.manager.Parameters,\n                                  protein_mode=False) -&gt; collections.defaultdict:\n    \"\"\"Calculate statistics of pyhmmer annotation for island proteins.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n        prms (ilund4u.manager.Parameters): Parameters object.\n\n    Returns:\n        collections.defaultdict: hits to the databases.\n\n    \"\"\"\n    hotspot_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n    for island in self.islands:\n        proteome = proteomes.proteomes.at[island.proteome]\n        island.calculate_database_hits_stat(proteome.cdss)\n        island_dbstat = island.databases_hits_stat\n        db_names = prms.args[\"databases_classes\"]\n        for db_name in db_names:\n            r_types = [\"cargo\", \"flanking\"]\n            for r_type in r_types:\n                if not protein_mode:\n                    self.island_annotation.at[island.island_id, f\"{db_name}_{r_type}\"] = \\\n                        \",\".join(island_dbstat[db_name][r_type].values())\n                try:\n                    hotspot_stat[db_name][r_type].update(island_dbstat[db_name][r_type])\n                except:\n                    # ! For old db version | to remove later\n                    db_name_transform_dict = {\"defence\": \"Defence\", \"AMR\": \"AMR\",\n                                              \"virulence\": \"Virulence\", \"anti-defence\": \"Anti-defence\"}\n                    hotspot_stat[db_name][r_type].update(island_dbstat[db_name_transform_dict[db_name]][r_type])\n\n    return hotspot_stat\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspot.get_hotspot_db_row","title":"<code>get_hotspot_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_hotspot_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"island_annotation\", \"islands\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspot.get_hotspot_groups","title":"<code>get_hotspot_groups(proteomes)</code>","text":"<p>Get protein groups found on island cargo or as flanking genes</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>cargo and flanking gene protein groups.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_hotspot_groups(self, proteomes: Proteomes) -&gt; dict:\n    \"\"\"Get protein groups found on island cargo or as flanking genes\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        dict: cargo and flanking gene protein groups.\n\n    \"\"\"\n    groups = dict(cargo=set(), flanking=set())\n    for island in self.islands:\n        proteome = proteomes.proteomes.at[island.proteome]\n        groups[\"cargo\"].update(set(island.get_island_groups(proteome.cdss)))\n        groups[\"flanking\"].update(set(island.get_flanking_groups(proteome.cdss)))\n    return groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots","title":"<code>Hotspots</code>","text":"<p>Hotspots object represents a set of annotated hotspots.</p> <p>Attributes:</p> <ul> <li> <code>hotspots</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Hotspot objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of hotspots.</p> </li> <li> <code>communities</code>             (<code>dict</code>)         \u2013          <p>Dictionary representing annotated communities of hotspots (key - community_id, value - list of hotspot ids)</p> </li> <li> <code>island_rep_proteins_fasta</code>             (<code>str</code>)         \u2013          <p>Path to a fasta file containing island representative proteins.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Hotspots:\n    \"\"\"Hotspots object represents a set of annotated hotspots.\n\n    Attributes:\n        hotspots (pd.Series): Series (list) of Hotspot objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n        communities (dict): Dictionary representing annotated communities of hotspots\n            (key - community_id, value - list of hotspot ids)\n        island_rep_proteins_fasta (str): Path to a fasta file containing island representative proteins.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, hotspots: pd.Series, annotation: pd.DataFrame, parameters: ilund4u.manager.Parameters):\n        \"\"\"Hotspots class constructor.\n\n        Arguments:\n            hotspots (pd.Series): Series (list) of Hotspot objects.\n            annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.hotspots = hotspots\n        self.annotation = annotation\n        self.communities = dict()\n        self.prms = parameters\n        self.__id_to_ind = {iid: idx for idx, iid in enumerate(self.annotation.index)}\n        self.island_rep_proteins_fasta = os.path.join(parameters.args[\"output_dir\"], \"island_rep_proteins.fa\")\n\n    def save_as_db(self, db_folder: str) -&gt; None:\n        \"\"\"Save Hotspots to the iLnd4u database.\n\n        Arguments:\n            db_folder (str): Database folder path.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            attributes_to_ignore = [\"hotspots\", \"annotation\", \"communities_annot\", \"prms\"]\n            attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n            attributes[\"island_rep_proteins_fasta\"] = os.path.basename(attributes[\"island_rep_proteins_fasta\"])\n            with open(os.path.join(db_folder, \"hotspots.attributes.json\"), 'w') as json_file:\n                json.dump(attributes, json_file)\n            self.annotation.to_csv(os.path.join(db_folder, \"hotspots.annotations.tsv\"), sep=\"\\t\",\n                                   index_label=\"hotspot_id\")\n            island_annotation_table = pd.DataFrame()\n            hotspot_db_ind = []\n            for hotspot in self.hotspots.to_list():\n                hotspot_db_ind.append(hotspot.get_hotspot_db_row())\n                h_island_annot = hotspot.island_annotation.copy()\n                h_island_annot[\"hotspot_id\"] = hotspot.hotspot_id\n                island_annotation_table = pd.concat([island_annotation_table, h_island_annot])\n\n            with open(os.path.join(db_folder, \"hotspot.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(hotspot_db_ind, json_file)\n\n            os.system(f\"cp {os.path.join(self.prms.args['output_dir'], 'protein_group_accumulated_statistics.tsv')} \"\n                      f\"{db_folder}\")\n\n            island_annotation_table.to_csv(os.path.join(db_folder, \"hotspot.ind.island.annotations.tsv\"), sep=\"\\t\",\n                                           index_label=\"island\")\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to write hotspots to the database.\") from error\n\n    @classmethod\n    def db_init(cls, db_path: str, proteomes: Proteomes, parameters: ilund4u.manager.Parameters):\n        \"\"\"Class method to load a Proteomes object from a database.\n\n        Arguments:\n            db_path (str): path to the database.\n            proteomes (Proteomes): Proteomes object.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            cls: Hotspots object.\n\n        \"\"\"\n        try:\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading hotspot objects...\", file=sys.stdout)\n            island_annotation = pd.read_table(os.path.join(db_path, \"hotspot.ind.island.annotations.tsv\"),\n                                              sep=\"\\t\", low_memory=False).set_index(\"island\")\n            with open(os.path.join(db_path, \"hotspot.ind.attributes.json\"), \"r\") as json_file:\n                hotspot_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(hotspot_ind_attributes), suffix='%(index)d/%(max)d')\n            hotspot_list = []\n            for hotspot_dict in hotspot_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                hotspot_dict[\"island_annotation\"] = island_annotation[\n                    island_annotation[\"hotspot_id\"] == hotspot_dict[\"hotspot_id\"]].copy()\n                hotspot_proteomes = proteomes.proteomes.loc[\n                    proteomes.communities[hotspot_dict[\"proteome_community\"]]].to_list()\n                islands_list = [island for proteome in hotspot_proteomes for island in proteome.islands.to_list()]\n                islands_series = pd.Series(islands_list, index=[island.island_id for island in islands_list])\n                hotspot_dict[\"islands\"] = islands_series.loc[hotspot_dict[\"island_annotation\"].index].to_list()\n                hotspot_list.append(Hotspot(**hotspot_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            hotspots = pd.Series(hotspot_list, index=[hotspot.hotspot_id for hotspot in hotspot_list])\n            annotation = pd.read_table(os.path.join(db_path, \"hotspots.annotations.tsv\"),\n                                       sep=\"\\t\", dtype={\"community\": \"Int32\"}).set_index(\"hotspot_id\")\n            cls_obj = cls(hotspots, annotation, parameters)\n            with open(os.path.join(db_path, \"hotspots.attributes.json\"), \"r\") as json_file:\n                attributes = json.load(json_file)\n            cls_obj.communities = {int(k): v for k, v in attributes[\"communities\"].items()}\n            cls_obj.island_rep_proteins_fasta = os.path.join(db_path, attributes[\"island_rep_proteins_fasta\"])\n            return cls_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to read hotspots from the database.\") from error\n\n    def pyhmmer_annotation(self, proteomes: Proteomes) -&gt; None:\n        \"\"\"Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Preparing data for additional island protein annotation with pyhmmer hmmscan...\",\n                      file=sys.stdout)\n            hotspots_repr_proteins = set()\n            for hotspot in self.hotspots.to_list():\n                for island in hotspot.islands:\n                    proteome = proteomes.proteomes.at[island.proteome]\n                    isl_groups = island.get_locus_groups(proteome.cdss)\n                    hotspots_repr_proteins.update(isl_groups)\n            initial_fasta_file = Bio.SeqIO.index(proteomes.proteins_fasta_file, \"fasta\")\n            with open(self.island_rep_proteins_fasta, \"wb\") as out_handle:\n                for acc in hotspots_repr_proteins:\n                    try:\n                        out_handle.write(initial_fasta_file.get_raw(acc))\n                    except:\n                        pass\n            alignment_table = ilund4u.methods.run_pyhmmer(self.island_rep_proteins_fasta, len(hotspots_repr_proteins),\n                                                          self.prms)\n            if not alignment_table.empty:\n                found_hits_for = alignment_table.index.to_list()\n                for proteome in proteomes.proteomes.to_list():\n                    proteome_cdss = proteome.cdss.to_list()\n                    proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.group in found_hits_for]\n                    if proteome_cdss_with_hits:\n                        cdss_with_hits = proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                        for cds in cdss_with_hits:\n                            alignment_table_row = alignment_table.loc[cds.group]\n                            cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                       db_name=alignment_table_row[\"target_db\"],\n                                                       target=alignment_table_row[\"target\"],\n                                                       evalue=alignment_table_row[\"hit_evalue\"])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run pyhmmer hmmscan annotation.\") from error\n\n    def build_hotspot_network(self):\n        \"\"\"Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Hotspot network construction...\", file=sys.stdout)\n            hotspot_signature_sizes = pd.Series()\n            hotspot_proteome_community = dict()\n            flanked_stat = dict()\n            signature_cluster_to_hotspot = collections.defaultdict(collections.deque)\n            for hid, hotspot in enumerate(self.hotspots):\n                flanked_stat[hid] = hotspot.flanked\n                hotspot_proteome_community[hid] = hotspot.proteome_community\n                hotspot_signature_sizes.at[hid] = len(hotspot.conserved_signature)\n                for cs_cluster in hotspot.conserved_signature:\n                    signature_cluster_to_hotspot[cs_cluster].append(hid)\n            edges, weights = [], []\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots), suffix=\"%(index)d/%(max)d\")\n            for i, hotspot_i in enumerate(self.hotspots):\n                bar.next()\n                pc_i = hotspot_i.proteome_community\n                signature_i = hotspot_i.conserved_signature\n                size_i = hotspot_signature_sizes.iat[i]\n                counts_i = collections.defaultdict(int)\n                for sc in signature_i:\n                    js = signature_cluster_to_hotspot[sc]\n                    for j in js.copy():\n                        if i &lt; j:\n                            if pc_i != hotspot_proteome_community[j]:  # to think about it\n                                counts_i[j] += 1\n                        else:\n                            js.popleft()\n                weights_i = pd.Series(counts_i)\n                connected_n_sizes = hotspot_signature_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                          index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"hotspot_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    if flanked_stat[i] == flanked_stat[j]:\n                        edges.append([i, j])\n                        weights.append(round(w, 4))\n            bar.finish()\n            print(\"\u25cb Hotspot network partitioning using the Leiden algorithm...\")\n            graph = igraph.Graph(len(hotspot_signature_sizes.index), edges, directed=False)\n            graph.vs[\"index\"] = hotspot_signature_sizes.index.to_list()\n            graph.vs[\"hotspot_id\"] = [h.hotspot_id for h in self.hotspots]\n            graph.vs[\"flanked\"] = [h.flanked for h in self.hotspots]\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(self.prms.args[\"output_dir\"], f\"hotspot_network.gml\"))\n            partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_h\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n            hotspot_communities_annot_rows = []\n            communities_sizes = []\n            self.annotation[\"hotspot_community\"] = pd.Series(dtype='Int64')\n            for community_index, community in enumerate(partition_leiden):\n                community_size = len(community)\n                subgraph = graph.subgraph(community)\n                hotspots = subgraph.vs[\"hotspot_id\"]\n                n_flanked = sum(subgraph.vs[\"flanked\"])\n                self.communities[community_index] = hotspots\n                self.annotation.loc[hotspots, \"hotspot_community\"] = community_index\n                if community_size &gt; 1:\n                    communities_sizes.append(community_size)\n                    subgraph_edges = subgraph.get_edgelist()\n                    num_of_edges = len(subgraph_edges)\n                    num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                    weights = subgraph.es[\"weight\"]\n                    avg_weight = round(np.mean(weights), 3)\n                    max_identity = max(weights)\n                else:\n                    num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n                hotspot_communities_annot_rows.append([community_index, community_size, avg_weight, n_flanked,\n                                                       max_identity, num_of_edges_fr, \";\".join(hotspots)])\n            communities_annot = pd.DataFrame(hotspot_communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\",\n                                                                                      \"n_flanked\", \"max_weight\",\n                                                                                      \"fr_edges\", \"hotspots\"])\n            communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                               \"hotspot_communities.tsv\")),\n                                     sep=\"\\t\", index=False)\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {sum(communities_sizes)} hotspots were merged to {len(communities_sizes)} not singleton \"\n                      f\"communities\")\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to build hotspot network.\") from error\n\n    def calculate_hotspot_and_island_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n        \"\"\"Calculate hotspot statistics based using hmmscan results and save annotation tables.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            pd.DataFrame: hotspot community annotation table.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Hotspot and island statistics calculation...\", file=sys.stdout)\n            hotspot_community_annot_rows = []\n            r_types = [\"cargo\", \"flanking\"]\n            # Create new columns\n            for r_type in r_types:\n                self.annotation[f\"N_{r_type}_groups\"] = pd.Series(dtype='Int64')\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    self.annotation[f\"N_{db_name}_{r_type}_groups\"] = pd.Series(dtype='Int64')\n            self.annotation[\"conserved_signature\"] = pd.Series(dtype=\"str\")\n            # Get stat\n            for h_com, hotspot_ids in self.communities.items():\n                h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n                hotspot_com_groups = dict(cargo=set(), flanking=set())\n                hotspots = self.hotspots.loc[hotspot_ids].to_list()\n                n_islands, n_flanked = 0, 0\n                for hotspot in hotspots:\n                    n_islands += hotspot.size\n                    n_flanked += hotspot.flanked\n                    hotspot_groups = hotspot.get_hotspot_groups(proteomes)\n                    db_stat = hotspot.calculate_database_hits_stats(proteomes, self.prms)\n                    self.annotation.at[hotspot.hotspot_id, \"conserved_signature\"] = \";\".join(\n                        hotspot.conserved_signature)\n                    for r_type in r_types:\n                        hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                        self.annotation.at[hotspot.hotspot_id, f\"N_{r_type}_groups\"] = len(hotspot_groups[r_type])\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        for r_type in r_types:\n                            h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                            self.annotation.at[hotspot.hotspot_id, f\"N_{db_name}_{r_type}_groups\"] = \\\n                                len(set(db_stat[db_name][r_type].values()))\n                hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                    N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                    pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n                hotspot_community_annot_rows.append(hc_annot_row)\n\n            hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n            for db_name in self.prms.args[\"databases_classes\"]:\n                self.annotation[f\"{db_name}_cargo_normalised\"] = \\\n                    self.annotation.apply(lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"],\n                                                            4), axis=1)\n                hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                    hotspot_community_annot.apply(\n                        lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n            self.annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"hotspot_annotation.tsv\"), sep=\"\\t\",\n                                   index_label=\"hotspot_id\")\n            hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                        \"hotspot_community_annotation.tsv\"), sep=\"\\t\", index=False)\n            # Save island annotation table\n            # Get non-hotspot island stat\n            island_annotation_table_rows = []\n            for pcom, com_proteomes in proteomes.communities.items():\n                for proteome_id in com_proteomes:\n                    proteome = proteomes.proteomes.at[proteome_id]\n                    for island in proteome.islands.to_list():\n                        island_annot = dict(island=island.island_id, proteome=proteome.proteome_id,\n                                            proteome_commuity=pcom, hotspot_id=island.hotspot_id,\n                                            flanked=island.flanked, island_size=island.size)\n                        island.calculate_database_hits_stat(proteome.cdss)\n                        island_dbstat = island.databases_hits_stat\n                        db_names = self.prms.args[\"databases_classes\"]\n                        for db_name in db_names:\n                            r_types = [\"cargo\", \"flanking\"]\n                            for r_type in r_types:\n                                island_annot[f\"N_{db_name}_{r_type}\"] = len(island_dbstat[db_name][r_type].values())\n                        isl_groups = island.get_island_groups(proteome.cdss)\n                        isl_proteins = island.get_island_proteins(proteome.cdss)\n                        island_annot[\"island_proteins\"] = \",\".join(isl_proteins)\n                        island_annot[\"island_protein_groups\"] = \",\".join(isl_groups)\n                        island_annotation_table_rows.append(island_annot)\n            island_annotation_table = pd.DataFrame(island_annotation_table_rows)\n            island_annotation_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"island_annotation.tsv\"),\n                                           sep=\"\\t\", index=False)\n            return hotspot_community_annot\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to calculate hotspot and hotspot community statistics based \"\n                                               \"on hmmscan results\") from error\n\n    def get_each_protein_group_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n        \"\"\"Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots\n            where it's encoded.\n\n        Arguments:\n            proteomes (Proteomes): Proteomes object.\n\n        Returns:\n            pd.DataFrame: Protein group statistics table\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Protein group statistics calculation...\", file=sys.stdout)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots.index), suffix='%(index)d/%(max)d')\n            protein_group_statistics_dict = collections.defaultdict(\n                lambda: {\"Hotspot_communities\": set(), \"Hotspots\": set(), \"Hotpot_islands\": set(),\n                         \"Non_hotspot_islands\": set(), \"Proteome_communities\": set(), \"Flanked_hotspot_islands\": set(),\n                         \"Flanked_non_hotspot_islands\": set(), \"Counts\": 0, \"db\": \"None\", \"db_hit\": \"None\", \"Name\": \"\",\n                         \"RepLength\": 0})\n            # Hotspot stat\n            for h_com, hotspots_ids in self.communities.items():\n                hotspots = self.hotspots.loc[hotspots_ids].to_list()\n                for hotspot in hotspots:\n                    if self.prms.args[\"verbose\"]:\n                        bar.next()\n                    for island in hotspot.islands:\n                        proteome = proteomes.proteomes.at[island.proteome]\n                        island_proteins = island.get_island_proteins(proteome.cdss)\n                        island_protein_groups = island.get_island_groups(proteome.cdss)\n                        for isp, ispg in zip(island_proteins, island_protein_groups):\n                            if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                cds_obj = proteome.cdss.at[isp]\n                                protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                if cds_obj.hmmscan_results:\n                                    protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\"target\"]\n                                    protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                    if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                        protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                            \"db_name\"]\n                            protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                            protein_group_statistics_dict[ispg][\"Hotspot_communities\"].add(h_com)\n                            protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(hotspot.proteome_community)\n                            protein_group_statistics_dict[ispg][\"Hotspots\"].add(hotspot.hotspot_id)\n                            protein_group_statistics_dict[ispg][\"Hotpot_islands\"].add(island.island_id)\n                            if island.flanked:\n                                protein_group_statistics_dict[ispg][\"Flanked_hotspot_islands\"].add(island.island_id)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            # Other accessory genes\n            for proteome_com, com_proteomes in proteomes.communities.items():\n                for proteome_id in com_proteomes:\n                    proteome = proteomes.proteomes.at[proteome_id]\n                    for island in proteome.islands.to_list():\n                        if island.hotspot_id == \"-\":\n                            island_proteins = island.get_island_proteins(proteome.cdss)\n                            island_protein_groups = island.get_island_groups(proteome.cdss)\n                            for isp, ispg in zip(island_proteins, island_protein_groups):\n                                if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                    cds_obj = proteome.cdss.at[isp]\n                                    protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                    protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                    if cds_obj.hmmscan_results:\n                                        protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\n                                            \"target\"]\n                                        protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                        if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                            protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                                \"db_name\"]\n                                protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                                protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(proteome_com)\n                                protein_group_statistics_dict[ispg][\"Non_hotspot_islands\"].add(island.island_id)\n                                if island.flanked:\n                                    protein_group_statistics_dict[ispg][\"Flanked_non_hotspot_islands\"].add(\n                                        island.island_id)\n\n            statistic_rows = []\n            for pg, pg_dict in protein_group_statistics_dict.items():\n                row_dict = dict(representative_protein=pg, db=pg_dict[\"db\"], pb_hit=pg_dict[\"db_hit\"],\n                                name=pg_dict[\"Name\"], counts=pg_dict[\"Counts\"], length=pg_dict[\"RepLength\"])\n                for k, v in pg_dict.items():\n                    if isinstance(v, set):\n                        row_dict[f\"N_{k}\"] = len(v)\n                if pg_dict[\"Hotspots\"]:\n                    for db_name in self.prms.args[\"databases_classes\"]:\n                        dbsn_norm_values = []\n                        for hotspot in pg_dict[\"Hotspots\"]:\n                            dbsn_norm_value = self.annotation.at[hotspot, f\"{db_name}_cargo_normalised\"]\n                            if pg_dict[\"db\"] == db_name:\n                                dbsn_norm_value -= 1 / self.annotation.at[hotspot, f\"N_cargo_groups\"]\n                            dbsn_norm_values.append(dbsn_norm_value)\n                        row_dict[f\"{db_name}_avg_cargo_fraction\"] = round(np.mean(dbsn_norm_values), 4)\n                        row_dict[f\"{db_name}_max_cargo_fraction\"] = round(max(dbsn_norm_values), 4)\n                    row_dict[\"hotspots\"] = \",\".join(pg_dict[\"Hotspots\"])\n                else:\n                    row_dict[\"hotspots\"] = \"Non\"\n\n                statistic_rows.append(row_dict)\n            statistic_table = pd.DataFrame(statistic_rows)\n            statistic_table.sort_values(by=[\"N_Hotspot_communities\", \"N_Hotspots\"], ascending=[False, False],\n                                        inplace=True)\n            statistic_table.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                \"protein_group_accumulated_statistics.tsv\"), sep=\"\\t\", index=False)\n            return statistic_table\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to calculate protein group statistics.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.__init__","title":"<code>__init__(hotspots, annotation, parameters)</code>","text":"<p>Hotspots class constructor.</p> <p>Parameters:</p> <ul> <li> <code>hotspots</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Hotspot objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of hotspots.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, hotspots: pd.Series, annotation: pd.DataFrame, parameters: ilund4u.manager.Parameters):\n    \"\"\"Hotspots class constructor.\n\n    Arguments:\n        hotspots (pd.Series): Series (list) of Hotspot objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of hotspots.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.hotspots = hotspots\n    self.annotation = annotation\n    self.communities = dict()\n    self.prms = parameters\n    self.__id_to_ind = {iid: idx for idx, iid in enumerate(self.annotation.index)}\n    self.island_rep_proteins_fasta = os.path.join(parameters.args[\"output_dir\"], \"island_rep_proteins.fa\")\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.build_hotspot_network","title":"<code>build_hotspot_network()</code>","text":"<p>Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.</p> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_hotspot_network(self):\n    \"\"\"Build hotspot network and merge similar hotspots from different proteome communities into hotspot communities.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Hotspot network construction...\", file=sys.stdout)\n        hotspot_signature_sizes = pd.Series()\n        hotspot_proteome_community = dict()\n        flanked_stat = dict()\n        signature_cluster_to_hotspot = collections.defaultdict(collections.deque)\n        for hid, hotspot in enumerate(self.hotspots):\n            flanked_stat[hid] = hotspot.flanked\n            hotspot_proteome_community[hid] = hotspot.proteome_community\n            hotspot_signature_sizes.at[hid] = len(hotspot.conserved_signature)\n            for cs_cluster in hotspot.conserved_signature:\n                signature_cluster_to_hotspot[cs_cluster].append(hid)\n        edges, weights = [], []\n        bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots), suffix=\"%(index)d/%(max)d\")\n        for i, hotspot_i in enumerate(self.hotspots):\n            bar.next()\n            pc_i = hotspot_i.proteome_community\n            signature_i = hotspot_i.conserved_signature\n            size_i = hotspot_signature_sizes.iat[i]\n            counts_i = collections.defaultdict(int)\n            for sc in signature_i:\n                js = signature_cluster_to_hotspot[sc]\n                for j in js.copy():\n                    if i &lt; j:\n                        if pc_i != hotspot_proteome_community[j]:  # to think about it\n                            counts_i[j] += 1\n                    else:\n                        js.popleft()\n            weights_i = pd.Series(counts_i)\n            connected_n_sizes = hotspot_signature_sizes.iloc[weights_i.index]\n            norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                      index=weights_i.index)\n            weights_i = weights_i.mul(norm_factor_i)\n            weights_i = weights_i[weights_i &gt;= self.prms.args[\"hotspot_similarity_cutoff\"]]\n            for j, w in weights_i.items():\n                if flanked_stat[i] == flanked_stat[j]:\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n        bar.finish()\n        print(\"\u25cb Hotspot network partitioning using the Leiden algorithm...\")\n        graph = igraph.Graph(len(hotspot_signature_sizes.index), edges, directed=False)\n        graph.vs[\"index\"] = hotspot_signature_sizes.index.to_list()\n        graph.vs[\"hotspot_id\"] = [h.hotspot_id for h in self.hotspots]\n        graph.vs[\"flanked\"] = [h.flanked for h in self.hotspots]\n        graph.es[\"weight\"] = weights\n        graph.save(os.path.join(self.prms.args[\"output_dir\"], f\"hotspot_network.gml\"))\n        partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                    resolution_parameter=self.prms.args[\n                                                        \"leiden_resolution_parameter_h\"],\n                                                    weights=\"weight\", n_iterations=-1)\n        graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n        hotspot_communities_annot_rows = []\n        communities_sizes = []\n        self.annotation[\"hotspot_community\"] = pd.Series(dtype='Int64')\n        for community_index, community in enumerate(partition_leiden):\n            community_size = len(community)\n            subgraph = graph.subgraph(community)\n            hotspots = subgraph.vs[\"hotspot_id\"]\n            n_flanked = sum(subgraph.vs[\"flanked\"])\n            self.communities[community_index] = hotspots\n            self.annotation.loc[hotspots, \"hotspot_community\"] = community_index\n            if community_size &gt; 1:\n                communities_sizes.append(community_size)\n                subgraph_edges = subgraph.get_edgelist()\n                num_of_edges = len(subgraph_edges)\n                num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                weights = subgraph.es[\"weight\"]\n                avg_weight = round(np.mean(weights), 3)\n                max_identity = max(weights)\n            else:\n                num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n            hotspot_communities_annot_rows.append([community_index, community_size, avg_weight, n_flanked,\n                                                   max_identity, num_of_edges_fr, \";\".join(hotspots)])\n        communities_annot = pd.DataFrame(hotspot_communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\",\n                                                                                  \"n_flanked\", \"max_weight\",\n                                                                                  \"fr_edges\", \"hotspots\"])\n        communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"hotspot_communities.tsv\")),\n                                 sep=\"\\t\", index=False)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {sum(communities_sizes)} hotspots were merged to {len(communities_sizes)} not singleton \"\n                  f\"communities\")\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to build hotspot network.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.calculate_hotspot_and_island_statistics","title":"<code>calculate_hotspot_and_island_statistics(proteomes)</code>","text":"<p>Calculate hotspot statistics based using hmmscan results and save annotation tables.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: hotspot community annotation table.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_hotspot_and_island_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n    \"\"\"Calculate hotspot statistics based using hmmscan results and save annotation tables.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        pd.DataFrame: hotspot community annotation table.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Hotspot and island statistics calculation...\", file=sys.stdout)\n        hotspot_community_annot_rows = []\n        r_types = [\"cargo\", \"flanking\"]\n        # Create new columns\n        for r_type in r_types:\n            self.annotation[f\"N_{r_type}_groups\"] = pd.Series(dtype='Int64')\n            for db_name in self.prms.args[\"databases_classes\"]:\n                self.annotation[f\"N_{db_name}_{r_type}_groups\"] = pd.Series(dtype='Int64')\n        self.annotation[\"conserved_signature\"] = pd.Series(dtype=\"str\")\n        # Get stat\n        for h_com, hotspot_ids in self.communities.items():\n            h_com_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n            hotspot_com_groups = dict(cargo=set(), flanking=set())\n            hotspots = self.hotspots.loc[hotspot_ids].to_list()\n            n_islands, n_flanked = 0, 0\n            for hotspot in hotspots:\n                n_islands += hotspot.size\n                n_flanked += hotspot.flanked\n                hotspot_groups = hotspot.get_hotspot_groups(proteomes)\n                db_stat = hotspot.calculate_database_hits_stats(proteomes, self.prms)\n                self.annotation.at[hotspot.hotspot_id, \"conserved_signature\"] = \";\".join(\n                    hotspot.conserved_signature)\n                for r_type in r_types:\n                    hotspot_com_groups[r_type].update(hotspot_groups[r_type])\n                    self.annotation.at[hotspot.hotspot_id, f\"N_{r_type}_groups\"] = len(hotspot_groups[r_type])\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    for r_type in r_types:\n                        h_com_stat[db_name][r_type].update(db_stat[db_name][r_type])\n                        self.annotation.at[hotspot.hotspot_id, f\"N_{db_name}_{r_type}_groups\"] = \\\n                            len(set(db_stat[db_name][r_type].values()))\n            hc_annot_row = dict(com_id=h_com, community_size=len(hotspot_ids), N_flanked=n_flanked,\n                                N_islands=n_islands, hotspots=\",\".join(hotspot_ids),\n                                pdf_filename=f\"{'_'.join(hotspot_ids)}.pdf\")\n            for r_type in r_types:\n                hc_annot_row[f\"N_{r_type}_groups\"] = len(hotspot_com_groups[r_type])\n            for db_name in self.prms.args[\"databases_classes\"]:\n                for r_type in r_types:\n                    hc_annot_row[f\"N_{db_name}_{r_type}_groups\"] = len(set(h_com_stat[db_name][r_type].values()))\n            hotspot_community_annot_rows.append(hc_annot_row)\n\n        hotspot_community_annot = pd.DataFrame(hotspot_community_annot_rows)\n        for db_name in self.prms.args[\"databases_classes\"]:\n            self.annotation[f\"{db_name}_cargo_normalised\"] = \\\n                self.annotation.apply(lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"],\n                                                        4), axis=1)\n            hotspot_community_annot[f\"{db_name}_cargo_normalised\"] = \\\n                hotspot_community_annot.apply(\n                    lambda row: round(row[f\"N_{db_name}_cargo_groups\"] / row[f\"N_cargo_groups\"], 4), axis=1)\n        self.annotation.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"hotspot_annotation.tsv\"), sep=\"\\t\",\n                               index_label=\"hotspot_id\")\n        hotspot_community_annot.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                                    \"hotspot_community_annotation.tsv\"), sep=\"\\t\", index=False)\n        # Save island annotation table\n        # Get non-hotspot island stat\n        island_annotation_table_rows = []\n        for pcom, com_proteomes in proteomes.communities.items():\n            for proteome_id in com_proteomes:\n                proteome = proteomes.proteomes.at[proteome_id]\n                for island in proteome.islands.to_list():\n                    island_annot = dict(island=island.island_id, proteome=proteome.proteome_id,\n                                        proteome_commuity=pcom, hotspot_id=island.hotspot_id,\n                                        flanked=island.flanked, island_size=island.size)\n                    island.calculate_database_hits_stat(proteome.cdss)\n                    island_dbstat = island.databases_hits_stat\n                    db_names = self.prms.args[\"databases_classes\"]\n                    for db_name in db_names:\n                        r_types = [\"cargo\", \"flanking\"]\n                        for r_type in r_types:\n                            island_annot[f\"N_{db_name}_{r_type}\"] = len(island_dbstat[db_name][r_type].values())\n                    isl_groups = island.get_island_groups(proteome.cdss)\n                    isl_proteins = island.get_island_proteins(proteome.cdss)\n                    island_annot[\"island_proteins\"] = \",\".join(isl_proteins)\n                    island_annot[\"island_protein_groups\"] = \",\".join(isl_groups)\n                    island_annotation_table_rows.append(island_annot)\n        island_annotation_table = pd.DataFrame(island_annotation_table_rows)\n        island_annotation_table.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"island_annotation.tsv\"),\n                                       sep=\"\\t\", index=False)\n        return hotspot_community_annot\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to calculate hotspot and hotspot community statistics based \"\n                                           \"on hmmscan results\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.db_init","title":"<code>db_init(db_path, proteomes, parameters)</code>  <code>classmethod</code>","text":"<p>Class method to load a Proteomes object from a database.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>path to the database.</p> </li> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code>        \u2013          <p>Hotspots object.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>@classmethod\ndef db_init(cls, db_path: str, proteomes: Proteomes, parameters: ilund4u.manager.Parameters):\n    \"\"\"Class method to load a Proteomes object from a database.\n\n    Arguments:\n        db_path (str): path to the database.\n        proteomes (Proteomes): Proteomes object.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        cls: Hotspots object.\n\n    \"\"\"\n    try:\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading hotspot objects...\", file=sys.stdout)\n        island_annotation = pd.read_table(os.path.join(db_path, \"hotspot.ind.island.annotations.tsv\"),\n                                          sep=\"\\t\", low_memory=False).set_index(\"island\")\n        with open(os.path.join(db_path, \"hotspot.ind.attributes.json\"), \"r\") as json_file:\n            hotspot_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(hotspot_ind_attributes), suffix='%(index)d/%(max)d')\n        hotspot_list = []\n        for hotspot_dict in hotspot_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            hotspot_dict[\"island_annotation\"] = island_annotation[\n                island_annotation[\"hotspot_id\"] == hotspot_dict[\"hotspot_id\"]].copy()\n            hotspot_proteomes = proteomes.proteomes.loc[\n                proteomes.communities[hotspot_dict[\"proteome_community\"]]].to_list()\n            islands_list = [island for proteome in hotspot_proteomes for island in proteome.islands.to_list()]\n            islands_series = pd.Series(islands_list, index=[island.island_id for island in islands_list])\n            hotspot_dict[\"islands\"] = islands_series.loc[hotspot_dict[\"island_annotation\"].index].to_list()\n            hotspot_list.append(Hotspot(**hotspot_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        hotspots = pd.Series(hotspot_list, index=[hotspot.hotspot_id for hotspot in hotspot_list])\n        annotation = pd.read_table(os.path.join(db_path, \"hotspots.annotations.tsv\"),\n                                   sep=\"\\t\", dtype={\"community\": \"Int32\"}).set_index(\"hotspot_id\")\n        cls_obj = cls(hotspots, annotation, parameters)\n        with open(os.path.join(db_path, \"hotspots.attributes.json\"), \"r\") as json_file:\n            attributes = json.load(json_file)\n        cls_obj.communities = {int(k): v for k, v in attributes[\"communities\"].items()}\n        cls_obj.island_rep_proteins_fasta = os.path.join(db_path, attributes[\"island_rep_proteins_fasta\"])\n        return cls_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to read hotspots from the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.get_each_protein_group_statistics","title":"<code>get_each_protein_group_statistics(proteomes)</code>","text":"<p>Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots     where it's encoded.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: Protein group statistics table</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_each_protein_group_statistics(self, proteomes: Proteomes) -&gt; pd.DataFrame:\n    \"\"\"Calculate statistics for each protein group that includes its \"jumping\" properties and types of hotspots\n        where it's encoded.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        pd.DataFrame: Protein group statistics table\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Protein group statistics calculation...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.hotspots.index), suffix='%(index)d/%(max)d')\n        protein_group_statistics_dict = collections.defaultdict(\n            lambda: {\"Hotspot_communities\": set(), \"Hotspots\": set(), \"Hotpot_islands\": set(),\n                     \"Non_hotspot_islands\": set(), \"Proteome_communities\": set(), \"Flanked_hotspot_islands\": set(),\n                     \"Flanked_non_hotspot_islands\": set(), \"Counts\": 0, \"db\": \"None\", \"db_hit\": \"None\", \"Name\": \"\",\n                     \"RepLength\": 0})\n        # Hotspot stat\n        for h_com, hotspots_ids in self.communities.items():\n            hotspots = self.hotspots.loc[hotspots_ids].to_list()\n            for hotspot in hotspots:\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                for island in hotspot.islands:\n                    proteome = proteomes.proteomes.at[island.proteome]\n                    island_proteins = island.get_island_proteins(proteome.cdss)\n                    island_protein_groups = island.get_island_groups(proteome.cdss)\n                    for isp, ispg in zip(island_proteins, island_protein_groups):\n                        if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                            cds_obj = proteome.cdss.at[isp]\n                            protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                            protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                            if cds_obj.hmmscan_results:\n                                protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\"target\"]\n                                protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                    protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                        \"db_name\"]\n                        protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                        protein_group_statistics_dict[ispg][\"Hotspot_communities\"].add(h_com)\n                        protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(hotspot.proteome_community)\n                        protein_group_statistics_dict[ispg][\"Hotspots\"].add(hotspot.hotspot_id)\n                        protein_group_statistics_dict[ispg][\"Hotpot_islands\"].add(island.island_id)\n                        if island.flanked:\n                            protein_group_statistics_dict[ispg][\"Flanked_hotspot_islands\"].add(island.island_id)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        # Other accessory genes\n        for proteome_com, com_proteomes in proteomes.communities.items():\n            for proteome_id in com_proteomes:\n                proteome = proteomes.proteomes.at[proteome_id]\n                for island in proteome.islands.to_list():\n                    if island.hotspot_id == \"-\":\n                        island_proteins = island.get_island_proteins(proteome.cdss)\n                        island_protein_groups = island.get_island_groups(proteome.cdss)\n                        for isp, ispg in zip(island_proteins, island_protein_groups):\n                            if protein_group_statistics_dict[ispg][\"Counts\"] == 0:\n                                cds_obj = proteome.cdss.at[isp]\n                                protein_group_statistics_dict[ispg][\"Name\"] = cds_obj.name\n                                protein_group_statistics_dict[ispg][\"RepLength\"] = cds_obj.length\n                                if cds_obj.hmmscan_results:\n                                    protein_group_statistics_dict[ispg][\"db_hit\"] = cds_obj.hmmscan_results[\n                                        \"target\"]\n                                    protein_group_statistics_dict[ispg][\"db\"] = cds_obj.hmmscan_results[\"db\"]\n                                    if \"db_name\" in cds_obj.hmmscan_results.keys():\n                                        protein_group_statistics_dict[ispg][\"db_name\"] = cds_obj.hmmscan_results[\n                                            \"db_name\"]\n                            protein_group_statistics_dict[ispg][\"Counts\"] += 1\n                            protein_group_statistics_dict[ispg][\"Proteome_communities\"].add(proteome_com)\n                            protein_group_statistics_dict[ispg][\"Non_hotspot_islands\"].add(island.island_id)\n                            if island.flanked:\n                                protein_group_statistics_dict[ispg][\"Flanked_non_hotspot_islands\"].add(\n                                    island.island_id)\n\n        statistic_rows = []\n        for pg, pg_dict in protein_group_statistics_dict.items():\n            row_dict = dict(representative_protein=pg, db=pg_dict[\"db\"], pb_hit=pg_dict[\"db_hit\"],\n                            name=pg_dict[\"Name\"], counts=pg_dict[\"Counts\"], length=pg_dict[\"RepLength\"])\n            for k, v in pg_dict.items():\n                if isinstance(v, set):\n                    row_dict[f\"N_{k}\"] = len(v)\n            if pg_dict[\"Hotspots\"]:\n                for db_name in self.prms.args[\"databases_classes\"]:\n                    dbsn_norm_values = []\n                    for hotspot in pg_dict[\"Hotspots\"]:\n                        dbsn_norm_value = self.annotation.at[hotspot, f\"{db_name}_cargo_normalised\"]\n                        if pg_dict[\"db\"] == db_name:\n                            dbsn_norm_value -= 1 / self.annotation.at[hotspot, f\"N_cargo_groups\"]\n                        dbsn_norm_values.append(dbsn_norm_value)\n                    row_dict[f\"{db_name}_avg_cargo_fraction\"] = round(np.mean(dbsn_norm_values), 4)\n                    row_dict[f\"{db_name}_max_cargo_fraction\"] = round(max(dbsn_norm_values), 4)\n                row_dict[\"hotspots\"] = \",\".join(pg_dict[\"Hotspots\"])\n            else:\n                row_dict[\"hotspots\"] = \"Non\"\n\n            statistic_rows.append(row_dict)\n        statistic_table = pd.DataFrame(statistic_rows)\n        statistic_table.sort_values(by=[\"N_Hotspot_communities\", \"N_Hotspots\"], ascending=[False, False],\n                                    inplace=True)\n        statistic_table.to_csv(os.path.join(self.prms.args[\"output_dir\"],\n                                            \"protein_group_accumulated_statistics.tsv\"), sep=\"\\t\", index=False)\n        return statistic_table\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to calculate protein group statistics.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.pyhmmer_annotation","title":"<code>pyhmmer_annotation(proteomes)</code>","text":"<p>Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def pyhmmer_annotation(self, proteomes: Proteomes) -&gt; None:\n    \"\"\"Run pyhhmmer hmmscan against a set of databases for additional annotation of hotspot proteins.\n\n    Arguments:\n        proteomes (Proteomes): Proteomes object.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Preparing data for additional island protein annotation with pyhmmer hmmscan...\",\n                  file=sys.stdout)\n        hotspots_repr_proteins = set()\n        for hotspot in self.hotspots.to_list():\n            for island in hotspot.islands:\n                proteome = proteomes.proteomes.at[island.proteome]\n                isl_groups = island.get_locus_groups(proteome.cdss)\n                hotspots_repr_proteins.update(isl_groups)\n        initial_fasta_file = Bio.SeqIO.index(proteomes.proteins_fasta_file, \"fasta\")\n        with open(self.island_rep_proteins_fasta, \"wb\") as out_handle:\n            for acc in hotspots_repr_proteins:\n                try:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n                except:\n                    pass\n        alignment_table = ilund4u.methods.run_pyhmmer(self.island_rep_proteins_fasta, len(hotspots_repr_proteins),\n                                                      self.prms)\n        if not alignment_table.empty:\n            found_hits_for = alignment_table.index.to_list()\n            for proteome in proteomes.proteomes.to_list():\n                proteome_cdss = proteome.cdss.to_list()\n                proteome_cdss_with_hits = [cds.cds_id for cds in proteome_cdss if cds.group in found_hits_for]\n                if proteome_cdss_with_hits:\n                    cdss_with_hits = proteome.cdss.loc[proteome_cdss_with_hits].to_list()\n                    for cds in cdss_with_hits:\n                        alignment_table_row = alignment_table.loc[cds.group]\n                        cds.hmmscan_results = dict(db=alignment_table_row[\"db_class\"],\n                                                   db_name=alignment_table_row[\"target_db\"],\n                                                   target=alignment_table_row[\"target\"],\n                                                   evalue=alignment_table_row[\"hit_evalue\"])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run pyhmmer hmmscan annotation.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Hotspots.save_as_db","title":"<code>save_as_db(db_folder)</code>","text":"<p>Save Hotspots to the iLnd4u database.</p> <p>Parameters:</p> <ul> <li> <code>db_folder</code>             (<code>str</code>)         \u2013          <p>Database folder path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def save_as_db(self, db_folder: str) -&gt; None:\n    \"\"\"Save Hotspots to the iLnd4u database.\n\n    Arguments:\n        db_folder (str): Database folder path.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        attributes_to_ignore = [\"hotspots\", \"annotation\", \"communities_annot\", \"prms\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"island_rep_proteins_fasta\"] = os.path.basename(attributes[\"island_rep_proteins_fasta\"])\n        with open(os.path.join(db_folder, \"hotspots.attributes.json\"), 'w') as json_file:\n            json.dump(attributes, json_file)\n        self.annotation.to_csv(os.path.join(db_folder, \"hotspots.annotations.tsv\"), sep=\"\\t\",\n                               index_label=\"hotspot_id\")\n        island_annotation_table = pd.DataFrame()\n        hotspot_db_ind = []\n        for hotspot in self.hotspots.to_list():\n            hotspot_db_ind.append(hotspot.get_hotspot_db_row())\n            h_island_annot = hotspot.island_annotation.copy()\n            h_island_annot[\"hotspot_id\"] = hotspot.hotspot_id\n            island_annotation_table = pd.concat([island_annotation_table, h_island_annot])\n\n        with open(os.path.join(db_folder, \"hotspot.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(hotspot_db_ind, json_file)\n\n        os.system(f\"cp {os.path.join(self.prms.args['output_dir'], 'protein_group_accumulated_statistics.tsv')} \"\n                  f\"{db_folder}\")\n\n        island_annotation_table.to_csv(os.path.join(db_folder, \"hotspot.ind.island.annotations.tsv\"), sep=\"\\t\",\n                                       index_label=\"island\")\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to write hotspots to the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island","title":"<code>Island</code>","text":"<p>Island object represents an annotated island defined as a region with a set of non-conserved proteins.</p> <p>Attributes:</p> <ul> <li> <code>island_id</code>             (<code>str</code>)         \u2013          <p>Island identifier.</p> </li> <li> <code>proteome</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where island is annotated.</p> </li> <li> <code>circular_proteome</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then     first and last genes are considered as neighbours.</p> </li> <li> <code>center</code>             (<code>int</code>)         \u2013          <p>CDS index of the island center.</p> </li> <li> <code>indexes</code>             (<code>list</code>)         \u2013          <p>CDS indexes of the island.</p> </li> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>Length of the island (number of CDSs).</p> </li> <li> <code>var_indexes</code>             (<code>list</code>)         \u2013          <p>Indexes of CDS which g_class is \"variable\".</p> </li> <li> <code>hotspot_id</code>             (<code>str</code>)         \u2013          <p>Hotspot id if Island was attributed to one of them or \"-\" value if not.</p> </li> <li> <code>left_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the left.</p> </li> <li> <code>right_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the right.</p> </li> <li> <code>flanked</code>             (<code>int</code>)         \u2013          <p>Whether island is flanked by conserved genes or not [1 or 0].</p> </li> <li> <code>databases_hits_stat</code>             (<code>dict</code>)         \u2013          <p>Statistics from hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Island:\n    \"\"\"Island object represents an annotated island defined as a region with a set of non-conserved proteins.\n\n    Attributes:\n        island_id (str): Island identifier.\n        proteome (str): Proteome identifier where island is annotated.\n        circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n                first and last genes are considered as neighbours.\n        center (int): CDS index of the island center.\n        indexes (list): CDS indexes of the island.\n        size (int): Length of the island (number of CDSs).\n        var_indexes (list): Indexes of CDS which g_class is \"variable\".\n        hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n        left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n        right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n        flanked (int): Whether island is flanked by conserved genes or not [1 or 0].\n        databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n    \"\"\"\n\n    def __init__(self, island_id: str, proteome: str,\n                 circular_proteome: int, center: int, indexes: list, var_indexes: list,\n                 left_cons_neighbours: list, right_cons_neighbours: list,\n                 hotspot_id=\"-\", databases_hits_stat: typing.Union[None, dict] = None):\n        \"\"\"Island class constructor.\n\n        Arguments:\n            island_id (str): Island identifier.\n            proteome (str): Proteome identifier where island is annotated.\n            circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n                first and last genes are considered as neighbours.\n            center (int): CDS index of the island center.\n            indexes (list): CDS indexes of the island.\n            var_indexes (list): Indexes of CDS which g_class is \"variable\".\n            left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n            right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n            hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n            databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n        \"\"\"\n        self.island_id = island_id\n        self.proteome = proteome\n        self.circular_proteome = circular_proteome\n        self.hotspot_id = hotspot_id\n        self.center = center\n        self.indexes = indexes\n        self.size = len(indexes)\n        self.var_indexes = var_indexes\n        self.left_cons_neighbours = left_cons_neighbours\n        self.right_cons_neighbours = right_cons_neighbours\n        if (not left_cons_neighbours or not right_cons_neighbours) and not circular_proteome:\n            self.flanked = 0\n        else:\n            self.flanked = 1\n        if databases_hits_stat is None:\n            databases_hits_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n        self.databases_hits_stat = databases_hits_stat\n\n    def get_island_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"size\", \"flanked\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        return attributes\n\n    def get_cons_neighbours_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of conserved neighbour CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of conserved neighbours.\n\n        \"\"\"\n        all_cons_neighbours = self.left_cons_neighbours + self.right_cons_neighbours\n        cons_neighbours_groups = cdss.iloc[all_cons_neighbours].apply(lambda cds: cds.group).to_list()\n        return cons_neighbours_groups\n\n    def get_locus_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of island CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.group).to_list()\n        return locus_groups\n\n    def get_island_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of island CDSs.\n\n        \"\"\"\n        island_groups = cdss.iloc[self.indexes].apply(lambda cds: cds.group).to_list()\n        return island_groups\n\n    def get_flanking_groups(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get homology group attribute of island flanking CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Groups of flanking CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        flanking_indexes = [ind for ind in locus_cdss_indexes if ind not in self.indexes]\n        flanking_groups = cdss.iloc[flanking_indexes].apply(lambda cds: cds.group).to_list()\n        return flanking_groups\n\n    def get_locus_proteins(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get ids of locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: ids of island locus CDSs.\n\n        \"\"\"\n        locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n        locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.cds_id).to_list()\n        return locus_groups\n\n    def get_island_proteins(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get ids of island CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Ids of island CDSs.\n\n        \"\"\"\n        island_proteins = cdss.iloc[self.indexes].apply(lambda cds: cds.cds_id).to_list()\n        return island_proteins\n\n    def get_all_locus_indexes(self, cdss: pd.Series) -&gt; list:\n        \"\"\"Get indexes of island locus CDSs.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            list: Indexes of island locus CDSs.\n\n        \"\"\"\n        if self.left_cons_neighbours:\n            island_left_border_cds_ind = self.left_cons_neighbours[0]\n        else:\n            island_left_border_cds_ind = self.indexes[0]\n        if self.right_cons_neighbours:\n            island_right_border_cds_ind = self.right_cons_neighbours[-1]\n        else:\n            island_right_border_cds_ind = self.indexes[-1]\n        if island_left_border_cds_ind &lt; island_right_border_cds_ind:\n            island_cdss_indexes = [i for i in range(island_left_border_cds_ind, island_right_border_cds_ind + 1)]\n        else:\n            island_cdss_indexes = [i for i in range(island_left_border_cds_ind, cdss.size)] + \\\n                                  [i for i in range(0, island_right_border_cds_ind + 1)]\n        return island_cdss_indexes\n\n    def calculate_database_hits_stat(self, cdss: pd.Series) -&gt; None:\n        \"\"\"Update statistics of hmmscan search results.\n\n        Arguments:\n            cdss (pd.Series): Series of annotated proteome CDSs.\n\n        Returns:\n            None\n\n        \"\"\"\n        all_locus_indexes = self.get_all_locus_indexes(cdss)\n        for ind in all_locus_indexes:\n            lind_cds = cdss.iat[ind]\n            lind_cds_hmmscan_res = lind_cds.hmmscan_results\n            if lind_cds_hmmscan_res:\n                if ind not in self.indexes:\n                    self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"flanking\"][lind_cds.group] = \\\n                        lind_cds_hmmscan_res[\"target\"]\n                else:\n                    self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"cargo\"][lind_cds.group] = \\\n                        lind_cds_hmmscan_res[\"target\"]\n        return None\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.__init__","title":"<code>__init__(island_id, proteome, circular_proteome, center, indexes, var_indexes, left_cons_neighbours, right_cons_neighbours, hotspot_id='-', databases_hits_stat=None)</code>","text":"<p>Island class constructor.</p> <p>Parameters:</p> <ul> <li> <code>island_id</code>             (<code>str</code>)         \u2013          <p>Island identifier.</p> </li> <li> <code>proteome</code>             (<code>str</code>)         \u2013          <p>Proteome identifier where island is annotated.</p> </li> <li> <code>circular_proteome</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then first and last genes are considered as neighbours.</p> </li> <li> <code>center</code>             (<code>int</code>)         \u2013          <p>CDS index of the island center.</p> </li> <li> <code>indexes</code>             (<code>list</code>)         \u2013          <p>CDS indexes of the island.</p> </li> <li> <code>var_indexes</code>             (<code>list</code>)         \u2013          <p>Indexes of CDS which g_class is \"variable\".</p> </li> <li> <code>left_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the left.</p> </li> <li> <code>right_cons_neighbours</code>             (<code>list</code>)         \u2013          <p>Indexes of conserved neighbours on the right.</p> </li> <li> <code>hotspot_id</code>             (<code>str</code>, default:                 <code>'-'</code> )         \u2013          <p>Hotspot id if Island was attributed to one of them or \"-\" value if not.</p> </li> <li> <code>databases_hits_stat</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Statistics from hmmscan annotation.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, island_id: str, proteome: str,\n             circular_proteome: int, center: int, indexes: list, var_indexes: list,\n             left_cons_neighbours: list, right_cons_neighbours: list,\n             hotspot_id=\"-\", databases_hits_stat: typing.Union[None, dict] = None):\n    \"\"\"Island class constructor.\n\n    Arguments:\n        island_id (str): Island identifier.\n        proteome (str): Proteome identifier where island is annotated.\n        circular_proteome (int): [1 or 0] int value whether locus is circular or not. If genome is circular then\n            first and last genes are considered as neighbours.\n        center (int): CDS index of the island center.\n        indexes (list): CDS indexes of the island.\n        var_indexes (list): Indexes of CDS which g_class is \"variable\".\n        left_cons_neighbours (list): Indexes of conserved neighbours on the left.\n        right_cons_neighbours (list): Indexes of conserved neighbours on the right.\n        hotspot_id (str): Hotspot id if Island was attributed to one of them or \"-\" value if not.\n        databases_hits_stat (dict): Statistics from hmmscan annotation.\n\n    \"\"\"\n    self.island_id = island_id\n    self.proteome = proteome\n    self.circular_proteome = circular_proteome\n    self.hotspot_id = hotspot_id\n    self.center = center\n    self.indexes = indexes\n    self.size = len(indexes)\n    self.var_indexes = var_indexes\n    self.left_cons_neighbours = left_cons_neighbours\n    self.right_cons_neighbours = right_cons_neighbours\n    if (not left_cons_neighbours or not right_cons_neighbours) and not circular_proteome:\n        self.flanked = 0\n    else:\n        self.flanked = 1\n    if databases_hits_stat is None:\n        databases_hits_stat = collections.defaultdict(lambda: collections.defaultdict(dict))\n    self.databases_hits_stat = databases_hits_stat\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.calculate_database_hits_stat","title":"<code>calculate_database_hits_stat(cdss)</code>","text":"<p>Update statistics of hmmscan search results.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def calculate_database_hits_stat(self, cdss: pd.Series) -&gt; None:\n    \"\"\"Update statistics of hmmscan search results.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        None\n\n    \"\"\"\n    all_locus_indexes = self.get_all_locus_indexes(cdss)\n    for ind in all_locus_indexes:\n        lind_cds = cdss.iat[ind]\n        lind_cds_hmmscan_res = lind_cds.hmmscan_results\n        if lind_cds_hmmscan_res:\n            if ind not in self.indexes:\n                self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"flanking\"][lind_cds.group] = \\\n                    lind_cds_hmmscan_res[\"target\"]\n            else:\n                self.databases_hits_stat[lind_cds_hmmscan_res[\"db\"]][\"cargo\"][lind_cds.group] = \\\n                    lind_cds_hmmscan_res[\"target\"]\n    return None\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_all_locus_indexes","title":"<code>get_all_locus_indexes(cdss)</code>","text":"<p>Get indexes of island locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Indexes of island locus CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_all_locus_indexes(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get indexes of island locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Indexes of island locus CDSs.\n\n    \"\"\"\n    if self.left_cons_neighbours:\n        island_left_border_cds_ind = self.left_cons_neighbours[0]\n    else:\n        island_left_border_cds_ind = self.indexes[0]\n    if self.right_cons_neighbours:\n        island_right_border_cds_ind = self.right_cons_neighbours[-1]\n    else:\n        island_right_border_cds_ind = self.indexes[-1]\n    if island_left_border_cds_ind &lt; island_right_border_cds_ind:\n        island_cdss_indexes = [i for i in range(island_left_border_cds_ind, island_right_border_cds_ind + 1)]\n    else:\n        island_cdss_indexes = [i for i in range(island_left_border_cds_ind, cdss.size)] + \\\n                              [i for i in range(0, island_right_border_cds_ind + 1)]\n    return island_cdss_indexes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_cons_neighbours_groups","title":"<code>get_cons_neighbours_groups(cdss)</code>","text":"<p>Get homology group attribute of conserved neighbour CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of conserved neighbours.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_cons_neighbours_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of conserved neighbour CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of conserved neighbours.\n\n    \"\"\"\n    all_cons_neighbours = self.left_cons_neighbours + self.right_cons_neighbours\n    cons_neighbours_groups = cdss.iloc[all_cons_neighbours].apply(lambda cds: cds.group).to_list()\n    return cons_neighbours_groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_flanking_groups","title":"<code>get_flanking_groups(cdss)</code>","text":"<p>Get homology group attribute of island flanking CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of flanking CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_flanking_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island flanking CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of flanking CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    flanking_indexes = [ind for ind in locus_cdss_indexes if ind not in self.indexes]\n    flanking_groups = cdss.iloc[flanking_indexes].apply(lambda cds: cds.group).to_list()\n    return flanking_groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_island_db_row","title":"<code>get_island_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"size\", \"flanked\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    return attributes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_island_groups","title":"<code>get_island_groups(cdss)</code>","text":"<p>Get homology group attribute of island CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of island CDSs.\n\n    \"\"\"\n    island_groups = cdss.iloc[self.indexes].apply(lambda cds: cds.group).to_list()\n    return island_groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_island_proteins","title":"<code>get_island_proteins(cdss)</code>","text":"<p>Get ids of island CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Ids of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_island_proteins(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get ids of island CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Ids of island CDSs.\n\n    \"\"\"\n    island_proteins = cdss.iloc[self.indexes].apply(lambda cds: cds.cds_id).to_list()\n    return island_proteins\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_locus_groups","title":"<code>get_locus_groups(cdss)</code>","text":"<p>Get homology group attribute of island locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>Groups of island CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_locus_groups(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get homology group attribute of island locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: Groups of island CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.group).to_list()\n    return locus_groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Island.get_locus_proteins","title":"<code>get_locus_proteins(cdss)</code>","text":"<p>Get ids of locus CDSs.</p> <p>Parameters:</p> <ul> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>Series of annotated proteome CDSs.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list</code> (            <code>list</code> )        \u2013          <p>ids of island locus CDSs.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_locus_proteins(self, cdss: pd.Series) -&gt; list:\n    \"\"\"Get ids of locus CDSs.\n\n    Arguments:\n        cdss (pd.Series): Series of annotated proteome CDSs.\n\n    Returns:\n        list: ids of island locus CDSs.\n\n    \"\"\"\n    locus_cdss_indexes = self.get_all_locus_indexes(cdss)\n    locus_groups = cdss.iloc[locus_cdss_indexes].apply(lambda cds: cds.cds_id).to_list()\n    return locus_groups\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteome","title":"<code>Proteome</code>","text":"<p>Proteome object represents a particular annotated proteome and its properties</p> <p>Attributes:</p> <ul> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier.</p> </li> <li> <code>gff_file</code>             (<code>str</code>)         \u2013          <p>Path to the corresponding gff file.</p> </li> <li> <code>circular</code>             (<code>int</code>)         \u2013          <p>[1 or 0] int value whether locus is circular or not. If genome is circular then first and last genes are considered as neighbours.</p> </li> <li> <code>cdss</code>             (<code>Series</code>)         \u2013          <p>List of annotated proteins.</p> </li> <li> <code>islands</code>             (<code>Series</code>)         \u2013          <p>Series of annotated islands.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Proteome:\n    \"\"\"Proteome object represents a particular annotated proteome and its properties\n\n    Attributes:\n        proteome_id (str): Proteome identifier.\n        gff_file (str): Path to the corresponding gff file.\n        circular (int): [1 or 0] int value whether locus is circular or not. If genome is circular then first and last\n            genes are considered as neighbours.\n        cdss (pd.Series): List of annotated proteins.\n        islands (pd.Series): Series of annotated islands.\n\n    \"\"\"\n\n    def __init__(self, proteome_id: str, gff_file: str, circular: int = 1, cdss: typing.Union[None, pd.Series] = None,\n                 islands: typing.Union[None, pd.Series] = None):\n        \"\"\"Proteome class constructor.\n\n        Arguments:\n            proteome_id (str): Proteome identifier.\n            gff_file (str): Path to the corresponding gff file.\n            circular (int): [1 or 0] int value whether locus is circular or not.\n            cdss (pd.Series): List of annotated proteins.\n            islands (pd.Series): Series of annotated islands.\n\n        \"\"\"\n        self.proteome_id = proteome_id\n        self.gff_file = gff_file\n        self.cdss = cdss\n        self.circular = circular\n        self.islands = islands\n\n    def get_proteome_db_row(self) -&gt; dict:\n        \"\"\"Database building method for saving object's attributes.\n\n        Returns:\n            dict: modified object's attributes.\n\n        \"\"\"\n        attributes_to_ignore = [\"prms\", \"cdss\", \"islands\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"gff_file\"] = os.path.basename(attributes[\"gff_file\"])\n        attributes[\"cdss\"] = self.cdss.apply(lambda cds: cds.cds_id).to_list()\n        attributes[\"islands\"] = self.islands.apply(lambda island: island.island_id).to_list()\n        return attributes\n\n    def annotate_variable_islands(self, prms: ilund4u.manager.Parameters) -&gt; None:\n        \"\"\"Annotate proteome variable islands defined as a region with a set of non-conserved proteins.\n\n        Arguments:\n            prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            proteome_size = len(self.cdss.index)\n            proteome_cds_classes = self.cdss.apply(lambda cds: cds.g_class).to_list()\n            proteome_not_conserved_indexes = [ind for ind in range(proteome_size) if\n                                              proteome_cds_classes[ind] != \"conserved\"]\n            proteome_conserved_indexes = [ind for ind in range(proteome_size) if\n                                          proteome_cds_classes[ind] == \"conserved\"]\n            n_conserved_proteins = len(proteome_conserved_indexes)\n            var_regions, cur_region = [], []\n            self.islands = pd.Series()\n            for pnci in proteome_not_conserved_indexes:\n                if cur_region:\n                    if pnci == cur_region[-1] + 1:\n                        cur_region.append(pnci)\n                    else:\n                        var_regions.append(cur_region)\n                        cur_region = [pnci]\n                else:\n                    cur_region.append(pnci)\n                if pnci == proteome_not_conserved_indexes[-1] and cur_region:\n                    var_regions.append(cur_region)\n            if not var_regions:\n                return None\n            if self.circular:\n                if var_regions[0][0] == 0 and var_regions[-1][-1] == proteome_size - 1:\n                    last_region = var_regions.pop()\n                    var_regions[0] = last_region + var_regions[0]\n            proteome_islands_l = []\n            for region in var_regions:\n                var_indexes = [ind for ind in region if proteome_cds_classes[ind] == \"variable\"]\n                if not var_indexes:\n                    continue\n                left_border, right_border = region[0], region[-1]\n                left_cons_neighbours, right_cons_neighbours = [], []\n                for dist in range(1, min(prms.args[\"neighbours_max_distance\"], n_conserved_proteins // 2) + 1):\n                    if self.circular:\n                        left_index = (left_border - dist) % proteome_size\n                        right_index = (right_border + dist) % proteome_size\n                    else:\n                        left_index = left_border - dist\n                        right_index = right_border + dist\n                    if left_index in proteome_conserved_indexes and \\\n                            len(left_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                        left_cons_neighbours.append(left_index)\n                    if right_index in proteome_conserved_indexes and \\\n                            len(right_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                        right_cons_neighbours.append(right_index)\n                left_cons_neighbours = left_cons_neighbours[::-1]\n                cons_neighbours = left_cons_neighbours + right_cons_neighbours\n                if len(set(cons_neighbours)) &lt; prms.args[\"neighbours_min_size\"]:\n                    continue\n                island_size = len(var_indexes)\n                island_center = var_indexes[int(island_size / 2)]\n                island_id = f\"{self.proteome_id}:{island_center}\"\n                island = Island(island_id=island_id, proteome=self.proteome_id, circular_proteome=self.circular,\n                                center=island_center, indexes=region, var_indexes=var_indexes,\n                                left_cons_neighbours=left_cons_neighbours, right_cons_neighbours=right_cons_neighbours)\n                proteome_islands_l.append(island)\n            self.islands = pd.Series(proteome_islands_l, index=[pi.island_id for pi in proteome_islands_l])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to annotate variable islands for {self.proteome_id}\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteome.__init__","title":"<code>__init__(proteome_id, gff_file, circular=1, cdss=None, islands=None)</code>","text":"<p>Proteome class constructor.</p> <p>Parameters:</p> <ul> <li> <code>proteome_id</code>             (<code>str</code>)         \u2013          <p>Proteome identifier.</p> </li> <li> <code>gff_file</code>             (<code>str</code>)         \u2013          <p>Path to the corresponding gff file.</p> </li> <li> <code>circular</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>[1 or 0] int value whether locus is circular or not.</p> </li> <li> <code>cdss</code>             (<code>Series</code>, default:                 <code>None</code> )         \u2013          <p>List of annotated proteins.</p> </li> <li> <code>islands</code>             (<code>Series</code>, default:                 <code>None</code> )         \u2013          <p>Series of annotated islands.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, proteome_id: str, gff_file: str, circular: int = 1, cdss: typing.Union[None, pd.Series] = None,\n             islands: typing.Union[None, pd.Series] = None):\n    \"\"\"Proteome class constructor.\n\n    Arguments:\n        proteome_id (str): Proteome identifier.\n        gff_file (str): Path to the corresponding gff file.\n        circular (int): [1 or 0] int value whether locus is circular or not.\n        cdss (pd.Series): List of annotated proteins.\n        islands (pd.Series): Series of annotated islands.\n\n    \"\"\"\n    self.proteome_id = proteome_id\n    self.gff_file = gff_file\n    self.cdss = cdss\n    self.circular = circular\n    self.islands = islands\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteome.annotate_variable_islands","title":"<code>annotate_variable_islands(prms)</code>","text":"<p>Annotate proteome variable islands defined as a region with a set of non-conserved proteins.</p> <p>Parameters:</p> <ul> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def annotate_variable_islands(self, prms: ilund4u.manager.Parameters) -&gt; None:\n    \"\"\"Annotate proteome variable islands defined as a region with a set of non-conserved proteins.\n\n    Arguments:\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        proteome_size = len(self.cdss.index)\n        proteome_cds_classes = self.cdss.apply(lambda cds: cds.g_class).to_list()\n        proteome_not_conserved_indexes = [ind for ind in range(proteome_size) if\n                                          proteome_cds_classes[ind] != \"conserved\"]\n        proteome_conserved_indexes = [ind for ind in range(proteome_size) if\n                                      proteome_cds_classes[ind] == \"conserved\"]\n        n_conserved_proteins = len(proteome_conserved_indexes)\n        var_regions, cur_region = [], []\n        self.islands = pd.Series()\n        for pnci in proteome_not_conserved_indexes:\n            if cur_region:\n                if pnci == cur_region[-1] + 1:\n                    cur_region.append(pnci)\n                else:\n                    var_regions.append(cur_region)\n                    cur_region = [pnci]\n            else:\n                cur_region.append(pnci)\n            if pnci == proteome_not_conserved_indexes[-1] and cur_region:\n                var_regions.append(cur_region)\n        if not var_regions:\n            return None\n        if self.circular:\n            if var_regions[0][0] == 0 and var_regions[-1][-1] == proteome_size - 1:\n                last_region = var_regions.pop()\n                var_regions[0] = last_region + var_regions[0]\n        proteome_islands_l = []\n        for region in var_regions:\n            var_indexes = [ind for ind in region if proteome_cds_classes[ind] == \"variable\"]\n            if not var_indexes:\n                continue\n            left_border, right_border = region[0], region[-1]\n            left_cons_neighbours, right_cons_neighbours = [], []\n            for dist in range(1, min(prms.args[\"neighbours_max_distance\"], n_conserved_proteins // 2) + 1):\n                if self.circular:\n                    left_index = (left_border - dist) % proteome_size\n                    right_index = (right_border + dist) % proteome_size\n                else:\n                    left_index = left_border - dist\n                    right_index = right_border + dist\n                if left_index in proteome_conserved_indexes and \\\n                        len(left_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                    left_cons_neighbours.append(left_index)\n                if right_index in proteome_conserved_indexes and \\\n                        len(right_cons_neighbours) &lt; prms.args[\"neighbours_one_side_max_size\"]:\n                    right_cons_neighbours.append(right_index)\n            left_cons_neighbours = left_cons_neighbours[::-1]\n            cons_neighbours = left_cons_neighbours + right_cons_neighbours\n            if len(set(cons_neighbours)) &lt; prms.args[\"neighbours_min_size\"]:\n                continue\n            island_size = len(var_indexes)\n            island_center = var_indexes[int(island_size / 2)]\n            island_id = f\"{self.proteome_id}:{island_center}\"\n            island = Island(island_id=island_id, proteome=self.proteome_id, circular_proteome=self.circular,\n                            center=island_center, indexes=region, var_indexes=var_indexes,\n                            left_cons_neighbours=left_cons_neighbours, right_cons_neighbours=right_cons_neighbours)\n            proteome_islands_l.append(island)\n        self.islands = pd.Series(proteome_islands_l, index=[pi.island_id for pi in proteome_islands_l])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to annotate variable islands for {self.proteome_id}\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteome.get_proteome_db_row","title":"<code>get_proteome_db_row()</code>","text":"<p>Database building method for saving object's attributes.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>modified object's attributes.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def get_proteome_db_row(self) -&gt; dict:\n    \"\"\"Database building method for saving object's attributes.\n\n    Returns:\n        dict: modified object's attributes.\n\n    \"\"\"\n    attributes_to_ignore = [\"prms\", \"cdss\", \"islands\"]\n    attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n    attributes[\"gff_file\"] = os.path.basename(attributes[\"gff_file\"])\n    attributes[\"cdss\"] = self.cdss.apply(lambda cds: cds.cds_id).to_list()\n    attributes[\"islands\"] = self.islands.apply(lambda island: island.island_id).to_list()\n    return attributes\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes","title":"<code>Proteomes</code>","text":"<p>Proteomes object represents a set of annotated genomes.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Series</code>)         \u2013          <p>Series (list) of Proteome objects.</p> </li> <li> <code>annotation</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table with description and statistics of proteomes.</p> </li> <li> <code>seq_to_ind</code>             (<code>dict</code>)         \u2013          <p>Dictionary with proteome id to proteome index pairs.</p> </li> <li> <code>communities</code>             (<code>dict</code>)         \u2013          <p>Dictionary representing annotated communities of proteomes (key - community_id, value - list of proteome ids)</p> </li> <li> <code>communities_annot</code>             (<code>DataFrame</code>)         \u2013          <p>Annotation table of proteome communities.</p> </li> <li> <code>proteins_fasta_file</code>             (<code>str</code>)         \u2013          <p>Path to a fasta file containing all protein sequences.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>class Proteomes:\n    \"\"\"Proteomes object represents a set of annotated genomes.\n\n    Attributes:\n        proteomes (pd.Series): Series (list) of Proteome objects.\n        annotation (pd.DataFrame): Annotation table with description and statistics of proteomes.\n        seq_to_ind (dict): Dictionary with proteome id to proteome index pairs.\n        communities (dict): Dictionary representing annotated communities of proteomes\n            (key - community_id, value - list of proteome ids)\n        communities_annot (pd.DataFrame): Annotation table of proteome communities.\n        proteins_fasta_file (str): Path to a fasta file containing all protein sequences.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, parameters: ilund4u.manager.Parameters):\n        \"\"\"Proteomes class constructor.\n\n        Arguments:\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = pd.Series()\n        self.annotation = None\n        self.__col_to_ind = None\n        self.seq_to_ind = None\n        self.communities = dict()\n        self.communities_annot = None\n        self.proteins_fasta_file = os.path.join(parameters.args[\"output_dir\"], \"all_proteins.fa\")\n        self.prms = parameters\n\n    def save_as_db(self, db_folder: str) -&gt; None:\n        \"\"\"Save Proteomes to the iLnd4u database.\n\n        Arguments:\n            db_folder (str): Database folder path.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            attributes_to_ignore = [\"proteomes\", \"annotation\", \"communities_annot\", \"prms\"]\n            attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n            attributes[\"proteins_fasta_file\"] = os.path.basename(attributes[\"proteins_fasta_file\"])\n            with open(os.path.join(db_folder, \"proteomes.attributes.json\"), 'w') as json_file:\n                json.dump(attributes, json_file)\n            self.communities_annot.to_csv(os.path.join(db_folder, \"proteomes.communities_annot.tsv\"), sep=\"\\t\",\n                                          index_label=\"id\")\n            self.annotation[\"protein_clusters\"] = self.annotation[\"protein_clusters\"].apply(lambda x: \";\".join(x))\n            self.annotation.to_csv(os.path.join(db_folder, \"proteomes.annotations.tsv\"), sep=\"\\t\",\n                                   index_label=\"proteome_id\")\n            os.mkdir(os.path.join(db_folder, \"gff\"))\n            proteome_db_ind, cdss_db_ind, islands_db_ind, cds_ids, repr_cds_ids = [], [], [], [], set()\n            for community, proteomes in self.communities.items():\n                for proteome_id in proteomes:\n                    proteome = self.proteomes.at[proteome_id]\n                    proteome_db_ind.append(proteome.get_proteome_db_row())\n                    os.system(f\"cp '{proteome.gff_file}' {os.path.join(db_folder, 'gff')}/\")\n                    for cds in proteome.cdss.to_list():\n                        cds_ids.append(cds.cds_id)\n                        cdss_db_ind.append(cds.get_cds_db_row())\n                        repr_cds_ids.add(cds.group)\n                    for island in proteome.islands.to_list():\n                        islands_db_ind.append(island.get_island_db_row())\n\n            with open(os.path.join(db_folder, \"proteome.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(proteome_db_ind, json_file)\n            with open(os.path.join(db_folder, \"cds.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(cdss_db_ind, json_file)\n            with open(os.path.join(db_folder, \"island.ind.attributes.json\"), \"w\") as json_file:\n                json.dump(islands_db_ind, json_file)\n\n            initial_fasta_file = Bio.SeqIO.index(self.proteins_fasta_file, \"fasta\")\n            with open(os.path.join(db_folder, attributes[\"proteins_fasta_file\"]), \"wb\") as out_handle:\n                for acc in cds_ids:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n\n            with open(os.path.join(db_folder, \"representative_seqs.fa\"), \"wb\") as out_handle:\n                for acc in repr_cds_ids:\n                    out_handle.write(initial_fasta_file.get_raw(acc))\n\n            mmseqs_db_folder = os.path.join(db_folder, \"mmseqs_db\")\n            if os.path.exists(mmseqs_db_folder):\n                shutil.rmtree(mmseqs_db_folder)\n            os.mkdir(mmseqs_db_folder)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\",\n                            os.path.join(db_folder, attributes[\"proteins_fasta_file\"]),\n                            os.path.join(mmseqs_db_folder, \"all_proteins\")],\n                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to write Proteomes to the database.\") from error\n\n    @classmethod\n    def db_init(cls, db_path: str, parameters: ilund4u.manager.Parameters):\n        \"\"\"Class method to load a Proteomes object from a database.\n\n        Arguments:\n            db_path (str): path to the database.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        Returns:\n            cls: Proteomes object.\n\n        \"\"\"\n        try:\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading cds objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"cds.ind.attributes.json\"), \"r\") as json_file:\n                cds_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(cds_ind_attributes), suffix='%(index)d/%(max)d')\n            cds_list = []\n            for cds_dict in cds_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                cds_list.append(CDS(**cds_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            cdss = pd.Series(cds_list, index=[cds.cds_id for cds in cds_list])\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading island objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"island.ind.attributes.json\"), \"r\") as json_file:\n                island_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(island_ind_attributes), suffix='%(index)d/%(max)d')\n            island_list = []\n            for proteome_dict in island_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                island_list.append(Island(**proteome_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            islands = pd.Series(island_list, index=[island.island_id for island in island_list])\n\n            if parameters.args[\"verbose\"]:\n                print(f\"\u25cb Loading proteome objects...\", file=sys.stdout)\n            with open(os.path.join(db_path, \"proteome.ind.attributes.json\"), \"r\") as json_file:\n                proteome_ind_attributes = json.load(json_file)\n            if parameters.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(proteome_ind_attributes), suffix='%(index)d/%(max)d')\n            proteome_list = []\n            for proteome_dict in proteome_ind_attributes:\n                if parameters.args[\"verbose\"]:\n                    bar.next()\n                proteome_dict[\"gff_file\"] = os.path.join(db_path, \"gff\", proteome_dict[\"gff_file\"])\n                proteome_dict[\"cdss\"] = cdss.loc[proteome_dict[\"cdss\"]]\n                proteome_dict[\"islands\"] = islands.loc[proteome_dict[\"islands\"]]\n                proteome_list.append(Proteome(**proteome_dict))\n            if parameters.args[\"verbose\"]:\n                bar.finish()\n            proteomes = pd.Series(proteome_list, index=[proteome.proteome_id for proteome in proteome_list])\n\n            cls_obj = cls(parameters)\n            cls_obj.proteomes = proteomes\n            with open(os.path.join(db_path, \"proteomes.attributes.json\"), \"r\") as json_file:\n                proteomes_attributes = json.load(json_file)\n            cls_obj.communities = {int(k): v for k, v in proteomes_attributes[\"communities\"].items()}\n\n            cls_obj.proteins_fasta_file = os.path.join(db_path, proteomes_attributes[\"proteins_fasta_file\"])\n\n            cls_obj.annotation = pd.read_table(os.path.join(db_path, \"proteomes.annotations.tsv\"),\n                                               sep=\"\\t\").set_index(\"proteome_id\")\n            cls_obj.__col_to_ind = {col: idx for idx, col in enumerate(cls_obj.annotation.columns)}\n            cls_obj.communities_annot = pd.read_table(os.path.join(db_path, \"proteomes.communities_annot.tsv\"),\n                                                      sep=\"\\t\").set_index(\"id\")\n            cls_obj.seq_to_ind = {sid: idx for idx, sid in enumerate(cls_obj.annotation.index)}\n\n            return cls_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to read Proteomes from the database.\") from error\n\n    def load_sequences_from_extended_gff(self, input_f: typing.Union[str, list], genome_annotation=None) -&gt; None:\n        \"\"\"Load proteomes from gff files.\n\n        Arguments:\n            input_f (str | list): List of file paths or path to a folder with gff files.\n            genome_annotation (path): Path to a table with annotation of genome circularity.\n                Format: two columns with names: id, circular; tab-separated, 1,0 values.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if isinstance(input_f, str):\n                input_folder = input_f\n                if not os.path.exists(input_folder):\n                    raise ilund4u.manager.ilund4uError(f\"Folder {input_folder} does not exist.\")\n                gff_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder)]\n            elif isinstance(input_f, list):\n                gff_files = input_f\n            else:\n                raise ilund4u.manager.ilund4uError(f\"The input for the GFF parsing function must be either a folder or \"\n                                                   f\"a list of files.\")\n            if not gff_files:\n                raise ilund4u.manager.ilund4uError(f\"Folder {input_f} does not contain files.\")\n            if not os.path.exists(self.prms.args[\"output_dir\"]):\n                os.mkdir(self.prms.args[\"output_dir\"])\n            else:\n                if os.path.exists(self.proteins_fasta_file):\n                    os.remove(self.proteins_fasta_file)\n            genome_circularity_dict = dict()\n            if genome_annotation:\n                try:\n                    genome_annotation_table = pd.read_table(genome_annotation, sep=\"\\t\").set_index(\"id\")\n                    genome_circularity_dict = genome_annotation_table[\"circular\"].to_dict()\n                except:\n                    raise ilund4u.manager.ilund4uError(\"\u25cb Warning: unable to read genome annotation table. \"\n                                                       \"Check the format.\")\n            num_of_gff_files = len(gff_files)\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Reading gff file{'s' if len(gff_files) &gt; 1 else ''}...\", file=sys.stdout)\n            if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=num_of_gff_files, suffix='%(index)d/%(max)d')\n            proteome_list, annotation_rows = [], []\n            gff_records_batch = []\n            for gff_file_index, gff_file_path in enumerate(gff_files):\n                try:\n                    if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                        bar.next()\n                    gff_records = list(BCBio.GFF.parse(gff_file_path, limit_info=dict(gff_type=[\"CDS\"])))\n                    if len(gff_records) != 1:\n                        print(f\"\\n\u25cb Warning: gff file {gff_file_path} contains information for more than 1 \"\n                              f\"sequence. File will be skipped.\")\n                        continue\n                    current_gff_records = []\n                    gff_record = gff_records[0]\n                    try:\n                        record_locus_sequence = gff_record.seq\n                    except Bio.Seq.UndefinedSequenceError as error:\n                        raise ilund4u.manager.ilund4uError(f\"gff file doesn't contain corresponding \"\n                                                           f\"sequences.\") from error\n                    if self.prms.args[\"use_filename_as_contig_id\"]:\n                        gff_record.id = os.path.splitext(os.path.basename(gff_file_path))[0]\n                    features_ids = [i.id for i in gff_record.features]\n                    if len(features_ids) != len(set(features_ids)):\n                        raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains duplicated feature \"\n                                                           f\"ids while only unique are allowed.\")\n                    if len(features_ids) &gt; self.prms.args[\"min_proteome_size\"]:\n                        if gff_record.id in genome_circularity_dict.keys():\n                            circular = int(genome_circularity_dict[gff_record.id])\n                        else:\n                            circular = int(self.prms.args[\"circular_genomes\"])\n                        record_proteome = Proteome(proteome_id=gff_record.id, gff_file=gff_file_path, cdss=pd.Series(),\n                                                   circular=circular)\n                        record_cdss = []\n                        all_defined = True\n                        for gff_feature in gff_record.features:\n                            cds_id = gff_feature.id.replace(\";\", \",\")\n                            if gff_record.id not in cds_id:\n                                cds_id = f\"{gff_record.id}-{cds_id}\"  # Attention\n                            transl_table = self.prms.args[\"default_transl_table\"]\n                            if \"transl_table\" in gff_feature.qualifiers.keys():\n                                transl_table = int(gff_feature.qualifiers[\"transl_table\"][0])\n                            name = \"\"\n                            if self.prms.args[\"gff_CDS_name_source\"] in gff_feature.qualifiers:\n                                name = gff_feature.qualifiers[self.prms.args[\"gff_CDS_name_source\"]][0]\n\n                            sequence = gff_feature.translate(record_locus_sequence, table=transl_table, cds=False)[:-1]\n\n                            if not sequence.defined:\n                                all_defined = False\n                                continue\n\n                            current_gff_records.append(Bio.SeqRecord.SeqRecord(seq=sequence, id=cds_id, description=\"\"))\n                            cds = CDS(cds_id=cds_id, proteome_id=gff_record.id,\n                                      start=int(gff_feature.location.start) + 1, end=int(gff_feature.location.end),\n                                      strand=gff_feature.location.strand, name=name)\n                            record_cdss.append(cds)\n                        if all_defined:\n                            gff_records_batch += current_gff_records\n                            record_proteome.cdss = pd.Series(record_cdss, index=[cds.cds_id for cds in record_cdss])\n                            proteome_list.append(record_proteome)\n                            annotation_rows.append(dict(id=gff_record.id, length=len(gff_record.seq),\n                                                        proteome_size=len(features_ids),\n                                                        proteome_size_unique=\"\", protein_clusters=\"\"))\n                        else:\n                            raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains not defined feature\")\n                    if gff_file_index % 1000 == 0 or gff_file_index == num_of_gff_files - 1:\n                        with open(self.proteins_fasta_file, \"a\") as handle:\n                            Bio.SeqIO.write(gff_records_batch, handle, \"fasta\")\n                        gff_records_batch = []\n                except:\n                    print(f\"\u25cb Warning: gff file {gff_file_path} was not read properly and skipped\")\n                    if self.prms.args[\"parsing_debug\"]:\n                        self.prms.args[\"debug\"] = True\n                        raise ilund4u.manager.ilund4uError(\"Gff file {gff_file_path} was not read properly\")\n            if len(gff_files) &gt; 1 and self.prms.args[\"verbose\"]:\n                bar.finish()\n            proteome_ids = [pr.proteome_id for pr in proteome_list]\n            if len(proteome_ids) != len(set(proteome_ids)):\n                raise ilund4u.manager.ilund4uError(f\"The input gff files have duplicated contig ids.\\n  \"\n                                                   f\"You can use `--use-filename-as-id` parameter to use file name \"\n                                                   f\"as contig id which can help to fix the problem.\")\n            self.proteomes = pd.Series(proteome_list, index=[pr.proteome_id for pr in proteome_list])\n            self.annotation = pd.DataFrame(annotation_rows).set_index(\"id\")\n            self.__col_to_ind = {col: idx for idx, col in enumerate(self.annotation.columns)}\n            self.annotation = self.annotation.sort_values(by=\"proteome_size\")\n            self.proteomes = self.proteomes.loc[self.annotation.index]\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(proteome_list)} {'locus was' if len(proteome_list) == 1 else 'loci were'} loaded from\"\n                      f\" the gff files folder\", file=sys.stdout)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to load proteomes from gff files.\") from error\n\n    def mmseqs_cluster(self) -&gt; dict:\n        \"\"\"Cluster all proteins using mmseqs in order to define groups of homologues.\n\n        Returns:\n            dict: protein id to cluster id dictionary.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Running mmseqs for protein clustering...\", file=sys.stdout)\n            mmseqs_input = self.proteins_fasta_file\n            mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n            if os.path.exists(mmseqs_output_folder):\n                shutil.rmtree(mmseqs_output_folder)\n            os.mkdir(mmseqs_output_folder)\n            mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DB\")\n            os.mkdir(mmseqs_output_folder_db)\n            mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n            mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", mmseqs_input,\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\")], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"cluster\",\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"tmp\"),\n                            \"--cluster-mode\", str(self.prms.args[\"mmseqs_cluster_mode\"]),\n                            \"--cov-mode\", str(self.prms.args[\"mmseqs_cov_mode\"]),\n                            \"--min-seq-id\", str(self.prms.args[\"mmseqs_min_seq_id\"]),\n                            \"-c\", str(self.prms.args[\"mmseqs_c\"]),\n                            \"-s\", str(self.prms.args[\"mmseqs_s\"])], stdout=mmseqs_stdout,\n                           stderr=mmseqs_stderr)  # threads!\n            subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createtsv\",\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                            os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                            os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\")],\n                           stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n            mmseqs_clustering_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\"),\n                                                      sep=\"\\t\", header=None, names=[\"cluster\", \"protein_id\"])\n            mmseqs_clustering_results = mmseqs_clustering_results.set_index(\"protein_id\")[\"cluster\"].to_dict()\n            num_of_unique_clusters = len(set(mmseqs_clustering_results.values()))\n            num_of_proteins = len(mmseqs_clustering_results.keys())\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {num_of_unique_clusters} clusters for {num_of_proteins} proteins were found with mmseqs\\n\"\n                      f\"  \u29bf mmseqs clustering results were saved to \"\n                      f\"{os.path.join(mmseqs_output_folder, 'mmseqs_clustering.tsv')}\", file=sys.stdout)\n            return mmseqs_clustering_results\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs clustering.\") from error\n\n    def process_mmseqs_results(self, mmseqs_results: dict) -&gt; dict:\n        \"\"\"Process results of mmseqs clustering run.\n\n        Arguments:\n            mmseqs_results (dict): results of mmseqs_cluster function.\n\n        Returns:\n            dict: dictionary with protein cluster id to list of protein ids items.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Processing mmseqs results ...\", file=sys.stdout)\n            sequences_to_drop, drop_reason = [], []\n            current_p_length, cpl_added_proteomes = None, None\n            cluster_to_sequences = collections.defaultdict(list)\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix='%(index)d/%(max)d')\n            for p_index, proteome in enumerate(self.proteomes.to_list()):\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                seq_p_size = self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size\"]]\n                if seq_p_size != current_p_length:\n                    current_p_length = seq_p_size\n                    cpl_added_proteomes = []\n                seq_protein_clusters = []\n                for cds in proteome.cdss.to_list():\n                    cds.group = mmseqs_results[cds.cds_id]\n                    seq_protein_clusters.append(cds.group)\n                seq_protein_clusters_set = set(seq_protein_clusters)\n                unique_p_size = len(seq_protein_clusters_set)\n                if seq_protein_clusters_set in cpl_added_proteomes:\n                    sequences_to_drop.append(proteome.proteome_id)\n                    drop_reason.append(f\"Duplicate\")\n                    continue\n                if unique_p_size / seq_p_size &lt; self.prms.args[\"proteome_uniqueness_cutoff\"]:\n                    sequences_to_drop.append(proteome.proteome_id)\n                    drop_reason.append(\"Proteome uniqueness cutoff\")\n                    continue\n                self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size_unique\"]] = unique_p_size\n                self.annotation.iat[p_index, self.__col_to_ind[\"protein_clusters\"]] = list(seq_protein_clusters_set)\n                cpl_added_proteomes.append(seq_protein_clusters_set)\n                for p_cluster in seq_protein_clusters_set:\n                    cluster_to_sequences[p_cluster].append(proteome.proteome_id)\n            dropped_sequences = pd.DataFrame(dict(sequence=sequences_to_drop, reason=drop_reason))\n            dropped_sequences.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"dropped_sequences.tsv\"), sep=\"\\t\",\n                                     index=False)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n                print(f\"  \u29bf {len(sequences_to_drop)} proteomes were excluded after proteome\"\n                      f\" deduplication and filtering\", file=sys.stdout)\n            self.annotation = self.annotation.drop(sequences_to_drop)\n            self.proteomes = self.proteomes.drop(sequences_to_drop)\n            self.annotation = self.annotation.sort_values(by=\"proteome_size_unique\")\n            self.proteomes = self.proteomes.loc[self.annotation.index]\n            self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n            self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n            return cluster_to_sequences\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to process mmseqs output.\") from error\n\n    def build_proteome_network(self, cluster_to_sequences: dict) -&gt; igraph.Graph:\n        \"\"\"Build proteome network where each proteome represented by node and weighted edges between nodes -\n            fraction of shared homologues.\n\n        Arguments:\n            cluster_to_sequences (dict): cluster id to list of proteins dictionary\n                (results of process_mmseqs_results() function)\n\n        Returns:\n            igraph.Graph: proteome network.\n\n        \"\"\"\n        try:\n            cluster_to_proteome_index = dict()\n            for cluster, sequences in cluster_to_sequences.items():\n                indexes = sorted([self.seq_to_ind[seq_id] for seq_id in sequences])\n                cluster_to_proteome_index[cluster] = collections.deque(indexes)\n\n            proteome_sizes = self.annotation[[\"proteome_size_unique\", \"index\"]]\n            first_index_for_size = proteome_sizes.groupby(\"proteome_size_unique\").tail(1).copy()\n            max_p_size = first_index_for_size[\"proteome_size_unique\"].max()\n            cut_off_mult = 1 / self.prms.args[\"proteome_similarity_cutoff\"]\n            first_index_for_size[\"cutoff\"] = first_index_for_size[\"proteome_size_unique\"].apply(\n                lambda size: first_index_for_size[first_index_for_size[\"proteome_size_unique\"] &gt;=\n                                                  min(size * cut_off_mult, max_p_size)][\"index\"].min() + 1)\n            upper_index_cutoff = first_index_for_size.set_index(\"proteome_size_unique\")[\"cutoff\"]\n            proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Proteomes network construction...\", file=sys.stdout)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix=\"%(index)d/%(max)d\")\n            stime = time.time()\n            edges, weights = [], []\n            for i in range(len(self.proteomes.index)):\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                clusters_i = self.annotation.iat[i, self.__col_to_ind[\"protein_clusters\"]]\n                size_i = self.annotation.iat[i, self.__col_to_ind[\"proteome_size_unique\"]]\n                counts_i = collections.defaultdict(int)\n                upper_i_cutoff = upper_index_cutoff.at[size_i]\n                for cl in clusters_i:\n                    js = cluster_to_proteome_index[cl]\n                    for j in js.copy():\n                        if i &lt; j &lt; upper_i_cutoff:\n                            counts_i[j] += 1\n                        elif j &lt;= i:\n                            js.popleft()\n                        else:\n                            break\n                weights_i = pd.Series(counts_i)\n                proteome_sizes_connected = proteome_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(\n                    0.5 * (size_i + proteome_sizes_connected) / (size_i * proteome_sizes_connected), \\\n                    index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            etime = time.time()\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Network building elapsed time: {round(etime - stime, 2)} sec\")\n            graph = igraph.Graph(len(self.proteomes.index), edges, directed=False)\n            graph.vs[\"index\"] = self.annotation[\"index\"].to_list()\n            graph.vs[\"sequence_id\"] = self.annotation.index.to_list()\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(self.prms.args[\"output_dir\"], \"proteome_network.gml\"))\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Proteomes network with {len(edges)} connections was built\\n\"\n                      f\"  \u29bf Network was saved as {os.path.join(self.prms.args['output_dir'], 'proteome_network.gml')}\",\n                      file=sys.stdout)\n            return graph\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to built proteome network.\") from error\n\n    def find_proteome_communities(self, graph: igraph.Graph) -&gt; None:\n        \"\"\"Find proteome communities using Leiden algorithm and update communities attribute.\n\n        Arguments:\n            graph (igraph.Graph): network of proteomes obtained by build_proteome_network function.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Proteome network partitioning using the Leiden algorithm...\")\n            partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_p\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(set(partition_leiden.membership))} proteome communities were found\")\n            communities_annot_rows = []\n            sequences_to_drop = []\n            for community_index, community in enumerate(partition_leiden):\n                community_size = len(community)\n                subgraph = graph.subgraph(community)\n                proteomes = subgraph.vs[\"sequence_id\"]\n                if community_size &gt;= self.prms.args[\"min_proteome_community_size\"]:\n                    self.communities[community_index] = proteomes\n                else:\n                    sequences_to_drop += proteomes\n                if community_size &gt; 1:\n                    subgraph_edges = subgraph.get_edgelist()\n                    num_of_edges = len(subgraph_edges)\n                    num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                    weights = subgraph.es[\"weight\"]\n                    avg_weight = round(np.mean(weights), 3)\n                    max_identity = max(weights)\n                else:\n                    num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n                communities_annot_rows.append([community_index, community_size, avg_weight, max_identity,\n                                               num_of_edges_fr, num_of_edges, \";\".join(proteomes)])\n            communities_annot = pd.DataFrame(communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\", \"max_weight\",\n                                                                              \"fr_edges\", \"n_edges\", \"proteomes\"])\n            communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                               \"proteome_communities.tsv\")), sep=\"\\t\", index=False)\n            communities_annot = communities_annot[communities_annot[\"size\"] &gt;=\n                                                  self.prms.args[\"min_proteome_community_size\"]]\n            self.communities_annot = communities_annot.set_index(\"id\")\n            self.annotation = self.annotation.drop(sequences_to_drop)\n            self.proteomes = self.proteomes.drop(sequences_to_drop)\n            self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n            self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {len(communities_annot.index)} proteomes communities with size &gt;= \"\n                      f\"{self.prms.args['min_proteome_community_size']} were taken for further analysis\",\n                      file=sys.stdout)\n                print(f\"  \u29bf {len(sequences_to_drop)} proteomes from smaller communities were excluded from the \"\n                      f\"analysis\", file=sys.stdout)\n            if len(communities_annot.index) == 0:\n                print(\"\u25cb Termination since no proteome community was taken for further analysis\")\n                sys.exit()\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to find proteome communities.\") from error\n\n    def define_protein_classes(self) -&gt; None:\n        \"\"\"Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Defining protein classes within each community...\")\n            number_of_communities = len(self.communities_annot.index)\n            protein_classes_trows = []\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix='%(index)d/%(max)d')\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_size = len(com_pr_ids)\n                com_annotation = self.annotation.loc[com_pr_ids]\n                com_protein_clusters = com_annotation[\"protein_clusters\"].sum()\n                com_protein_clusters_count = collections.Counter(com_protein_clusters)\n                com_protein_classes = dict()\n                for pc, counts in com_protein_clusters_count.items():\n                    pc_fraction = counts / com_size\n                    if pc_fraction &lt; self.prms.args[\"variable_protein_cluster_cutoff\"]:\n                        pc_class = \"variable\"\n                    elif pc_fraction &gt; self.prms.args[\"conserved_protein_cluster_cutoff\"]:\n                        pc_class = \"conserved\"\n                    else:\n                        pc_class = \"intermediate\"\n                    com_protein_classes[pc] = pc_class\n                    protein_classes_trows.append(dict(community=com_id, community_size=com_size, protein_group=pc,\n                                                      protein_group_class=pc_class, fraction=round(pc_fraction, 3),\n                                                      protein_group_counts=counts))\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                for com_proteome in com_proteomes:\n                    for cds in com_proteome.cdss:\n                        cds.g_class = com_protein_classes[cds.group]\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            protein_classes_t = pd.DataFrame(protein_classes_trows)\n            protein_classes_t.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_classes.tsv\"), sep=\"\\t\",\n                                     index=False)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to define protein classes.\") from error\n\n    def annotate_variable_islands(self) -&gt; None:\n        \"\"\"Annotate variable islands defined as a region with a set of non-conserved proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Annotating variable islands within each proteome...\", file=sys.stdout)\n            total_number_of_variable_regions = 0\n            for proteome_index, proteome in enumerate(self.proteomes):\n                proteome.annotate_variable_islands(self.prms)\n                total_number_of_variable_regions += proteome.islands.size\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {total_number_of_variable_regions} variable regions are annotated in \"\n                      f\"{len(self.proteomes.index)} proteomes \"\n                      f\"({round(total_number_of_variable_regions / len(self.proteomes.index), 3)} per proteome)\",\n                      file=sys.stdout)\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to annotate variable islands.\") from error\n\n    def build_islands_network(self) -&gt; igraph.Graph:\n        \"\"\"Build island network where each node - an island and weighted edges - fraction of shared\n            conserved neighbours homologues.\n\n        Returns:\n            igraph.Graph: Island network.\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Island network construction within each proteome community...\")\n            output_network_folder = os.path.join(self.prms.args[\"output_dir\"], \"island_networks\")\n            if os.path.exists(output_network_folder):\n                shutil.rmtree(output_network_folder)\n            os.mkdir(output_network_folder)\n            number_of_communities = len(self.communities_annot.index)\n            stime = time.time()\n            networks = []\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                com_island_n_sizes = pd.Series()\n                com_neighbours = pd.Series()\n                cluster_to_island = collections.defaultdict(collections.deque)\n\n                islands_list = [island for proteome in com_proteomes.to_list() for island in proteome.islands.to_list()]\n                island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(islands_list)}\n                for proteome in com_proteomes:\n                    for island in proteome.islands.to_list():\n                        island_id = island.island_id\n                        island_index = island_id_to_index[island_id]\n                        conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(proteome.cdss))\n                        com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                        com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                        for cing in conserved_island_neighbours_groups:\n                            cluster_to_island[cing].append(island_index)\n                edges, weights = [], []\n\n                for i in range(len(com_island_n_sizes.index)):\n                    neighbours_i = com_neighbours.iat[i]\n                    size_i = com_island_n_sizes.iat[i]\n                    counts_i = collections.defaultdict(int)\n                    for ncl in neighbours_i:\n                        js = cluster_to_island[ncl]\n                        for j in js.copy():\n                            if i &lt; j:\n                                counts_i[j] += 1\n                            else:\n                                js.popleft()\n                    weights_i = pd.Series(counts_i)\n                    connected_n_sizes = com_island_n_sizes.iloc[weights_i.index]\n                    norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                              index=weights_i.index)\n                    weights_i = weights_i.mul(norm_factor_i)\n                    weights_i = weights_i[weights_i &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                    for j, w in weights_i.items():\n                        edges.append([i, j])\n                        weights.append(round(w, 4))\n                graph = igraph.Graph(len(com_island_n_sizes.index), edges, directed=False)\n                graph.vs[\"index\"] = com_island_n_sizes.index.to_list()\n                graph.vs[\"island_id\"] = [isl.island_id for isl in islands_list]\n                graph.vs[\"island_size\"] = [isl.size for isl in islands_list]\n                graph.vs[\"flanked\"] = [isl.flanked for isl in islands_list]\n                graph.vs[\"proteome_id\"] = [isl.proteome for isl in islands_list]\n                graph.es[\"weight\"] = weights\n                graph.save(os.path.join(output_network_folder, f\"{com_id}.gml\"))\n                networks.append(graph)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            etime = time.time()\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf Island network building elapsed time: {round(etime - stime, 2)} sec\")\n            return networks\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to build island network.\") from error\n\n    def find_hotspots(self, networks: igraph.Graph) -&gt; None:\n        \"\"\"Find hotspots in an island network using Leiden algorithm.\n\n        Args:\n            networks (igraph.Graph): Island network obtained by build_islands_network() function.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Searching for hotspots within each community...\")\n            number_of_communities = len(self.communities_annot.index)\n            if self.prms.args[\"verbose\"]:\n                bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n            hotspots_l, h_annotation_rows = [], []\n            for com_id, com_pr_ids in self.communities.items():\n                if self.prms.args[\"verbose\"]:\n                    bar.next()\n                com_size = len(com_pr_ids)\n                com_proteomes = self.proteomes.loc[com_pr_ids]\n                com_island_network = networks[com_id]\n                partition_leiden = leidenalg.find_partition(com_island_network, leidenalg.CPMVertexPartition,\n                                                            resolution_parameter=self.prms.args[\n                                                                \"leiden_resolution_parameter_i\"],\n                                                            weights=\"weight\", n_iterations=-1)\n                com_island_network.vs[\"communities_Leiden\"] = partition_leiden.membership\n                islands_list = [island for proteome in com_proteomes for island in proteome.islands.to_list()]\n                for icom_ind, i_com in enumerate(partition_leiden):\n                    subgraph = com_island_network.subgraph(i_com)\n                    proteomes = subgraph.vs[\"proteome_id\"]\n                    hotspot_uniq_size = len(set(proteomes))\n                    hotspot_presence = hotspot_uniq_size / com_size\n                    if hotspot_presence &gt; self.prms.args[\"hotspot_presence_cutoff\"]:\n                        island_indexes, island_ids = subgraph.vs[\"index\"], subgraph.vs[\"island_id\"]\n                        island_size, island_flanked = subgraph.vs[\"island_size\"], subgraph.vs[\"flanked\"]\n                        strength, degree = subgraph.strength(weights=\"weight\"), subgraph.degree()\n                        island_annotation = pd.DataFrame(dict(island=island_ids, island_index=island_indexes,\n                                                              island_size=island_size, proteome=proteomes,\n                                                              flanked=island_flanked, strength=strength,\n                                                              degree=degree)).set_index(\"island\")\n                        if self.prms.args[\"deduplicate_proteomes_within_hotspot\"]:  # To update usage wo it\n                            island_annotation = island_annotation.sort_values(by=\"strength\", ascending=False)\n                            island_annotation = island_annotation.drop_duplicates(subset=\"proteome\", keep=\"first\")\n                            island_annotation = island_annotation.sort_values(by=\"island_index\")\n                            nodes_to_remove = subgraph.vs.select(island_id_notin=island_annotation.index.to_list())\n                            subgraph.delete_vertices(nodes_to_remove)\n                        islands = [islands_list[ind] for ind in island_annotation[\"island_index\"].to_list()]\n                        unique_island_cds_groups = []\n                        conserved_island_groups_count = collections.defaultdict(int)\n                        flanked_count = 0\n                        for island in islands:\n                            flanked_count += island.flanked\n                            island_proteome_cdss = com_proteomes.at[island.proteome].cdss\n                            island_cds_groups = island_proteome_cdss.iloc[island.indexes].apply(\n                                lambda isl: isl.group).to_list()\n                            ic_indexes = island.left_cons_neighbours + island.right_cons_neighbours\n                            island_conserved_groups = island_proteome_cdss.iloc[ic_indexes].apply(\n                                lambda isl: isl.group).to_list()\n                            if set(island_cds_groups) not in unique_island_cds_groups:\n                                unique_island_cds_groups.append(set(island_cds_groups))\n                            for icg in set(island_conserved_groups):\n                                conserved_island_groups_count[icg] += 1\n                        if flanked_count / hotspot_uniq_size &gt;= self.prms.args[\"flanked_fraction_cutoff\"]:\n                            flanked_hotspot = 1\n                        else:\n                            flanked_hotspot = 0\n                        if not self.prms.args[\"report_not_flanked\"] and not flanked_hotspot:\n                            continue\n\n                        signature_cutoff = int(self.prms.args[\"hotspot_signature_presence_cutoff\"] * hotspot_uniq_size)\n                        hotspot_conserved_signature = [g for g, c in conserved_island_groups_count.items() if\n                                                       c &gt;= signature_cutoff]\n                        number_of_unique_islands = len(unique_island_cds_groups)\n                        hotspot = Hotspot(hotspot_id=f\"{com_id}-{icom_ind}\", proteome_community=com_id,\n                                          size=hotspot_uniq_size, islands=islands,\n                                          conserved_signature=hotspot_conserved_signature,\n                                          island_annotation=island_annotation, flanked=flanked_hotspot)\n                        for island in islands:\n                            island.hotspot_id = hotspot.hotspot_id\n                        h_annotation_row = dict(hotspot_id=f\"{com_id}-{icom_ind}\", size=hotspot_uniq_size,\n                                                uniqueness=round(number_of_unique_islands / hotspot_uniq_size, 3),\n                                                number_of_unique_islands=number_of_unique_islands,\n                                                proteome_community=com_id, flanked=flanked_hotspot,\n                                                flanked_fraction=round(flanked_count / hotspot_uniq_size, 3))\n                        h_annotation_rows.append(h_annotation_row)\n                        hotspots_l.append(hotspot)\n            if self.prms.args[\"verbose\"]:\n                bar.finish()\n            h_annotation = pd.DataFrame(h_annotation_rows).set_index(\"hotspot_id\")\n            hotspots_s = pd.Series(hotspots_l, index=[hotspot.hotspot_id for hotspot in hotspots_l])\n            hotspots_obj = Hotspots(hotspots_s, h_annotation, parameters=self.prms)\n            num_of_hotspots = len(hotspots_l)\n            num_of_flanked = sum([hotspot.flanked for hotspot in hotspots_l])\n            if self.prms.args[\"verbose\"]:\n                print(f\"  \u29bf {num_of_hotspots} hotspots were found in {number_of_communities} proteome communities\"\n                      f\"  (Avg: {round(num_of_hotspots / number_of_communities, 3)} per community)\\n\"\n                      f\"  {num_of_flanked}/{num_of_hotspots} hotspots are flanked (consist of islands that have \"\n                      f\"conserved genes on both sides)\",\n                      file=sys.stdout)\n            return hotspots_obj\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to find communities in the island network.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.__init__","title":"<code>__init__(parameters)</code>","text":"<p>Proteomes class constructor.</p> <p>Parameters:</p> <ul> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def __init__(self, parameters: ilund4u.manager.Parameters):\n    \"\"\"Proteomes class constructor.\n\n    Arguments:\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = pd.Series()\n    self.annotation = None\n    self.__col_to_ind = None\n    self.seq_to_ind = None\n    self.communities = dict()\n    self.communities_annot = None\n    self.proteins_fasta_file = os.path.join(parameters.args[\"output_dir\"], \"all_proteins.fa\")\n    self.prms = parameters\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.annotate_variable_islands","title":"<code>annotate_variable_islands()</code>","text":"<p>Annotate variable islands defined as a region with a set of non-conserved proteins.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def annotate_variable_islands(self) -&gt; None:\n    \"\"\"Annotate variable islands defined as a region with a set of non-conserved proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Annotating variable islands within each proteome...\", file=sys.stdout)\n        total_number_of_variable_regions = 0\n        for proteome_index, proteome in enumerate(self.proteomes):\n            proteome.annotate_variable_islands(self.prms)\n            total_number_of_variable_regions += proteome.islands.size\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {total_number_of_variable_regions} variable regions are annotated in \"\n                  f\"{len(self.proteomes.index)} proteomes \"\n                  f\"({round(total_number_of_variable_regions / len(self.proteomes.index), 3)} per proteome)\",\n                  file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to annotate variable islands.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.build_islands_network","title":"<code>build_islands_network()</code>","text":"<p>Build island network where each node - an island and weighted edges - fraction of shared     conserved neighbours homologues.</p> <p>Returns:</p> <ul> <li> <code>Graph</code>         \u2013          <p>igraph.Graph: Island network.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_islands_network(self) -&gt; igraph.Graph:\n    \"\"\"Build island network where each node - an island and weighted edges - fraction of shared\n        conserved neighbours homologues.\n\n    Returns:\n        igraph.Graph: Island network.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Island network construction within each proteome community...\")\n        output_network_folder = os.path.join(self.prms.args[\"output_dir\"], \"island_networks\")\n        if os.path.exists(output_network_folder):\n            shutil.rmtree(output_network_folder)\n        os.mkdir(output_network_folder)\n        number_of_communities = len(self.communities_annot.index)\n        stime = time.time()\n        networks = []\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            com_island_n_sizes = pd.Series()\n            com_neighbours = pd.Series()\n            cluster_to_island = collections.defaultdict(collections.deque)\n\n            islands_list = [island for proteome in com_proteomes.to_list() for island in proteome.islands.to_list()]\n            island_id_to_index = {isl.island_id: ind for ind, isl in enumerate(islands_list)}\n            for proteome in com_proteomes:\n                for island in proteome.islands.to_list():\n                    island_id = island.island_id\n                    island_index = island_id_to_index[island_id]\n                    conserved_island_neighbours_groups = set(island.get_cons_neighbours_groups(proteome.cdss))\n                    com_neighbours.at[island_index] = list(conserved_island_neighbours_groups)\n                    com_island_n_sizes.at[island_index] = len(conserved_island_neighbours_groups)\n                    for cing in conserved_island_neighbours_groups:\n                        cluster_to_island[cing].append(island_index)\n            edges, weights = [], []\n\n            for i in range(len(com_island_n_sizes.index)):\n                neighbours_i = com_neighbours.iat[i]\n                size_i = com_island_n_sizes.iat[i]\n                counts_i = collections.defaultdict(int)\n                for ncl in neighbours_i:\n                    js = cluster_to_island[ncl]\n                    for j in js.copy():\n                        if i &lt; j:\n                            counts_i[j] += 1\n                        else:\n                            js.popleft()\n                weights_i = pd.Series(counts_i)\n                connected_n_sizes = com_island_n_sizes.iloc[weights_i.index]\n                norm_factor_i = pd.Series(0.5 * (size_i + connected_n_sizes) / (size_i * connected_n_sizes), \\\n                                          index=weights_i.index)\n                weights_i = weights_i.mul(norm_factor_i)\n                weights_i = weights_i[weights_i &gt;= self.prms.args[\"island_neighbours_similarity_cutoff\"]]\n                for j, w in weights_i.items():\n                    edges.append([i, j])\n                    weights.append(round(w, 4))\n            graph = igraph.Graph(len(com_island_n_sizes.index), edges, directed=False)\n            graph.vs[\"index\"] = com_island_n_sizes.index.to_list()\n            graph.vs[\"island_id\"] = [isl.island_id for isl in islands_list]\n            graph.vs[\"island_size\"] = [isl.size for isl in islands_list]\n            graph.vs[\"flanked\"] = [isl.flanked for isl in islands_list]\n            graph.vs[\"proteome_id\"] = [isl.proteome for isl in islands_list]\n            graph.es[\"weight\"] = weights\n            graph.save(os.path.join(output_network_folder, f\"{com_id}.gml\"))\n            networks.append(graph)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        etime = time.time()\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Island network building elapsed time: {round(etime - stime, 2)} sec\")\n        return networks\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to build island network.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.build_proteome_network","title":"<code>build_proteome_network(cluster_to_sequences)</code>","text":"<p>Build proteome network where each proteome represented by node and weighted edges between nodes -     fraction of shared homologues.</p> <p>Parameters:</p> <ul> <li> <code>cluster_to_sequences</code>             (<code>dict</code>)         \u2013          <p>cluster id to list of proteins dictionary (results of process_mmseqs_results() function)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Graph</code>         \u2013          <p>igraph.Graph: proteome network.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def build_proteome_network(self, cluster_to_sequences: dict) -&gt; igraph.Graph:\n    \"\"\"Build proteome network where each proteome represented by node and weighted edges between nodes -\n        fraction of shared homologues.\n\n    Arguments:\n        cluster_to_sequences (dict): cluster id to list of proteins dictionary\n            (results of process_mmseqs_results() function)\n\n    Returns:\n        igraph.Graph: proteome network.\n\n    \"\"\"\n    try:\n        cluster_to_proteome_index = dict()\n        for cluster, sequences in cluster_to_sequences.items():\n            indexes = sorted([self.seq_to_ind[seq_id] for seq_id in sequences])\n            cluster_to_proteome_index[cluster] = collections.deque(indexes)\n\n        proteome_sizes = self.annotation[[\"proteome_size_unique\", \"index\"]]\n        first_index_for_size = proteome_sizes.groupby(\"proteome_size_unique\").tail(1).copy()\n        max_p_size = first_index_for_size[\"proteome_size_unique\"].max()\n        cut_off_mult = 1 / self.prms.args[\"proteome_similarity_cutoff\"]\n        first_index_for_size[\"cutoff\"] = first_index_for_size[\"proteome_size_unique\"].apply(\n            lambda size: first_index_for_size[first_index_for_size[\"proteome_size_unique\"] &gt;=\n                                              min(size * cut_off_mult, max_p_size)][\"index\"].min() + 1)\n        upper_index_cutoff = first_index_for_size.set_index(\"proteome_size_unique\")[\"cutoff\"]\n        proteome_sizes = proteome_sizes.set_index(\"index\")[\"proteome_size_unique\"]\n\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Proteomes network construction...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix=\"%(index)d/%(max)d\")\n        stime = time.time()\n        edges, weights = [], []\n        for i in range(len(self.proteomes.index)):\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            clusters_i = self.annotation.iat[i, self.__col_to_ind[\"protein_clusters\"]]\n            size_i = self.annotation.iat[i, self.__col_to_ind[\"proteome_size_unique\"]]\n            counts_i = collections.defaultdict(int)\n            upper_i_cutoff = upper_index_cutoff.at[size_i]\n            for cl in clusters_i:\n                js = cluster_to_proteome_index[cl]\n                for j in js.copy():\n                    if i &lt; j &lt; upper_i_cutoff:\n                        counts_i[j] += 1\n                    elif j &lt;= i:\n                        js.popleft()\n                    else:\n                        break\n            weights_i = pd.Series(counts_i)\n            proteome_sizes_connected = proteome_sizes.iloc[weights_i.index]\n            norm_factor_i = pd.Series(\n                0.5 * (size_i + proteome_sizes_connected) / (size_i * proteome_sizes_connected), \\\n                index=weights_i.index)\n            weights_i = weights_i.mul(norm_factor_i)\n            weights_i = weights_i[weights_i &gt;= self.prms.args[\"proteome_similarity_cutoff\"]]\n            for j, w in weights_i.items():\n                edges.append([i, j])\n                weights.append(round(w, 4))\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        etime = time.time()\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Network building elapsed time: {round(etime - stime, 2)} sec\")\n        graph = igraph.Graph(len(self.proteomes.index), edges, directed=False)\n        graph.vs[\"index\"] = self.annotation[\"index\"].to_list()\n        graph.vs[\"sequence_id\"] = self.annotation.index.to_list()\n        graph.es[\"weight\"] = weights\n        graph.save(os.path.join(self.prms.args[\"output_dir\"], \"proteome_network.gml\"))\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Proteomes network with {len(edges)} connections was built\\n\"\n                  f\"  \u29bf Network was saved as {os.path.join(self.prms.args['output_dir'], 'proteome_network.gml')}\",\n                  file=sys.stdout)\n        return graph\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to built proteome network.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.db_init","title":"<code>db_init(db_path, parameters)</code>  <code>classmethod</code>","text":"<p>Class method to load a Proteomes object from a database.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>path to the database.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>cls</code>        \u2013          <p>Proteomes object.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>@classmethod\ndef db_init(cls, db_path: str, parameters: ilund4u.manager.Parameters):\n    \"\"\"Class method to load a Proteomes object from a database.\n\n    Arguments:\n        db_path (str): path to the database.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        cls: Proteomes object.\n\n    \"\"\"\n    try:\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading cds objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"cds.ind.attributes.json\"), \"r\") as json_file:\n            cds_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(cds_ind_attributes), suffix='%(index)d/%(max)d')\n        cds_list = []\n        for cds_dict in cds_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            cds_list.append(CDS(**cds_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        cdss = pd.Series(cds_list, index=[cds.cds_id for cds in cds_list])\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading island objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"island.ind.attributes.json\"), \"r\") as json_file:\n            island_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(island_ind_attributes), suffix='%(index)d/%(max)d')\n        island_list = []\n        for proteome_dict in island_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            island_list.append(Island(**proteome_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        islands = pd.Series(island_list, index=[island.island_id for island in island_list])\n\n        if parameters.args[\"verbose\"]:\n            print(f\"\u25cb Loading proteome objects...\", file=sys.stdout)\n        with open(os.path.join(db_path, \"proteome.ind.attributes.json\"), \"r\") as json_file:\n            proteome_ind_attributes = json.load(json_file)\n        if parameters.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(proteome_ind_attributes), suffix='%(index)d/%(max)d')\n        proteome_list = []\n        for proteome_dict in proteome_ind_attributes:\n            if parameters.args[\"verbose\"]:\n                bar.next()\n            proteome_dict[\"gff_file\"] = os.path.join(db_path, \"gff\", proteome_dict[\"gff_file\"])\n            proteome_dict[\"cdss\"] = cdss.loc[proteome_dict[\"cdss\"]]\n            proteome_dict[\"islands\"] = islands.loc[proteome_dict[\"islands\"]]\n            proteome_list.append(Proteome(**proteome_dict))\n        if parameters.args[\"verbose\"]:\n            bar.finish()\n        proteomes = pd.Series(proteome_list, index=[proteome.proteome_id for proteome in proteome_list])\n\n        cls_obj = cls(parameters)\n        cls_obj.proteomes = proteomes\n        with open(os.path.join(db_path, \"proteomes.attributes.json\"), \"r\") as json_file:\n            proteomes_attributes = json.load(json_file)\n        cls_obj.communities = {int(k): v for k, v in proteomes_attributes[\"communities\"].items()}\n\n        cls_obj.proteins_fasta_file = os.path.join(db_path, proteomes_attributes[\"proteins_fasta_file\"])\n\n        cls_obj.annotation = pd.read_table(os.path.join(db_path, \"proteomes.annotations.tsv\"),\n                                           sep=\"\\t\").set_index(\"proteome_id\")\n        cls_obj.__col_to_ind = {col: idx for idx, col in enumerate(cls_obj.annotation.columns)}\n        cls_obj.communities_annot = pd.read_table(os.path.join(db_path, \"proteomes.communities_annot.tsv\"),\n                                                  sep=\"\\t\").set_index(\"id\")\n        cls_obj.seq_to_ind = {sid: idx for idx, sid in enumerate(cls_obj.annotation.index)}\n\n        return cls_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to read Proteomes from the database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.define_protein_classes","title":"<code>define_protein_classes()</code>","text":"<p>Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def define_protein_classes(self) -&gt; None:\n    \"\"\"Define protein classes (conserved, intermediate, variable) based on presence in a proteome community.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Defining protein classes within each community...\")\n        number_of_communities = len(self.communities_annot.index)\n        protein_classes_trows = []\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix='%(index)d/%(max)d')\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_size = len(com_pr_ids)\n            com_annotation = self.annotation.loc[com_pr_ids]\n            com_protein_clusters = com_annotation[\"protein_clusters\"].sum()\n            com_protein_clusters_count = collections.Counter(com_protein_clusters)\n            com_protein_classes = dict()\n            for pc, counts in com_protein_clusters_count.items():\n                pc_fraction = counts / com_size\n                if pc_fraction &lt; self.prms.args[\"variable_protein_cluster_cutoff\"]:\n                    pc_class = \"variable\"\n                elif pc_fraction &gt; self.prms.args[\"conserved_protein_cluster_cutoff\"]:\n                    pc_class = \"conserved\"\n                else:\n                    pc_class = \"intermediate\"\n                com_protein_classes[pc] = pc_class\n                protein_classes_trows.append(dict(community=com_id, community_size=com_size, protein_group=pc,\n                                                  protein_group_class=pc_class, fraction=round(pc_fraction, 3),\n                                                  protein_group_counts=counts))\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            for com_proteome in com_proteomes:\n                for cds in com_proteome.cdss:\n                    cds.g_class = com_protein_classes[cds.group]\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        protein_classes_t = pd.DataFrame(protein_classes_trows)\n        protein_classes_t.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"protein_group_classes.tsv\"), sep=\"\\t\",\n                                 index=False)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to define protein classes.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.find_hotspots","title":"<code>find_hotspots(networks)</code>","text":"<p>Find hotspots in an island network using Leiden algorithm.</p> <p>Parameters:</p> <ul> <li> <code>networks</code>             (<code>Graph</code>)         \u2013          <p>Island network obtained by build_islands_network() function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def find_hotspots(self, networks: igraph.Graph) -&gt; None:\n    \"\"\"Find hotspots in an island network using Leiden algorithm.\n\n    Args:\n        networks (igraph.Graph): Island network obtained by build_islands_network() function.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Searching for hotspots within each community...\")\n        number_of_communities = len(self.communities_annot.index)\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=number_of_communities, suffix=\"%(index)d/%(max)d\")\n        hotspots_l, h_annotation_rows = [], []\n        for com_id, com_pr_ids in self.communities.items():\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            com_size = len(com_pr_ids)\n            com_proteomes = self.proteomes.loc[com_pr_ids]\n            com_island_network = networks[com_id]\n            partition_leiden = leidenalg.find_partition(com_island_network, leidenalg.CPMVertexPartition,\n                                                        resolution_parameter=self.prms.args[\n                                                            \"leiden_resolution_parameter_i\"],\n                                                        weights=\"weight\", n_iterations=-1)\n            com_island_network.vs[\"communities_Leiden\"] = partition_leiden.membership\n            islands_list = [island for proteome in com_proteomes for island in proteome.islands.to_list()]\n            for icom_ind, i_com in enumerate(partition_leiden):\n                subgraph = com_island_network.subgraph(i_com)\n                proteomes = subgraph.vs[\"proteome_id\"]\n                hotspot_uniq_size = len(set(proteomes))\n                hotspot_presence = hotspot_uniq_size / com_size\n                if hotspot_presence &gt; self.prms.args[\"hotspot_presence_cutoff\"]:\n                    island_indexes, island_ids = subgraph.vs[\"index\"], subgraph.vs[\"island_id\"]\n                    island_size, island_flanked = subgraph.vs[\"island_size\"], subgraph.vs[\"flanked\"]\n                    strength, degree = subgraph.strength(weights=\"weight\"), subgraph.degree()\n                    island_annotation = pd.DataFrame(dict(island=island_ids, island_index=island_indexes,\n                                                          island_size=island_size, proteome=proteomes,\n                                                          flanked=island_flanked, strength=strength,\n                                                          degree=degree)).set_index(\"island\")\n                    if self.prms.args[\"deduplicate_proteomes_within_hotspot\"]:  # To update usage wo it\n                        island_annotation = island_annotation.sort_values(by=\"strength\", ascending=False)\n                        island_annotation = island_annotation.drop_duplicates(subset=\"proteome\", keep=\"first\")\n                        island_annotation = island_annotation.sort_values(by=\"island_index\")\n                        nodes_to_remove = subgraph.vs.select(island_id_notin=island_annotation.index.to_list())\n                        subgraph.delete_vertices(nodes_to_remove)\n                    islands = [islands_list[ind] for ind in island_annotation[\"island_index\"].to_list()]\n                    unique_island_cds_groups = []\n                    conserved_island_groups_count = collections.defaultdict(int)\n                    flanked_count = 0\n                    for island in islands:\n                        flanked_count += island.flanked\n                        island_proteome_cdss = com_proteomes.at[island.proteome].cdss\n                        island_cds_groups = island_proteome_cdss.iloc[island.indexes].apply(\n                            lambda isl: isl.group).to_list()\n                        ic_indexes = island.left_cons_neighbours + island.right_cons_neighbours\n                        island_conserved_groups = island_proteome_cdss.iloc[ic_indexes].apply(\n                            lambda isl: isl.group).to_list()\n                        if set(island_cds_groups) not in unique_island_cds_groups:\n                            unique_island_cds_groups.append(set(island_cds_groups))\n                        for icg in set(island_conserved_groups):\n                            conserved_island_groups_count[icg] += 1\n                    if flanked_count / hotspot_uniq_size &gt;= self.prms.args[\"flanked_fraction_cutoff\"]:\n                        flanked_hotspot = 1\n                    else:\n                        flanked_hotspot = 0\n                    if not self.prms.args[\"report_not_flanked\"] and not flanked_hotspot:\n                        continue\n\n                    signature_cutoff = int(self.prms.args[\"hotspot_signature_presence_cutoff\"] * hotspot_uniq_size)\n                    hotspot_conserved_signature = [g for g, c in conserved_island_groups_count.items() if\n                                                   c &gt;= signature_cutoff]\n                    number_of_unique_islands = len(unique_island_cds_groups)\n                    hotspot = Hotspot(hotspot_id=f\"{com_id}-{icom_ind}\", proteome_community=com_id,\n                                      size=hotspot_uniq_size, islands=islands,\n                                      conserved_signature=hotspot_conserved_signature,\n                                      island_annotation=island_annotation, flanked=flanked_hotspot)\n                    for island in islands:\n                        island.hotspot_id = hotspot.hotspot_id\n                    h_annotation_row = dict(hotspot_id=f\"{com_id}-{icom_ind}\", size=hotspot_uniq_size,\n                                            uniqueness=round(number_of_unique_islands / hotspot_uniq_size, 3),\n                                            number_of_unique_islands=number_of_unique_islands,\n                                            proteome_community=com_id, flanked=flanked_hotspot,\n                                            flanked_fraction=round(flanked_count / hotspot_uniq_size, 3))\n                    h_annotation_rows.append(h_annotation_row)\n                    hotspots_l.append(hotspot)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n        h_annotation = pd.DataFrame(h_annotation_rows).set_index(\"hotspot_id\")\n        hotspots_s = pd.Series(hotspots_l, index=[hotspot.hotspot_id for hotspot in hotspots_l])\n        hotspots_obj = Hotspots(hotspots_s, h_annotation, parameters=self.prms)\n        num_of_hotspots = len(hotspots_l)\n        num_of_flanked = sum([hotspot.flanked for hotspot in hotspots_l])\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {num_of_hotspots} hotspots were found in {number_of_communities} proteome communities\"\n                  f\"  (Avg: {round(num_of_hotspots / number_of_communities, 3)} per community)\\n\"\n                  f\"  {num_of_flanked}/{num_of_hotspots} hotspots are flanked (consist of islands that have \"\n                  f\"conserved genes on both sides)\",\n                  file=sys.stdout)\n        return hotspots_obj\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to find communities in the island network.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.find_proteome_communities","title":"<code>find_proteome_communities(graph)</code>","text":"<p>Find proteome communities using Leiden algorithm and update communities attribute.</p> <p>Parameters:</p> <ul> <li> <code>graph</code>             (<code>Graph</code>)         \u2013          <p>network of proteomes obtained by build_proteome_network function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def find_proteome_communities(self, graph: igraph.Graph) -&gt; None:\n    \"\"\"Find proteome communities using Leiden algorithm and update communities attribute.\n\n    Arguments:\n        graph (igraph.Graph): network of proteomes obtained by build_proteome_network function.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Proteome network partitioning using the Leiden algorithm...\")\n        partition_leiden = leidenalg.find_partition(graph, leidenalg.CPMVertexPartition,\n                                                    resolution_parameter=self.prms.args[\n                                                        \"leiden_resolution_parameter_p\"],\n                                                    weights=\"weight\", n_iterations=-1)\n        graph.vs[\"communities_Leiden\"] = partition_leiden.membership\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(set(partition_leiden.membership))} proteome communities were found\")\n        communities_annot_rows = []\n        sequences_to_drop = []\n        for community_index, community in enumerate(partition_leiden):\n            community_size = len(community)\n            subgraph = graph.subgraph(community)\n            proteomes = subgraph.vs[\"sequence_id\"]\n            if community_size &gt;= self.prms.args[\"min_proteome_community_size\"]:\n                self.communities[community_index] = proteomes\n            else:\n                sequences_to_drop += proteomes\n            if community_size &gt; 1:\n                subgraph_edges = subgraph.get_edgelist()\n                num_of_edges = len(subgraph_edges)\n                num_of_edges_fr = num_of_edges / (community_size * (community_size - 1) * 0.5)\n                weights = subgraph.es[\"weight\"]\n                avg_weight = round(np.mean(weights), 3)\n                max_identity = max(weights)\n            else:\n                num_of_edges, num_of_edges_fr, avg_weight, max_identity = \"\", \"\", \"\", \"\"\n            communities_annot_rows.append([community_index, community_size, avg_weight, max_identity,\n                                           num_of_edges_fr, num_of_edges, \";\".join(proteomes)])\n        communities_annot = pd.DataFrame(communities_annot_rows, columns=[\"id\", \"size\", \"avg_weight\", \"max_weight\",\n                                                                          \"fr_edges\", \"n_edges\", \"proteomes\"])\n        communities_annot.to_csv(os.path.join(os.path.join(self.prms.args[\"output_dir\"],\n                                                           \"proteome_communities.tsv\")), sep=\"\\t\", index=False)\n        communities_annot = communities_annot[communities_annot[\"size\"] &gt;=\n                                              self.prms.args[\"min_proteome_community_size\"]]\n        self.communities_annot = communities_annot.set_index(\"id\")\n        self.annotation = self.annotation.drop(sequences_to_drop)\n        self.proteomes = self.proteomes.drop(sequences_to_drop)\n        self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n        self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(communities_annot.index)} proteomes communities with size &gt;= \"\n                  f\"{self.prms.args['min_proteome_community_size']} were taken for further analysis\",\n                  file=sys.stdout)\n            print(f\"  \u29bf {len(sequences_to_drop)} proteomes from smaller communities were excluded from the \"\n                  f\"analysis\", file=sys.stdout)\n        if len(communities_annot.index) == 0:\n            print(\"\u25cb Termination since no proteome community was taken for further analysis\")\n            sys.exit()\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to find proteome communities.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.load_sequences_from_extended_gff","title":"<code>load_sequences_from_extended_gff(input_f, genome_annotation=None)</code>","text":"<p>Load proteomes from gff files.</p> <p>Parameters:</p> <ul> <li> <code>input_f</code>             (<code>str | list</code>)         \u2013          <p>List of file paths or path to a folder with gff files.</p> </li> <li> <code>genome_annotation</code>             (<code>path</code>, default:                 <code>None</code> )         \u2013          <p>Path to a table with annotation of genome circularity. Format: two columns with names: id, circular; tab-separated, 1,0 values.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def load_sequences_from_extended_gff(self, input_f: typing.Union[str, list], genome_annotation=None) -&gt; None:\n    \"\"\"Load proteomes from gff files.\n\n    Arguments:\n        input_f (str | list): List of file paths or path to a folder with gff files.\n        genome_annotation (path): Path to a table with annotation of genome circularity.\n            Format: two columns with names: id, circular; tab-separated, 1,0 values.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if isinstance(input_f, str):\n            input_folder = input_f\n            if not os.path.exists(input_folder):\n                raise ilund4u.manager.ilund4uError(f\"Folder {input_folder} does not exist.\")\n            gff_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder)]\n        elif isinstance(input_f, list):\n            gff_files = input_f\n        else:\n            raise ilund4u.manager.ilund4uError(f\"The input for the GFF parsing function must be either a folder or \"\n                                               f\"a list of files.\")\n        if not gff_files:\n            raise ilund4u.manager.ilund4uError(f\"Folder {input_f} does not contain files.\")\n        if not os.path.exists(self.prms.args[\"output_dir\"]):\n            os.mkdir(self.prms.args[\"output_dir\"])\n        else:\n            if os.path.exists(self.proteins_fasta_file):\n                os.remove(self.proteins_fasta_file)\n        genome_circularity_dict = dict()\n        if genome_annotation:\n            try:\n                genome_annotation_table = pd.read_table(genome_annotation, sep=\"\\t\").set_index(\"id\")\n                genome_circularity_dict = genome_annotation_table[\"circular\"].to_dict()\n            except:\n                raise ilund4u.manager.ilund4uError(\"\u25cb Warning: unable to read genome annotation table. \"\n                                                   \"Check the format.\")\n        num_of_gff_files = len(gff_files)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Reading gff file{'s' if len(gff_files) &gt; 1 else ''}...\", file=sys.stdout)\n        if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=num_of_gff_files, suffix='%(index)d/%(max)d')\n        proteome_list, annotation_rows = [], []\n        gff_records_batch = []\n        for gff_file_index, gff_file_path in enumerate(gff_files):\n            try:\n                if num_of_gff_files &gt; 1 and self.prms.args[\"verbose\"]:\n                    bar.next()\n                gff_records = list(BCBio.GFF.parse(gff_file_path, limit_info=dict(gff_type=[\"CDS\"])))\n                if len(gff_records) != 1:\n                    print(f\"\\n\u25cb Warning: gff file {gff_file_path} contains information for more than 1 \"\n                          f\"sequence. File will be skipped.\")\n                    continue\n                current_gff_records = []\n                gff_record = gff_records[0]\n                try:\n                    record_locus_sequence = gff_record.seq\n                except Bio.Seq.UndefinedSequenceError as error:\n                    raise ilund4u.manager.ilund4uError(f\"gff file doesn't contain corresponding \"\n                                                       f\"sequences.\") from error\n                if self.prms.args[\"use_filename_as_contig_id\"]:\n                    gff_record.id = os.path.splitext(os.path.basename(gff_file_path))[0]\n                features_ids = [i.id for i in gff_record.features]\n                if len(features_ids) != len(set(features_ids)):\n                    raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains duplicated feature \"\n                                                       f\"ids while only unique are allowed.\")\n                if len(features_ids) &gt; self.prms.args[\"min_proteome_size\"]:\n                    if gff_record.id in genome_circularity_dict.keys():\n                        circular = int(genome_circularity_dict[gff_record.id])\n                    else:\n                        circular = int(self.prms.args[\"circular_genomes\"])\n                    record_proteome = Proteome(proteome_id=gff_record.id, gff_file=gff_file_path, cdss=pd.Series(),\n                                               circular=circular)\n                    record_cdss = []\n                    all_defined = True\n                    for gff_feature in gff_record.features:\n                        cds_id = gff_feature.id.replace(\";\", \",\")\n                        if gff_record.id not in cds_id:\n                            cds_id = f\"{gff_record.id}-{cds_id}\"  # Attention\n                        transl_table = self.prms.args[\"default_transl_table\"]\n                        if \"transl_table\" in gff_feature.qualifiers.keys():\n                            transl_table = int(gff_feature.qualifiers[\"transl_table\"][0])\n                        name = \"\"\n                        if self.prms.args[\"gff_CDS_name_source\"] in gff_feature.qualifiers:\n                            name = gff_feature.qualifiers[self.prms.args[\"gff_CDS_name_source\"]][0]\n\n                        sequence = gff_feature.translate(record_locus_sequence, table=transl_table, cds=False)[:-1]\n\n                        if not sequence.defined:\n                            all_defined = False\n                            continue\n\n                        current_gff_records.append(Bio.SeqRecord.SeqRecord(seq=sequence, id=cds_id, description=\"\"))\n                        cds = CDS(cds_id=cds_id, proteome_id=gff_record.id,\n                                  start=int(gff_feature.location.start) + 1, end=int(gff_feature.location.end),\n                                  strand=gff_feature.location.strand, name=name)\n                        record_cdss.append(cds)\n                    if all_defined:\n                        gff_records_batch += current_gff_records\n                        record_proteome.cdss = pd.Series(record_cdss, index=[cds.cds_id for cds in record_cdss])\n                        proteome_list.append(record_proteome)\n                        annotation_rows.append(dict(id=gff_record.id, length=len(gff_record.seq),\n                                                    proteome_size=len(features_ids),\n                                                    proteome_size_unique=\"\", protein_clusters=\"\"))\n                    else:\n                        raise ilund4u.manager.ilund4uError(f\"Gff file {gff_file_path} contains not defined feature\")\n                if gff_file_index % 1000 == 0 or gff_file_index == num_of_gff_files - 1:\n                    with open(self.proteins_fasta_file, \"a\") as handle:\n                        Bio.SeqIO.write(gff_records_batch, handle, \"fasta\")\n                    gff_records_batch = []\n            except:\n                print(f\"\u25cb Warning: gff file {gff_file_path} was not read properly and skipped\")\n                if self.prms.args[\"parsing_debug\"]:\n                    self.prms.args[\"debug\"] = True\n                    raise ilund4u.manager.ilund4uError(\"Gff file {gff_file_path} was not read properly\")\n        if len(gff_files) &gt; 1 and self.prms.args[\"verbose\"]:\n            bar.finish()\n        proteome_ids = [pr.proteome_id for pr in proteome_list]\n        if len(proteome_ids) != len(set(proteome_ids)):\n            raise ilund4u.manager.ilund4uError(f\"The input gff files have duplicated contig ids.\\n  \"\n                                               f\"You can use `--use-filename-as-id` parameter to use file name \"\n                                               f\"as contig id which can help to fix the problem.\")\n        self.proteomes = pd.Series(proteome_list, index=[pr.proteome_id for pr in proteome_list])\n        self.annotation = pd.DataFrame(annotation_rows).set_index(\"id\")\n        self.__col_to_ind = {col: idx for idx, col in enumerate(self.annotation.columns)}\n        self.annotation = self.annotation.sort_values(by=\"proteome_size\")\n        self.proteomes = self.proteomes.loc[self.annotation.index]\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {len(proteome_list)} {'locus was' if len(proteome_list) == 1 else 'loci were'} loaded from\"\n                  f\" the gff files folder\", file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to load proteomes from gff files.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.mmseqs_cluster","title":"<code>mmseqs_cluster()</code>","text":"<p>Cluster all proteins using mmseqs in order to define groups of homologues.</p> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>protein id to cluster id dictionary.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def mmseqs_cluster(self) -&gt; dict:\n    \"\"\"Cluster all proteins using mmseqs in order to define groups of homologues.\n\n    Returns:\n        dict: protein id to cluster id dictionary.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Running mmseqs for protein clustering...\", file=sys.stdout)\n        mmseqs_input = self.proteins_fasta_file\n        mmseqs_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"mmseqs\")\n        if os.path.exists(mmseqs_output_folder):\n            shutil.rmtree(mmseqs_output_folder)\n        os.mkdir(mmseqs_output_folder)\n        mmseqs_output_folder_db = os.path.join(mmseqs_output_folder, \"DB\")\n        os.mkdir(mmseqs_output_folder_db)\n        mmseqs_stdout = open(os.path.join(mmseqs_output_folder, \"mmseqs_stdout.txt\"), \"w\")\n        mmseqs_stderr = open(os.path.join(mmseqs_output_folder, \"mmseqs_stderr.txt\"), \"w\")\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\", mmseqs_input,\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\")], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"cluster\",\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"tmp\"),\n                        \"--cluster-mode\", str(self.prms.args[\"mmseqs_cluster_mode\"]),\n                        \"--cov-mode\", str(self.prms.args[\"mmseqs_cov_mode\"]),\n                        \"--min-seq-id\", str(self.prms.args[\"mmseqs_min_seq_id\"]),\n                        \"-c\", str(self.prms.args[\"mmseqs_c\"]),\n                        \"-s\", str(self.prms.args[\"mmseqs_s\"])], stdout=mmseqs_stdout,\n                       stderr=mmseqs_stderr)  # threads!\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createtsv\",\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"sequencesDB\"),\n                        os.path.join(mmseqs_output_folder_db, \"clusterDB\"),\n                        os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\")],\n                       stdout=mmseqs_stdout, stderr=mmseqs_stderr)\n        mmseqs_clustering_results = pd.read_table(os.path.join(mmseqs_output_folder, \"mmseqs_clustering.tsv\"),\n                                                  sep=\"\\t\", header=None, names=[\"cluster\", \"protein_id\"])\n        mmseqs_clustering_results = mmseqs_clustering_results.set_index(\"protein_id\")[\"cluster\"].to_dict()\n        num_of_unique_clusters = len(set(mmseqs_clustering_results.values()))\n        num_of_proteins = len(mmseqs_clustering_results.keys())\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf {num_of_unique_clusters} clusters for {num_of_proteins} proteins were found with mmseqs\\n\"\n                  f\"  \u29bf mmseqs clustering results were saved to \"\n                  f\"{os.path.join(mmseqs_output_folder, 'mmseqs_clustering.tsv')}\", file=sys.stdout)\n        return mmseqs_clustering_results\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to run mmseqs clustering.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.process_mmseqs_results","title":"<code>process_mmseqs_results(mmseqs_results)</code>","text":"<p>Process results of mmseqs clustering run.</p> <p>Parameters:</p> <ul> <li> <code>mmseqs_results</code>             (<code>dict</code>)         \u2013          <p>results of mmseqs_cluster function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (            <code>dict</code> )        \u2013          <p>dictionary with protein cluster id to list of protein ids items.</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def process_mmseqs_results(self, mmseqs_results: dict) -&gt; dict:\n    \"\"\"Process results of mmseqs clustering run.\n\n    Arguments:\n        mmseqs_results (dict): results of mmseqs_cluster function.\n\n    Returns:\n        dict: dictionary with protein cluster id to list of protein ids items.\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Processing mmseqs results ...\", file=sys.stdout)\n        sequences_to_drop, drop_reason = [], []\n        current_p_length, cpl_added_proteomes = None, None\n        cluster_to_sequences = collections.defaultdict(list)\n        if self.prms.args[\"verbose\"]:\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.index), suffix='%(index)d/%(max)d')\n        for p_index, proteome in enumerate(self.proteomes.to_list()):\n            if self.prms.args[\"verbose\"]:\n                bar.next()\n            seq_p_size = self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size\"]]\n            if seq_p_size != current_p_length:\n                current_p_length = seq_p_size\n                cpl_added_proteomes = []\n            seq_protein_clusters = []\n            for cds in proteome.cdss.to_list():\n                cds.group = mmseqs_results[cds.cds_id]\n                seq_protein_clusters.append(cds.group)\n            seq_protein_clusters_set = set(seq_protein_clusters)\n            unique_p_size = len(seq_protein_clusters_set)\n            if seq_protein_clusters_set in cpl_added_proteomes:\n                sequences_to_drop.append(proteome.proteome_id)\n                drop_reason.append(f\"Duplicate\")\n                continue\n            if unique_p_size / seq_p_size &lt; self.prms.args[\"proteome_uniqueness_cutoff\"]:\n                sequences_to_drop.append(proteome.proteome_id)\n                drop_reason.append(\"Proteome uniqueness cutoff\")\n                continue\n            self.annotation.iat[p_index, self.__col_to_ind[\"proteome_size_unique\"]] = unique_p_size\n            self.annotation.iat[p_index, self.__col_to_ind[\"protein_clusters\"]] = list(seq_protein_clusters_set)\n            cpl_added_proteomes.append(seq_protein_clusters_set)\n            for p_cluster in seq_protein_clusters_set:\n                cluster_to_sequences[p_cluster].append(proteome.proteome_id)\n        dropped_sequences = pd.DataFrame(dict(sequence=sequences_to_drop, reason=drop_reason))\n        dropped_sequences.to_csv(os.path.join(self.prms.args[\"output_dir\"], \"dropped_sequences.tsv\"), sep=\"\\t\",\n                                 index=False)\n        if self.prms.args[\"verbose\"]:\n            bar.finish()\n            print(f\"  \u29bf {len(sequences_to_drop)} proteomes were excluded after proteome\"\n                  f\" deduplication and filtering\", file=sys.stdout)\n        self.annotation = self.annotation.drop(sequences_to_drop)\n        self.proteomes = self.proteomes.drop(sequences_to_drop)\n        self.annotation = self.annotation.sort_values(by=\"proteome_size_unique\")\n        self.proteomes = self.proteomes.loc[self.annotation.index]\n        self.annotation[\"index\"] = list(range(len(self.proteomes.index)))\n        self.seq_to_ind = {sid: idx for idx, sid in enumerate(self.annotation.index)}\n        return cluster_to_sequences\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to process mmseqs output.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.data_processing.Proteomes.save_as_db","title":"<code>save_as_db(db_folder)</code>","text":"<p>Save Proteomes to the iLnd4u database.</p> <p>Parameters:</p> <ul> <li> <code>db_folder</code>             (<code>str</code>)         \u2013          <p>Database folder path.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/data_processing.py</code> <pre><code>def save_as_db(self, db_folder: str) -&gt; None:\n    \"\"\"Save Proteomes to the iLnd4u database.\n\n    Arguments:\n        db_folder (str): Database folder path.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        attributes_to_ignore = [\"proteomes\", \"annotation\", \"communities_annot\", \"prms\"]\n        attributes = {k: v for k, v in self.__dict__.items() if k not in attributes_to_ignore}\n        attributes[\"proteins_fasta_file\"] = os.path.basename(attributes[\"proteins_fasta_file\"])\n        with open(os.path.join(db_folder, \"proteomes.attributes.json\"), 'w') as json_file:\n            json.dump(attributes, json_file)\n        self.communities_annot.to_csv(os.path.join(db_folder, \"proteomes.communities_annot.tsv\"), sep=\"\\t\",\n                                      index_label=\"id\")\n        self.annotation[\"protein_clusters\"] = self.annotation[\"protein_clusters\"].apply(lambda x: \";\".join(x))\n        self.annotation.to_csv(os.path.join(db_folder, \"proteomes.annotations.tsv\"), sep=\"\\t\",\n                               index_label=\"proteome_id\")\n        os.mkdir(os.path.join(db_folder, \"gff\"))\n        proteome_db_ind, cdss_db_ind, islands_db_ind, cds_ids, repr_cds_ids = [], [], [], [], set()\n        for community, proteomes in self.communities.items():\n            for proteome_id in proteomes:\n                proteome = self.proteomes.at[proteome_id]\n                proteome_db_ind.append(proteome.get_proteome_db_row())\n                os.system(f\"cp '{proteome.gff_file}' {os.path.join(db_folder, 'gff')}/\")\n                for cds in proteome.cdss.to_list():\n                    cds_ids.append(cds.cds_id)\n                    cdss_db_ind.append(cds.get_cds_db_row())\n                    repr_cds_ids.add(cds.group)\n                for island in proteome.islands.to_list():\n                    islands_db_ind.append(island.get_island_db_row())\n\n        with open(os.path.join(db_folder, \"proteome.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(proteome_db_ind, json_file)\n        with open(os.path.join(db_folder, \"cds.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(cdss_db_ind, json_file)\n        with open(os.path.join(db_folder, \"island.ind.attributes.json\"), \"w\") as json_file:\n            json.dump(islands_db_ind, json_file)\n\n        initial_fasta_file = Bio.SeqIO.index(self.proteins_fasta_file, \"fasta\")\n        with open(os.path.join(db_folder, attributes[\"proteins_fasta_file\"]), \"wb\") as out_handle:\n            for acc in cds_ids:\n                out_handle.write(initial_fasta_file.get_raw(acc))\n\n        with open(os.path.join(db_folder, \"representative_seqs.fa\"), \"wb\") as out_handle:\n            for acc in repr_cds_ids:\n                out_handle.write(initial_fasta_file.get_raw(acc))\n\n        mmseqs_db_folder = os.path.join(db_folder, \"mmseqs_db\")\n        if os.path.exists(mmseqs_db_folder):\n            shutil.rmtree(mmseqs_db_folder)\n        os.mkdir(mmseqs_db_folder)\n        subprocess.run([self.prms.args[\"mmseqs_binary\"], \"createdb\",\n                        os.path.join(db_folder, attributes[\"proteins_fasta_file\"]),\n                        os.path.join(mmseqs_db_folder, \"all_proteins\")],\n                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to write Proteomes to the database.\") from error\n</code></pre>"},{"location":"API/package/#manager-module","title":"manager module","text":"<p>This module provides managing classes and methods for the tool.</p>"},{"location":"API/package/#ilund4u.manager.Parameters","title":"<code>Parameters</code>","text":"<p>A Parameters object holds and parse command line and config arguments.</p> <p>A Parameters object have to be created in each script since it's used almost by each     class of the tool as a mandatory argument.</p> <p>Attributes:</p> <ul> <li> <code>args</code>             (<code>dict</code>)         \u2013          <p>dictionary that holds all arguments.</p> </li> <li> <code>cmd_arguments</code>             (<code>dict</code>)         \u2013          <p>dictionary wich command-line arguments.</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>class Parameters:\n    \"\"\"A Parameters object holds and parse command line and config arguments.\n\n    A Parameters object have to be created in each script since it's used almost by each\n        class of the tool as a mandatory argument.\n\n    Attributes:\n        args (dict): dictionary that holds all arguments.\n        cmd_arguments (dict): dictionary wich command-line arguments.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Parameters class constructor.\n\n        \"\"\"\n        self.args = dict(debug=True)\n        self.cmd_arguments = dict()\n\n    def parse_cmd_arguments(self) -&gt; None:\n        \"\"\"Parse command-line arguments.\n\n        Returns:\n            None\n\n        \"\"\"\n        parser = argparse.ArgumentParser(prog=\"ilund4u\", add_help=False)\n        parser.add_argument(\"-data\", \"--data\", dest=\"ilund4u_data\", action=\"store_true\")\n        parser.add_argument(\"-get-hmms\", \"--get-hmms\", dest=\"get_hmms\", action=\"store_true\")\n        parser.add_argument(\"-database\", \"--database\", dest=\"get_database\", default=None, type=str,\n                            choices=[\"phages\", \"plasmids\"])\n\n        parser.add_argument(\"-linux\", \"--linux\", dest=\"linux\", action=\"store_true\", default=None)\n        parser.add_argument(\"-mac\", \"--mac\", dest=\"mac\", action=\"store_true\", default=None)\n        parser.add_argument(\"-h\", \"--help\", dest=\"help\", action=\"store_true\")\n        parser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 0.0.8\")\n\n        subparsers = parser.add_subparsers(dest=\"mode\")\n\n        parser_hm = subparsers.add_parser(\"hotspots\")\n        parser_hm.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n        parser_hm.add_argument(\"-ufid\", \"--use-filename-as-id\", dest=\"use_filename_as_contig_id\", action=\"store_true\",\n                               default=None)\n        parser_hm.add_argument(\"-mps\", \"--min-proteome-size\", dest=\"min_proteome_size\", type=int, default=None)\n        parser_hm.add_argument(\"-gct\", \"--genome-circularity-table\", dest=\"genome_annotation\", type=str, default=None)\n        parser_hm.add_argument(\"-psc\", \"--proteome-sim-cutoff\", dest=\"proteome_similarity_cutoff\", type=float,\n                               default=None)\n        parser_hm.add_argument(\"-mpcs\", \"--min-proteome-community-size\", dest=\"min_proteome_community_size\", type=int,\n                               default=None)\n        parser_hm.add_argument(\"-vpc\", \"--variable-protein-cutoff\", dest=\"variable_protein_cluster_cutoff\", type=float,\n                               default=None)\n        parser_hm.add_argument(\"-cpc\", \"--conserved-protein-cutoff\", dest=\"conserved_protein_cluster_cutoff\",\n                               type=float, default=None)\n        parser_hm.add_argument(\"-cg\", \"--circular-genomes\", dest=\"circular_genomes\", default=None,\n                               action=\"store_true\")\n        parser_hm.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=None,\n                               action=\"store_false\")\n        parser_hm.add_argument(\"-hpc\", \"--hotspot-presence-cutoff\", dest=\"hotspot_presence_cutoff\",\n                               type=float, default=None)\n        parser_hm.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_hm.add_argument(\"-o-db\", \"--output-database\", dest=\"output_database\", type=str, default=None)\n        parser_hm.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_hm.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_hm.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_hm.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n        parser_hm.add_argument(\"--parsing-debug\", \"-parsing-debug\", dest=\"parsing_debug\", action=\"store_true\")\n\n        parser_ps = subparsers.add_parser(\"protein\")\n        parser_ps.add_argument(\"-fa\", \"--fa\", dest=\"fa\", type=str, default=None)\n        parser_ps.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n        parser_ps.add_argument(\"-ql\", \"--query-label\", dest=\"query_label\", type=str, default=None)\n        parser_ps.add_argument(\"-hsm\", \"--homology-search-mode\", dest=\"protein_search_target_mode\", type=str,\n                               choices=[\"group\", \"proteins\"], default=\"group\")\n        parser_ps.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n        parser_ps.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n        parser_ps.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n        parser_ps.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n        parser_ps.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                               default=None)\n        parser_ps.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_ps.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_ps.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_ps.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_ps.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n        parser_pa = subparsers.add_parser(\"proteome\")\n        parser_pa.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n        parser_pa.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n        parser_pa.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=True,\n                               action=\"store_false\")\n        parser_pa.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n        parser_pa.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n        parser_pa.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n        parser_pa.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n        parser_pa.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                               default=None)\n        parser_pa.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                               default=None)\n        parser_pa.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n        parser_pa.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n        parser_pa.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n        parser_pa.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n        args = vars(parser.parse_args())\n        if len(sys.argv[1:]) == 0:\n            args[\"help\"] = True\n        if args[\"ilund4u_data\"]:\n            ilund4u.methods.copy_package_data()\n            sys.exit()\n        if args[\"linux\"]:\n            ilund4u.methods.adjust_paths(\"linux\")\n            sys.exit()\n        if args[\"mac\"]:\n            ilund4u.methods.adjust_paths(\"mac\")\n            sys.exit()\n        if args[\"get_hmms\"]:\n            self.load_config()\n            ilund4u.methods.get_HMM_models(self.args)\n            sys.exit()\n        if args[\"get_database\"]:\n            self.load_config()\n            ilund4u.methods.get_ilund4u_db(self.args, args[\"get_database\"])\n            sys.exit()\n        if args[\"help\"]:\n            if not args[\"mode\"]:\n                help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"help_main.txt\")\n            else:\n                help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", f\"help_{args['mode']}.txt\")\n            with open(help_message_path, \"r\") as help_message:\n                print(help_message.read(), file=sys.stdout)\n                sys.exit()\n        if args[\"mode\"] == \"hotspots\":\n            if not args[\"gff\"]:\n                raise ilund4uError(\"-gff argument is required for hotspots mode.\")\n        elif args[\"mode\"] == \"protein\":\n            if not args[\"fa\"] or not args[\"database\"]:\n                raise ilund4uError(\"Both -fa/--fa and -db/--database arguments are required for protein mode.\")\n        elif args[\"mode\"] == \"proteome\":\n            if not args[\"gff\"] or not args[\"database\"]:\n                raise ilund4uError(\"Both -gff/--gff and -db/--database arguments are required for protein mode.\")\n\n        args_to_keep = [\"gff\", \"output_database\", \"query_label\", \"genome_annotation\"]\n        filtered_args = {k: v for k, v in args.items() if v is not None or k in args_to_keep}\n        self.cmd_arguments = filtered_args\n        return None\n\n    def load_config(self, path: str = \"standard\") -&gt; None:\n        \"\"\"Load configuration file.\n\n        Arguments\n            path (str): path to a config file or name (only standard available at this moment).\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if path == \"standard\":\n                path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"standard.cfg\")\n            config = configs.load(path).get_config()\n            internal_dir = os.path.dirname(__file__)\n            for key in config[\"root\"].keys():\n                if type(config[\"root\"][key]) is str and \"{internal}\" in config[\"root\"][key]:\n                    config[\"root\"][key] = config[\"root\"][key].replace(\"{internal}\",\n                                                                      os.path.join(internal_dir, \"ilund4u_data\"))\n            config[\"root\"][\"output_dir\"] = config[\"root\"][\"output_dir\"].replace(\"{current_date}\",\n                                                                                time.strftime(\"%Y_%m_%d-%H_%M\"))\n            keys_to_transform_to_list = []\n            for ktl in keys_to_transform_to_list:\n                if isinstance(config[\"root\"][ktl], str):\n                    if config[\"root\"][ktl] != \"None\":\n                        config[\"root\"][ktl] = [config[\"root\"][ktl]]\n                    else:\n                        config[\"root\"][ktl] = []\n            self.args.update(config[\"root\"])\n            if self.cmd_arguments:\n                self.args.update(self.cmd_arguments)\n            return None\n        except Exception as error:\n            raise ilund4uError(\"Unable to parse the specified config file. Please check your config file \"\n                               \"or provided name.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.manager.Parameters.__init__","title":"<code>__init__()</code>","text":"<p>Parameters class constructor.</p> Source code in <code>ilund4u/manager.py</code> <pre><code>def __init__(self):\n    \"\"\"Parameters class constructor.\n\n    \"\"\"\n    self.args = dict(debug=True)\n    self.cmd_arguments = dict()\n</code></pre>"},{"location":"API/package/#ilund4u.manager.Parameters.load_config","title":"<code>load_config(path='standard')</code>","text":"<p>Load configuration file.</p> <p>Arguments     path (str): path to a config file or name (only standard available at this moment).</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>def load_config(self, path: str = \"standard\") -&gt; None:\n    \"\"\"Load configuration file.\n\n    Arguments\n        path (str): path to a config file or name (only standard available at this moment).\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if path == \"standard\":\n            path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"standard.cfg\")\n        config = configs.load(path).get_config()\n        internal_dir = os.path.dirname(__file__)\n        for key in config[\"root\"].keys():\n            if type(config[\"root\"][key]) is str and \"{internal}\" in config[\"root\"][key]:\n                config[\"root\"][key] = config[\"root\"][key].replace(\"{internal}\",\n                                                                  os.path.join(internal_dir, \"ilund4u_data\"))\n        config[\"root\"][\"output_dir\"] = config[\"root\"][\"output_dir\"].replace(\"{current_date}\",\n                                                                            time.strftime(\"%Y_%m_%d-%H_%M\"))\n        keys_to_transform_to_list = []\n        for ktl in keys_to_transform_to_list:\n            if isinstance(config[\"root\"][ktl], str):\n                if config[\"root\"][ktl] != \"None\":\n                    config[\"root\"][ktl] = [config[\"root\"][ktl]]\n                else:\n                    config[\"root\"][ktl] = []\n        self.args.update(config[\"root\"])\n        if self.cmd_arguments:\n            self.args.update(self.cmd_arguments)\n        return None\n    except Exception as error:\n        raise ilund4uError(\"Unable to parse the specified config file. Please check your config file \"\n                           \"or provided name.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.manager.Parameters.parse_cmd_arguments","title":"<code>parse_cmd_arguments()</code>","text":"<p>Parse command-line arguments.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/manager.py</code> <pre><code>def parse_cmd_arguments(self) -&gt; None:\n    \"\"\"Parse command-line arguments.\n\n    Returns:\n        None\n\n    \"\"\"\n    parser = argparse.ArgumentParser(prog=\"ilund4u\", add_help=False)\n    parser.add_argument(\"-data\", \"--data\", dest=\"ilund4u_data\", action=\"store_true\")\n    parser.add_argument(\"-get-hmms\", \"--get-hmms\", dest=\"get_hmms\", action=\"store_true\")\n    parser.add_argument(\"-database\", \"--database\", dest=\"get_database\", default=None, type=str,\n                        choices=[\"phages\", \"plasmids\"])\n\n    parser.add_argument(\"-linux\", \"--linux\", dest=\"linux\", action=\"store_true\", default=None)\n    parser.add_argument(\"-mac\", \"--mac\", dest=\"mac\", action=\"store_true\", default=None)\n    parser.add_argument(\"-h\", \"--help\", dest=\"help\", action=\"store_true\")\n    parser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"%(prog)s 0.0.8\")\n\n    subparsers = parser.add_subparsers(dest=\"mode\")\n\n    parser_hm = subparsers.add_parser(\"hotspots\")\n    parser_hm.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n    parser_hm.add_argument(\"-ufid\", \"--use-filename-as-id\", dest=\"use_filename_as_contig_id\", action=\"store_true\",\n                           default=None)\n    parser_hm.add_argument(\"-mps\", \"--min-proteome-size\", dest=\"min_proteome_size\", type=int, default=None)\n    parser_hm.add_argument(\"-gct\", \"--genome-circularity-table\", dest=\"genome_annotation\", type=str, default=None)\n    parser_hm.add_argument(\"-psc\", \"--proteome-sim-cutoff\", dest=\"proteome_similarity_cutoff\", type=float,\n                           default=None)\n    parser_hm.add_argument(\"-mpcs\", \"--min-proteome-community-size\", dest=\"min_proteome_community_size\", type=int,\n                           default=None)\n    parser_hm.add_argument(\"-vpc\", \"--variable-protein-cutoff\", dest=\"variable_protein_cluster_cutoff\", type=float,\n                           default=None)\n    parser_hm.add_argument(\"-cpc\", \"--conserved-protein-cutoff\", dest=\"conserved_protein_cluster_cutoff\",\n                           type=float, default=None)\n    parser_hm.add_argument(\"-cg\", \"--circular-genomes\", dest=\"circular_genomes\", default=None,\n                           action=\"store_true\")\n    parser_hm.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=None,\n                           action=\"store_false\")\n    parser_hm.add_argument(\"-hpc\", \"--hotspot-presence-cutoff\", dest=\"hotspot_presence_cutoff\",\n                           type=float, default=None)\n    parser_hm.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_hm.add_argument(\"-o-db\", \"--output-database\", dest=\"output_database\", type=str, default=None)\n    parser_hm.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_hm.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_hm.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_hm.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n    parser_hm.add_argument(\"--parsing-debug\", \"-parsing-debug\", dest=\"parsing_debug\", action=\"store_true\")\n\n    parser_ps = subparsers.add_parser(\"protein\")\n    parser_ps.add_argument(\"-fa\", \"--fa\", dest=\"fa\", type=str, default=None)\n    parser_ps.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n    parser_ps.add_argument(\"-ql\", \"--query-label\", dest=\"query_label\", type=str, default=None)\n    parser_ps.add_argument(\"-hsm\", \"--homology-search-mode\", dest=\"protein_search_target_mode\", type=str,\n                           choices=[\"group\", \"proteins\"], default=\"group\")\n    parser_ps.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n    parser_ps.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n    parser_ps.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n    parser_ps.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n    parser_ps.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                           default=None)\n    parser_ps.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_ps.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_ps.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_ps.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_ps.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n    parser_pa = subparsers.add_parser(\"proteome\")\n    parser_pa.add_argument(\"-gff\", \"--gff\", dest=\"gff\", type=str, default=None)\n    parser_pa.add_argument(\"-db\", \"--database\", dest=\"database\", type=str, default=None)\n    parser_pa.add_argument(\"-ncg\", \"--non-circular-genomes\", dest=\"circular_genomes\", default=True,\n                           action=\"store_false\")\n    parser_pa.add_argument(\"-msqc\", \"--mmseqs-query-cov\", dest=\"mmseqs_search_qcov\", type=float, default=None)\n    parser_pa.add_argument(\"-mstc\", \"--mmseqs-target-cov\", dest=\"mmseqs_search_tcov\", type=float, default=None)\n    parser_pa.add_argument(\"-msf\", \"--mmseqs-fident\", dest=\"mmseqs_search_fident\", type=float, default=None)\n    parser_pa.add_argument(\"-mse\", \"--mmseqs-evalue\", dest=\"mmseqs_search_evalue\", type=float, default=None)\n    parser_pa.add_argument(\"-fm\", \"--fast-mmseqs\", dest=\"fast_mmseqs_search_mode\", action=\"store_true\",\n                           default=None)\n    parser_pa.add_argument(\"-rnf\", \"--report-not-flanked\", dest=\"report_not_flanked\", action=\"store_true\",\n                           default=None)\n    parser_pa.add_argument(\"-c\", dest=\"config_file\", type=str, default=\"standard\")\n    parser_pa.add_argument(\"-o\", dest=\"output_dir\", type=str, default=None)\n    parser_pa.add_argument(\"-q\", \"--quiet\", dest=\"verbose\", default=True, action=\"store_false\")\n    parser_pa.add_argument(\"--debug\", \"-debug\", dest=\"debug\", action=\"store_true\")\n\n    args = vars(parser.parse_args())\n    if len(sys.argv[1:]) == 0:\n        args[\"help\"] = True\n    if args[\"ilund4u_data\"]:\n        ilund4u.methods.copy_package_data()\n        sys.exit()\n    if args[\"linux\"]:\n        ilund4u.methods.adjust_paths(\"linux\")\n        sys.exit()\n    if args[\"mac\"]:\n        ilund4u.methods.adjust_paths(\"mac\")\n        sys.exit()\n    if args[\"get_hmms\"]:\n        self.load_config()\n        ilund4u.methods.get_HMM_models(self.args)\n        sys.exit()\n    if args[\"get_database\"]:\n        self.load_config()\n        ilund4u.methods.get_ilund4u_db(self.args, args[\"get_database\"])\n        sys.exit()\n    if args[\"help\"]:\n        if not args[\"mode\"]:\n            help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", \"help_main.txt\")\n        else:\n            help_message_path = os.path.join(os.path.dirname(__file__), \"ilund4u_data\", f\"help_{args['mode']}.txt\")\n        with open(help_message_path, \"r\") as help_message:\n            print(help_message.read(), file=sys.stdout)\n            sys.exit()\n    if args[\"mode\"] == \"hotspots\":\n        if not args[\"gff\"]:\n            raise ilund4uError(\"-gff argument is required for hotspots mode.\")\n    elif args[\"mode\"] == \"protein\":\n        if not args[\"fa\"] or not args[\"database\"]:\n            raise ilund4uError(\"Both -fa/--fa and -db/--database arguments are required for protein mode.\")\n    elif args[\"mode\"] == \"proteome\":\n        if not args[\"gff\"] or not args[\"database\"]:\n            raise ilund4uError(\"Both -gff/--gff and -db/--database arguments are required for protein mode.\")\n\n    args_to_keep = [\"gff\", \"output_database\", \"query_label\", \"genome_annotation\"]\n    filtered_args = {k: v for k, v in args.items() if v is not None or k in args_to_keep}\n    self.cmd_arguments = filtered_args\n    return None\n</code></pre>"},{"location":"API/package/#ilund4u.manager.ilund4uError","title":"<code>ilund4uError</code>","text":"<p>             Bases: <code>Exception</code></p> <p>A class for exceptions parsing inherited from the Exception class.</p> Source code in <code>ilund4u/manager.py</code> <pre><code>class ilund4uError(Exception):\n    \"\"\"A class for exceptions parsing inherited from the Exception class.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"API/package/#data-manager-module","title":"data manager module","text":"<p>This module provides data managing classes and methods for the tool.</p>"},{"location":"API/package/#ilund4u.data_manager.DatabaseManager","title":"<code>DatabaseManager</code>","text":"<p>Manager for loading and building iLund4u database.</p> <p>Attributes:</p> <ul> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>class DatabaseManager:\n    \"\"\"Manager for loading and building iLund4u database.\n\n    Attributes:\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n\n    def __init__(self, parameters: ilund4u.manager.Parameters):\n        \"\"\"DatabaseManager class constructor.\n\n        Arguments:\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.prms = parameters\n\n    def build_database(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                       db_path: str) -&gt; None:\n        \"\"\"Write database.\n\n        Arguments:\n            proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n            hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n            db_path (str): Path to the database folder.\n\n        Returns:\n\n        \"\"\"\n        if os.path.exists(db_path):\n            if self.prms.args[\"verbose\"]:\n                print(\"\u25cb Warning: database folder will be rewritten.\")\n            shutil.rmtree(db_path)\n        os.mkdir(db_path)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Database building...\", file=sys.stdout)\n        proteomes.save_as_db(db_path)\n        hotspots.save_as_db(db_path)\n        database_info_txt = f\"Date and time of building: {time.strftime('%Y.%m.%d-%H:%M')}\\n\" \\\n                            f\"iLund4u version: {self.prms.args['version']}\"\n        with open(os.path.join(db_path, \"db_info.txt\"), \"w\") as db_info:\n            db_info.write(database_info_txt)\n        with open(os.path.join(db_path, \"parameters.json\"), \"w\") as parameters:\n            json.dump(self.prms.args, parameters)\n        if self.prms.args[\"verbose\"]:\n            print(f\"  \u29bf Database was successfully saved to {db_path}\", file=sys.stdout)\n        return None\n\n    def load_database(self, db_path: str) -&gt; ilund4u.data_processing.Database:\n        \"\"\"Load database from its folder path and create a Database class object.\n\n        Arguments:\n            db_path (str): Path to the pre-built database folder.\n\n        Returns:\n            ilund4u.data_processing.Database: Database class object.\n\n        \"\"\"\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Loading database from {db_path}...\", file=sys.stdout)\n        proteomes = ilund4u.data_processing.Proteomes.db_init(db_path, self.prms)\n        hotspots = ilund4u.data_processing.Hotspots.db_init(db_path, proteomes, self.prms)\n        db_paths = dict(db_path=db_path, rep_fasta=os.path.join(db_path, \"representative_seqs.fa\"),\n                        proteins_db=os.path.join(db_path, \"mmseqs_db\", \"all_proteins\"))\n        if os.path.exists(os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")):\n            db_paths[\"protein_group_stat\"] = os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")\n\n        database = ilund4u.data_processing.Database(proteomes, hotspots, db_paths, self.prms)\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u29bf The {db_path} database was successfully loaded\", file=sys.stdout)\n        return database\n</code></pre>"},{"location":"API/package/#ilund4u.data_manager.DatabaseManager.__init__","title":"<code>__init__(parameters)</code>","text":"<p>DatabaseManager class constructor.</p> <p>Parameters:</p> <ul> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def __init__(self, parameters: ilund4u.manager.Parameters):\n    \"\"\"DatabaseManager class constructor.\n\n    Arguments:\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.prms = parameters\n</code></pre>"},{"location":"API/package/#ilund4u.data_manager.DatabaseManager.build_database","title":"<code>build_database(proteomes, hotspots, db_path)</code>","text":"<p>Write database.</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>Path to the database folder.</p> </li> </ul> <p>Returns:</p> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def build_database(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                   db_path: str) -&gt; None:\n    \"\"\"Write database.\n\n    Arguments:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        db_path (str): Path to the database folder.\n\n    Returns:\n\n    \"\"\"\n    if os.path.exists(db_path):\n        if self.prms.args[\"verbose\"]:\n            print(\"\u25cb Warning: database folder will be rewritten.\")\n        shutil.rmtree(db_path)\n    os.mkdir(db_path)\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u25cb Database building...\", file=sys.stdout)\n    proteomes.save_as_db(db_path)\n    hotspots.save_as_db(db_path)\n    database_info_txt = f\"Date and time of building: {time.strftime('%Y.%m.%d-%H:%M')}\\n\" \\\n                        f\"iLund4u version: {self.prms.args['version']}\"\n    with open(os.path.join(db_path, \"db_info.txt\"), \"w\") as db_info:\n        db_info.write(database_info_txt)\n    with open(os.path.join(db_path, \"parameters.json\"), \"w\") as parameters:\n        json.dump(self.prms.args, parameters)\n    if self.prms.args[\"verbose\"]:\n        print(f\"  \u29bf Database was successfully saved to {db_path}\", file=sys.stdout)\n    return None\n</code></pre>"},{"location":"API/package/#ilund4u.data_manager.DatabaseManager.load_database","title":"<code>load_database(db_path)</code>","text":"<p>Load database from its folder path and create a Database class object.</p> <p>Parameters:</p> <ul> <li> <code>db_path</code>             (<code>str</code>)         \u2013          <p>Path to the pre-built database folder.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Database</code>         \u2013          <p>ilund4u.data_processing.Database: Database class object.</p> </li> </ul> Source code in <code>ilund4u/data_manager.py</code> <pre><code>def load_database(self, db_path: str) -&gt; ilund4u.data_processing.Database:\n    \"\"\"Load database from its folder path and create a Database class object.\n\n    Arguments:\n        db_path (str): Path to the pre-built database folder.\n\n    Returns:\n        ilund4u.data_processing.Database: Database class object.\n\n    \"\"\"\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u25cb Loading database from {db_path}...\", file=sys.stdout)\n    proteomes = ilund4u.data_processing.Proteomes.db_init(db_path, self.prms)\n    hotspots = ilund4u.data_processing.Hotspots.db_init(db_path, proteomes, self.prms)\n    db_paths = dict(db_path=db_path, rep_fasta=os.path.join(db_path, \"representative_seqs.fa\"),\n                    proteins_db=os.path.join(db_path, \"mmseqs_db\", \"all_proteins\"))\n    if os.path.exists(os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")):\n        db_paths[\"protein_group_stat\"] = os.path.join(db_path, \"protein_group_accumulated_statistics.tsv\")\n\n    database = ilund4u.data_processing.Database(proteomes, hotspots, db_paths, self.prms)\n    if self.prms.args[\"verbose\"]:\n        print(f\"\u29bf The {db_path} database was successfully loaded\", file=sys.stdout)\n    return database\n</code></pre>"},{"location":"API/package/#methods-module","title":"methods module","text":"<p>This module provides some methods (e.g. colour transformation, data copying, etc.) used by the tool.</p>"},{"location":"API/package/#ilund4u.methods.adjust_paths","title":"<code>adjust_paths(to)</code>","text":"<p>Change paths in the internal config files for linux or mac.</p> <p>Parameters:</p> <ul> <li> <code>to</code>             (<code>str</code>)         \u2013          <p>mac | linux</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def adjust_paths(to: str) -&gt; None:\n    \"\"\"Change paths in the internal config files for linux or mac.\n\n    Arguments:\n        to (str): mac | linux\n\n    Returns:\n        None\n\n    \"\"\"\n    internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n    config_files = [\"standard.cfg\"]\n    for config_file in config_files:\n        config_file_path = os.path.join(internal_dir, config_file)\n        with open(config_file_path, \"r+\") as config:\n            if to == \"linux\":\n                if not os.path.exists(os.path.join(internal_dir, \"bin/mmseqs_linux\")):\n                    os.system(f\"unzip -q -d {os.path.join(internal_dir, 'bin/')} \"\n                              f\"{os.path.join(internal_dir, 'bin/mmseqs_linux.zip')}\")\n                config_txt = re.sub(r\"mmseqs_mac/bin/mmseqs\", \"mmseqs_linux/bin/mmseqs\", config.read())\n                os.system(\"msa4u --linux &gt;&gt; /dev/null\")\n            else:\n                config_txt = re.sub(r\"mmseqs_linux/bin/mmseqs\", \"mmseqs_mac/bin/mmseqs\", config.read())\n            config.seek(0)\n            config.truncate()\n            config.write(config_txt)\n    print(f\"\u29bf mmseqs path was adjusted to {to}\", file=sys.stdout)\n    return None\n</code></pre>"},{"location":"API/package/#ilund4u.methods.copy_package_data","title":"<code>copy_package_data()</code>","text":"<p>Copy the ilund4u package data folder to your current dir.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def copy_package_data() -&gt; None:\n    \"\"\"Copy the ilund4u package data folder to your current dir.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        users_dir = os.path.join(os.getcwd(), \"ilund4u_data\")\n        internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n        if os.path.exists(users_dir):\n            print(\"Warning: ilund4u_data folder already exists. Remove it or change its name first before \"\n                  \"updating with default.\")\n            return None\n        shutil.copytree(internal_dir, users_dir, ignore=shutil.ignore_patterns(\"help*\", \".*\", \"HMMs*\", \"bin\"))\n        print(\"\u29bf ilund4u_data folder was copied to the current working directory\", file=sys.stdout)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to copy ilund4u folder in your working dir.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.methods.download_file_with_progress","title":"<code>download_file_with_progress(url, local_folder)</code>","text":"<p>Function for downloading a particular file from a web server.</p> <p>Parameters:</p> <ul> <li> <code>url</code>             (<code>str</code>)         \u2013          <p>Link to the file.</p> </li> <li> <code>local_folder</code>             (<code>str</code>)         \u2013          <p>Path to a folder where file will be saved.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def download_file_with_progress(url: str, local_folder: str) -&gt; None:\n    \"\"\"Function for downloading a particular file from a web server.\n\n    Arguments:\n        url (str): Link to the file.\n        local_folder (str): Path to a folder where file will be saved.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        response = requests.head(url)\n        file_size = int(response.headers.get('content-length', 0))\n        # Extract the original file name from the URL\n        file_name = os.path.basename(url)\n        local_path = os.path.join(local_folder, file_name)\n        # Stream the file download and show progress bar\n        with requests.get(url, stream=True) as r, open(local_path, 'wb') as f:\n            bar = progress.bar.FillingCirclesBar(\" \", max=file_size // 8192, suffix='%(percent)d%%')\n            downloaded_size = 0\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    downloaded_size += len(chunk)\n                    f.write(chunk)\n                    bar.next()\n            bar.finish()\n        # Verify that the file was fully downloaded\n        if downloaded_size != file_size:\n            raise ilund4u.manager.ilund4uError(f\"Downloaded file size ({downloaded_size} bytes) does not match \"\n                                               f\"expected size ({file_size} bytes).\")\n        print(f\"\u29bf File was saved to {local_path}\")\n        if file_name.endswith('.tar.gz'):\n            with tarfile.open(local_path, 'r:gz') as tar:\n                tar.extractall(path=local_folder)\n            print(f\"\u29bf Folder was successfully unarchived\")\n            os.remove(local_path)\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to get file from the {url}.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.methods.get_HMM_models","title":"<code>get_HMM_models(parameters)</code>","text":"<p>Download HMM models</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_HMM_models(parameters) -&gt; None:\n    \"\"\"Download HMM models\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        url =  parameters[\"hmm_models\"]\n        internal_dir = os.path.join(os.path.dirname(__file__), \"ilund4u_data\")\n        if os.path.exists(os.path.join(internal_dir, \"HMMs\")):\n            print(f\"\u25cb HMMs folder already exists and will be rewritten...\", file=sys.stdout)\n        # Add checking if it's already downloaded\n        print(f\"\u25cb Downloading HMM models...\\n\"\n              f\"  Source: {url}\", file=sys.stdout)\n        download_file_with_progress(url, internal_dir)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to download HMM models.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.methods.get_color","title":"<code>get_color(name, parameters)</code>","text":"<p>Get HEX color by its name</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>name of a color.</p> </li> <li> <code>parameters</code>             (<code>dict</code>)         \u2013          <p>Parameters' object dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>HEX color.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_color(name: str, parameters: dict) -&gt; str:\n    \"\"\"Get HEX color by its name\n\n    Arguments:\n        name (str): name of a color.\n        parameters (dict): Parameters' object dict.\n\n    Returns:\n        str: HEX color.\n\n    \"\"\"\n    hex_c = parameters.args[\"palette\"][parameters.args[name]]\n    return hex_c\n</code></pre>"},{"location":"API/package/#ilund4u.methods.get_colour_rgba","title":"<code>get_colour_rgba(name, parameters)</code>","text":"<p>Get rgba colour by its name</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>name of a colour.</p> </li> <li> <code>parameters</code>             (<code>dict</code>)         \u2013          <p>Parameters' object dict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (            <code>tuple</code> )        \u2013          <p>RGBA colour</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_colour_rgba(name: str, parameters: dict) -&gt; tuple:\n    \"\"\"Get rgba colour by its name\n\n    Arguments:\n        name (str): name of a colour.\n        parameters (dict): Parameters' object dict.\n\n    Returns:\n        tuple: RGBA colour\n\n    \"\"\"\n    return *matplotlib.colors.hex2color(get_color(name, parameters)), parameters.args[f\"{name}_alpha\"]\n</code></pre>"},{"location":"API/package/#ilund4u.methods.get_ilund4u_db","title":"<code>get_ilund4u_db(parameters, db, path='./')</code>","text":"<p>Download ilund4u database</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def get_ilund4u_db(parameters, db, path = \"./\") -&gt; None:\n    \"\"\"Download ilund4u database\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        url = parameters[f\"{db}_db\"]\n        # Add checking if it's already downloaded\n        print(f\"\u25cb Downloading iLund4u {db} database...\\n\"\n              f\"  Source: {url}\", file=sys.stdout)\n        download_file_with_progress(url, path)\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to download {db} database.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.methods.run_pyhmmer","title":"<code>run_pyhmmer(query_fasta, query_size, prms)</code>","text":"<p>Run pyhmmer hmmscan for a set of query proteins</p> <p>Parameters:</p> <ul> <li> <code>query_fasta</code>             (<code>str</code>)         \u2013          <p>Path to a query fasta file.</p> </li> <li> <code>query_size</code>             (<code>int</code>)         \u2013          <p>Number of query proteins.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>pd.DataFrame: Table with hmmscan search results.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def run_pyhmmer(query_fasta: str, query_size: int, prms: ilund4u.manager.Parameters) -&gt; pd.DataFrame:\n    \"\"\"Run pyhmmer hmmscan for a set of query proteins\n\n    Arguments:\n        query_fasta (str): Path to a query fasta file.\n        query_size (int): Number of query proteins.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    Returns:\n        pd.DataFrame: Table with hmmscan search results.\n\n    \"\"\"\n    with pyhmmer.easel.SequenceFile(query_fasta, digital=True) as seqs_file:\n        query_proteins = seqs_file.read_block()\n    num_of_query_proteins = query_size\n\n    hmmscan_output_folder = os.path.join(prms.args[\"output_dir\"], \"hmmscan\")\n    if os.path.exists(hmmscan_output_folder):\n        shutil.rmtree(hmmscan_output_folder)\n    os.mkdir(hmmscan_output_folder)\n\n    databases_cname = prms.args[\"hmm_config_names\"]\n    databases_short_names = prms.args[\"database_names\"]\n\n    databases_names = {\"hmm_defence_df\": \"DefenceFinder and CasFinder databases\",\n                       \"hmm_defence_padloc\": \"PADLOC database\", \"hmm_virulence\": \"virulence factor database (VFDB)\",\n                       \"hmm_anti_defence\": \"anti-prokaryotic immune systems database (dbAPIS)\",\n                       \"hmm_amr\": \"AMRFinderPlus Database\"}\n    databases_class = {\"hmm_defence_df\": \"defence\", \"hmm_defence_padloc\": \"defence\", \"hmm_virulence\": \"virulence\",\n                       \"hmm_anti_defence\": \"anti-defence\",\n                       \"hmm_amr\": \"AMR\"}\n\n    alignment_table_rows = []\n    for db_ind, db_name in enumerate(databases_cname):\n        if prms.args[\"defence_models\"] == \"DefenseFinder\" and db_name == \"hmm_defence_padloc\":\n            continue\n        if prms.args[\"defence_models\"] == \"PADLOC\" and db_name == \"hmm_defence_df\":\n            continue\n        db_alignment_table_rows = []\n        db_shortname = databases_short_names[db_ind]\n        db_path = prms.args[db_name]\n        db_full_name = databases_names[db_name]\n        db_class = databases_class[db_name]\n        if not os.path.exists(db_path):\n            print(f\"  \u29bf Database {db_full_name} was not found.\", file=sys.stdout)\n            continue\n        hmm_files = [fp for fp in os.listdir(db_path) if os.path.splitext(fp)[1].lower() == \".hmm\" and fp[0] != \".\"]\n        hmms = []\n        for hmm_file in hmm_files:\n            hmms.append(pyhmmer.plan7.HMMFile(os.path.join(db_path, hmm_file)).read())\n        if prms.args[\"verbose\"]:\n            print(f\"  \u29bf Running pyhmmer hmmscan versus {db_full_name}...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\"   \", max=num_of_query_proteins, suffix=\"%(index)d/%(max)d\")\n        for hits in pyhmmer.hmmscan(query_proteins, hmms, E=prms.args[\"hmmscan_evalue\"], cpus=0):\n            if prms.args[\"verbose\"]:\n                bar.next()\n            for hit in hits:\n                if hit.included:\n                    for domain in hit.domains.reported:\n                        if domain.i_evalue &lt; prms.args[\"hmmscan_evalue\"]:\n                            alignment = domain.alignment\n                            hit_name = hit.name.decode()\n                            hit_description = hit.description\n                            if hit.description:\n                                hit_description = hit_description.decode()\n                                if hit_description == \"NA\":\n                                    hit_description = \"\"\n                            else:\n                                hit_description = \"\"\n                            if hit_name != hit_description and hit_name not in hit_description and hit_description:\n                                hname = f\"{hit_name} {hit_description}\"\n                            elif hit_description:\n                                hname = hit_description\n                            else:\n                                hname = hit_name\n                            alignment_row = dict(query=alignment.target_name.decode(),  db_class = db_class,\n                                                 target_db=db_shortname, target=hname,t_name=hit_name,\n                                                 t_description=hit_description,\n                                                 hit_evalue=hit.evalue, di_evalue=domain.i_evalue,\n                                                 q_from=alignment.target_from, q_to=alignment.target_to,\n                                                 qlen=alignment.target_length, t_from=alignment.hmm_from,\n                                                 t_to=alignment.hmm_to, tlen=alignment.hmm_length)\n                            alignment_row[\"q_cov\"] = round((alignment_row[\"q_to\"] - alignment_row[\"q_from\"]) / \\\n                                                           alignment_row[\"qlen\"], 2)\n                            alignment_row[\"t_cov\"] = round((alignment_row[\"t_to\"] - alignment_row[\"t_from\"]) / \\\n                                                           alignment_row[\"tlen\"], 2)\n                            if alignment_row[\"q_cov\"] &gt;= prms.args[\"hmmscan_query_coverage_cutoff\"] and \\\n                                    alignment_row[\"t_cov\"] &gt;= prms.args[\"hmmscan_hmm_coverage_cutoff\"]:\n                                if hit.description:\n                                    alignment_row[\"t_description\"] = hit.description.decode()\n                                else:\n                                    alignment_row[\"t_description\"] = hit.name.decode()\n                                db_alignment_table_rows.append(alignment_row)\n        if prms.args[\"verbose\"]:\n            bar.finish()\n        alignment_table_rows += db_alignment_table_rows\n        db_alignment_table = pd.DataFrame(db_alignment_table_rows)\n        if not db_alignment_table.empty:\n            db_alignment_table = db_alignment_table.sort_values(by=\"hit_evalue\", ascending=True)\n            db_alignment_table = db_alignment_table.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n            db_alignment_table.to_csv(os.path.join(hmmscan_output_folder, f\"{db_shortname}.tsv\"), sep=\"\\t\",\n                                      index_label=\"query\")\n        n_hits = len(db_alignment_table.index)\n        if prms.args[\"verbose\"]:\n            print(f\"    Number of hits: {n_hits}\", file=sys.stdout)\n    alignment_table = pd.DataFrame(alignment_table_rows)\n    if not alignment_table.empty:\n        alignment_table = alignment_table.sort_values(by=\"hit_evalue\", ascending=True)\n        alignment_table = alignment_table.drop_duplicates(subset=\"query\", keep=\"first\").set_index(\"query\")\n\n    return alignment_table\n</code></pre>"},{"location":"API/package/#ilund4u.methods.update_path_extension","title":"<code>update_path_extension(path, new_extension)</code>","text":"<p>Get path basename and replace its extension</p> <p>Parameters:</p> <ul> <li> <code>path</code>             (<code>str</code>)         \u2013          <p>path to a file</p> </li> <li> <code>new_extension</code>             (<code>str</code>)         \u2013          <p>new extension</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code> (            <code>str</code> )        \u2013          <p>basename of a file with new extension.</p> </li> </ul> Source code in <code>ilund4u/methods.py</code> <pre><code>def update_path_extension(path: str, new_extension: str) -&gt; str:\n    \"\"\"Get path basename and replace its extension\n\n    Arguments:\n        path (str): path to a file\n        new_extension (str): new extension\n\n    Returns:\n        str: basename of a file with new extension.\n\n    \"\"\"\n    updated_filename = f\"{os.path.splitext(os.path.basename(path))[0]}.{new_extension}\"\n    return updated_filename\n</code></pre>"},{"location":"API/package/#drawing-module","title":"drawing module","text":"<p>This module provides visualisation classes and methods for the tool.</p>"},{"location":"API/package/#ilund4u.drawing.DrawingManager","title":"<code>DrawingManager</code>","text":"<p>Manager for data visualisation using LoVis4u library.</p> <p>Attributes:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>prms</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>class DrawingManager:\n    \"\"\"Manager for data visualisation using LoVis4u library.\n\n    Attributes:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        prms (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n\n    \"\"\"\n\n    def __init__(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n                 parameters: ilund4u.manager.Parameters):\n        \"\"\"DrawingManager class constructor\n\n        Arguments:\n            proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n            hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n            parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n        \"\"\"\n        self.proteomes = proteomes\n        self.hotspots = hotspots\n        self.prms = parameters\n\n    def plot_hotspot_communities(self, communities: typing.Union[None, list] = None,\n                                 shortest_labels=\"auto\") -&gt; None:\n        \"\"\"Run visualisation of hotspot list for each hotspot community.\n\n        Arguments:\n            communities (None | list): list of communities to be plotted.\n            shortest_labels (bool | auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n                proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n            if not communities:\n                communities = self.hotspots.communities.values()\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Visualisation of hotspot communities using lovis4u...\", file=sys.stdout)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(communities), suffix='%(index)d/%(max)d')\n            for hc in communities:\n                self.plot_hotspots(hc, vis_output_folder, shortest_labels=shortest_labels)\n                bar.next()\n            bar.finish()\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot hotspot communities.\") from error\n\n    def plot_hotspots(self, hotspot_ids: list, output_folder: str = \"default\",\n                      island_ids: typing.Union[None, list] = None,\n                      proteome_ids: typing.Union[None, list] = None,\n                      additional_annotation: typing.Union[None, dict] = None, keep_while_deduplication: list = [],\n                      shortest_labels: typing.Union[str, bool] = \"auto\", compact_mode: bool = False,\n                      keep_temp_data=True):\n        \"\"\"Visualise set of hotspots using Lovis4u.\n\n        Arguments:\n            hotspot_ids (list): List of hotspot ids to be plotted.\n            output_folder (str): Output folder to save pdf file.\n            island_ids (None | list): List of island ids. In case it's specified only listed islands will be plotted.\n            proteome_ids (None | list): List of proteome ids. In case it's specifiedd only listed proteome will\n                be plotted.\n            additional_annotation (dict): Additional LoVis4u feature annotation dict.\n            keep_while_deduplication (list): List of island ids to be kept during deduplication.\n            shortest_labels (bool| auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n                proteins.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if output_folder == \"default\":\n                output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n            if not os.path.exists(output_folder):\n                os.mkdir(output_folder)\n            locus_annotation_rows, feature_annotation_rows, mmseqs_results_rows, gff_files = [], [], [], []\n            cds_table_rows = []\n            already_added_groups = []\n            hotspot_subset = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n            added_proteomes = []\n            for hotspot in hotspot_subset:\n                for h_island in hotspot.islands:\n                    if island_ids:\n                        if h_island.island_id not in island_ids:\n                            continue\n                    proteome = self.proteomes.proteomes.at[h_island.proteome]\n                    if proteome_ids:\n                        if proteome.proteome_id not in proteome_ids:\n                            continue\n                    if proteome.proteome_id in added_proteomes:\n                        continue\n                    added_proteomes.append(proteome.proteome_id)\n                    proteome_annotation = self.proteomes.annotation.loc[h_island.proteome]\n                    proteome_cdss = proteome.cdss.to_list()\n                    locus_indexes = h_island.get_all_locus_indexes(proteome.cdss)\n                    locus_groups = h_island.get_locus_groups(proteome.cdss)\n                    if locus_groups not in already_added_groups:\n                        already_added_groups.append(locus_groups)\n                    else:\n                        if h_island.island_id not in keep_while_deduplication:\n                            continue\n                    gff_files.append(proteome.gff_file)\n                    start_coordinate = proteome_cdss[locus_indexes[0]].start\n                    end_coordinate = proteome_cdss[locus_indexes[-1]].end\n                    if compact_mode:\n                        nf = 1\n                        start_coordinate = proteome_cdss[h_island.indexes[0] - nf].start  #\n                        end_coordinate = proteome_cdss[h_island.indexes[-1] + nf].end  #\n                    if end_coordinate &gt; start_coordinate:\n                        sequence_coordinate = f\"{start_coordinate}:{end_coordinate}:1\"\n                    else:\n                        sequence_coordinate = f\"{start_coordinate}:{proteome_annotation['length']}:1,1:{end_coordinate}:1\"\n                    locus_annotation_row = dict(sequence_id=h_island.proteome, coordinates=sequence_coordinate,\n                                                circular=proteome.circular, group=hotspot.proteome_community)\n                    if len(hotspot_ids) &gt; 1:\n                        locus_annotation_row[\"description\"] = f\"proteome community: {hotspot.proteome_community}\"\n                    locus_annotation_rows.append(locus_annotation_row)\n                    for cds_ind, cds in enumerate(proteome_cdss):\n                        if cds_ind in locus_indexes:\n                            short_id = cds.cds_id.replace(proteome.proteome_id, \"\").strip().strip(\"_\").strip(\"-\")\n                            if cds_ind not in h_island.indexes:\n                                if cds.g_class == \"conserved\":\n                                    group_type = \"conserved\"\n                                    fcolour = \"#8B9697\"  # attention\n                                else:\n                                    group_type = \"conserved\"\n                                    fcolour = \"#D3D5D6\"\n                                scolour = \"#000000\"\n                            else:\n                                fcolour = \"default\"\n                                scolour = \"default\"\n                                group_type = \"variable\"\n                                cds_table_row = dict(hotspot=hotspot.hotspot_id, sequence=h_island.proteome,\n                                                     island_index=h_island.indexes.index(cds_ind),\n                                                     group=cds.group,\n                                                     coordinates=f\"{cds.start}:{cds.end}:{cds.strand}\",\n                                                     cds_id=cds.cds_id, cds_type=cds.g_class,\n                                                     name=cds.name)\n                                if cds.hmmscan_results:\n                                    cds_table_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                    if \"db_name\" in cds.hmmscan_results.keys():\n                                        cds_table_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                    else:\n                                        cds_table_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                                cds_table_rows.append(cds_table_row)\n                            feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type,\n                                                          fill_colour=fcolour, stroke_colour=scolour,\n                                                          name=cds.name)\n                            if feature_annotation_row[\"name\"] == \"hypothetical protein\":\n                                feature_annotation_row[\"name\"] = \"\"\n                            if cds.hmmscan_results:\n                                feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                if \"db_name\" in cds.hmmscan_results.keys():\n                                    feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                else:\n                                    feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                            if additional_annotation:\n                                if cds.cds_id in additional_annotation.keys():\n                                    feature_annotation_row.update(additional_annotation[cds.cds_id])\n                            if feature_annotation_row[\"name\"]:\n                                if not compact_mode:\n                                    feature_annotation_row[\"name\"] += f\" ({short_id})\"\n                            else:\n                                if shortest_labels == True or \\\n                                        (shortest_labels == \"auto\" and len(h_island.indexes) &gt;= self.prms.args[\n                                            \"island_size_cutoff_to_show_index_only\"]):\n                                    feature_annotation_row[\"name\"] = str(cds_ind + 1)\n                                    if compact_mode:\n                                        feature_annotation_row[\"name\"] = \"\"\n                                else:\n                                    feature_annotation_row[\"name\"] = str(short_id)\n                            feature_annotation_rows.append(feature_annotation_row)\n                        mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n            cds_tables_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots_annotation\")\n            if not os.path.exists(cds_tables_folder):\n                os.mkdir(cds_tables_folder)\n            cds_table = pd.DataFrame(cds_table_rows)\n            table_name = f\"{'_'.join(hotspot_ids)}\"\n            if len(table_name) &gt; 200:\n                table_name = f\"{table_name[:200]}..._{hotspot_ids[-1]}\"\n            cds_table.to_csv(os.path.join(cds_tables_folder, f\"{table_name}.tsv\"), sep=\"\\t\", index=False)\n\n            locus_annotation_t = pd.DataFrame(locus_annotation_rows)\n            feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n            temp_input_f = tempfile.NamedTemporaryFile()\n            temp_input_l = tempfile.NamedTemporaryFile()\n            locus_annotation_t.to_csv(temp_input_l.name, sep=\"\\t\", index=False)\n            feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n\n            l_parameters = lovis4u.Manager.Parameters()\n            if compact_mode:\n                l_parameters.load_config(\"A4p1\")\n            else:\n                l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n            # l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n            l_parameters.args[\"cluster_all_proteins\"] = False\n            l_parameters.args[\"locus_label_style\"] = \"id\"\n            l_parameters.args[\"locus_label_position\"] = \"bottom\"\n            l_parameters.args[\"verbose\"] = False\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n\n            l_parameters.args[\"draw_middle_line\"] = True\n            l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n            l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n\n            if compact_mode:\n                l_parameters.args[\"feature_group_types_to_show_label\"] = []\n                l_parameters.args[\"feature_group_types_to_show_label_on_first_occurrence\"] = [\"conserved\", \"variable\"]\n                l_parameters.args[\"draw_individual_x_axis\"] = False\n\n            loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n            loci.load_feature_annotation_file(temp_input_f.name)\n            loci.load_locus_annotation_file(temp_input_l.name)\n\n            mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n            loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n            loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n            loci.reorient_loci(ilund4u_mode=True)\n            loci.set_feature_colours_based_on_groups()\n            loci.set_category_colours()\n            loci.define_labels_to_be_shown()\n\n            loci.save_feature_annotation_table()\n            loci.save_locus_annotation_table()\n\n            canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n            canvas_manager.define_layout(loci)\n            canvas_manager.add_loci_tracks(loci)\n            canvas_manager.add_scale_line_track()\n            canvas_manager.add_categories_colour_legend_track(loci)\n            canvas_manager.add_homology_track()\n            pdf_name = f\"{'_'.join(hotspot_ids)}\"\n            if len(pdf_name) &gt; 200:\n                pdf_name = f\"{pdf_name[:200]}..._{hotspot_ids[-1]}\"\n            canvas_manager.plot(f\"{pdf_name}.pdf\")\n            os.system(f\"mv {l_parameters.args['output_dir']}/{pdf_name}.pdf {output_folder}/\")\n            if keep_temp_data:\n                if not os.path.exists(os.path.join(output_folder, \"lovis4u_output\")):\n                    os.mkdir(os.path.join(output_folder, \"lovis4u_output\"))\n                os.system(f\"mv {l_parameters.args['output_dir']} \"\n                          f\"{os.path.join(output_folder, 'lovis4u_output', pdf_name)}\")\n                os.mkdir(os.path.join(output_folder, \"lovis4u_output\", pdf_name, \"gff_files\"))\n                for gff_file in gff_files:\n                    os.system(f\"cp '{gff_file}' {os.path.join(output_folder, 'lovis4u_output', pdf_name, 'gff_files')}/\")\n            else:\n                shutil.rmtree(l_parameters.args[\"output_dir\"])\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot set of hotspots using LoVis4u.\") from error\n\n    def plot_proteome_communities(self):\n        \"\"\"Run visualisation of proteome list for each proteome community.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            if self.prms.args[\"verbose\"]:\n                print(f\"\u25cb Visualisation of proteome communities with corresponding hotspots using lovis4u...\",\n                      file=sys.stdout)\n                vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_proteome_communities\")\n                if os.path.exists(vis_output_folder):\n                    shutil.rmtree(vis_output_folder)\n                os.mkdir(vis_output_folder)\n                bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.communities.keys()),\n                                                     suffix='%(index)d/%(max)d')\n                for com_id, com_pr_ids in self.proteomes.communities.items():\n                    bar.next()\n                    self.plot_proteome_community(com_id, vis_output_folder)\n                bar.finish()\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(\"Unable to plot proteome communities.\") from error\n\n    def plot_proteome_community(self, community: int, output_folder: str, mode: str = \"hotspot\",\n                                proteome_ids: typing.Union[None, list] = None,\n                                additional_annotation: typing.Union[None, dict] = None,\n                                filename: typing.Union[None, str] = None):\n        \"\"\"Visualise proteome community using LoVis4u.\n\n        Arguments:\n            community (int): Id of proteome community to be plotted.\n            output_folder (str): Output folder to save pdf file.\n            mode (str): Mode of visualisation.\n            proteome_ids: (None | list): List of proteome ids. In case it's specified only listed proteomes will\n                be plotted.\n            additional_annotation (dict): Additional LoVis4u feature annotation dict.\n            filename (None | str): Pdf file name. If not specified id of community will be used.\n\n        Returns:\n            None\n\n        \"\"\"\n        try:\n            community_proteomes = self.proteomes.communities[community]\n            if mode == \"hotspot\":\n                hotspot_annotation_com = self.hotspots.annotation[\n                    self.hotspots.annotation[\"proteome_community\"] == community]\n                num_of_hotspots = len(hotspot_annotation_com.index)\n                colours_rgb = seaborn.color_palette(\"husl\", num_of_hotspots, desat=1)\n                colours = list(map(lambda x: matplotlib.colors.rgb2hex(x), colours_rgb))\n                colours_dict = ({g: c for g, c in zip(list(hotspot_annotation_com.index.to_list()), colours)})\n                com_hotspots = self.hotspots.hotspots.loc[hotspot_annotation_com.index]\n                island_proteins_d = dict()\n                for hotspot in com_hotspots.to_list():\n                    for island in hotspot.islands:\n                        proteome = self.proteomes.proteomes.at[island.proteome]\n                        island_indexes = island.indexes\n                        island_cds_ids = proteome.cdss.iloc[island_indexes].apply(lambda cds: cds.cds_id).to_list()\n                        for ic_id in island_cds_ids:\n                            island_proteins_d[ic_id] = hotspot.hotspot_id\n            gff_files = []\n            feature_annotation_rows = []\n            mmseqs_results_rows = []\n            n_of_added_proteomes = 0\n            for proteome_id in community_proteomes:\n                if proteome_ids:\n                    if proteome_id not in proteome_ids:\n                        continue\n                n_of_added_proteomes += 1\n                proteome = self.proteomes.proteomes.at[proteome_id]\n                gff_files.append(proteome.gff_file)\n                for cds_ind, cds in enumerate(proteome.cdss.to_list()):\n                    if cds.g_class == \"conserved\":\n                        group_type = \"conserved\"\n                    else:\n                        group_type = \"variable\"\n                    if mode == \"hotspot\":\n                        if cds.cds_id in island_proteins_d.keys():\n                            fcolour = colours_dict[island_proteins_d[cds.cds_id]]\n                        else:\n                            if cds.g_class == \"conserved\":\n                                fcolour = \"#BDC6CA\"\n                            else:\n                                fcolour = \"#8C9295\"\n                    feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type)\n                    if mode == \"hotspot\":\n                        feature_annotation_row[\"show_label\"] = 0\n                        feature_annotation_row[\"stroke_colour\"] = \"#000000\"\n                        feature_annotation_row[\"fill_colour\"] = fcolour\n                    if cds.hmmscan_results and self.prms.args[\"show_hmmscan_hits_on_full_proteomes\"]:\n                        feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                        if \"db_name\" in cds.hmmscan_results.keys():\n                            feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                        else:\n                            feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"].lower()\n                    if additional_annotation:\n                        if cds.cds_id in additional_annotation.keys():\n                            feature_annotation_row.update(additional_annotation[cds.cds_id])\n                    feature_annotation_rows.append(feature_annotation_row)\n                    mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n            l_parameters = lovis4u.Manager.Parameters()\n            l_parameters.load_config(self.prms.args[\"lovis4u_proteome_config_filename\"])\n            l_parameters.args[\"cluster_all_proteins\"] = False\n            if n_of_added_proteomes &gt; 1:\n                l_parameters.args[\"draw_individual_x_axis\"] = False\n            else:\n                l_parameters.args[\"draw_individual_x_axis\"] = True\n            l_parameters.args[\"verbose\"] = False\n            l_parameters.args[\"locus_label_style\"] = \"id\"\n            if mode == \"hotspot\" and n_of_added_proteomes != 1:\n                l_parameters.args[\"gff_CDS_category_source\"] = \"-\"\n            l_parameters.args[\"draw_middle_line\"] = False\n            l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n            l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n            if os.path.exists(l_parameters.args[\"output_dir\"]):\n                shutil.rmtree(l_parameters.args[\"output_dir\"])\n            os.mkdir(l_parameters.args[\"output_dir\"])\n            loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n            feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n            temp_input_f = tempfile.NamedTemporaryFile()\n            feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n            loci.load_feature_annotation_file(temp_input_f.name)\n            mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n            loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n            if len(gff_files) &lt;= self.prms.args[\"max_number_of_seqs_to_redefine_order\"]:\n                loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n            loci.reorient_loci(ilund4u_mode=True)\n            if mode == \"regular\" or n_of_added_proteomes == 1:\n                loci.define_labels_to_be_shown()\n            loci.set_feature_colours_based_on_groups()\n            loci.set_category_colours()\n            loci.save_feature_annotation_table()\n            canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n            canvas_manager.define_layout(loci)\n            canvas_manager.add_loci_tracks(loci)\n            if n_of_added_proteomes &gt; 1:\n                canvas_manager.add_scale_line_track()\n            canvas_manager.add_categories_colour_legend_track(loci)\n            canvas_manager.add_homology_track()\n            if not filename:\n                filename = f\"{community}.pdf\"\n            canvas_manager.plot(filename)\n            os.system(f\"mv {l_parameters.args['output_dir']}/{filename} {output_folder}/\")\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n            return None\n        except Exception as error:\n            raise ilund4u.manager.ilund4uError(f\"Unable to plot proteome community {community}.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.drawing.DrawingManager.__init__","title":"<code>__init__(proteomes, hotspots, parameters)</code>","text":"<p>DrawingManager class constructor</p> <p>Parameters:</p> <ul> <li> <code>proteomes</code>             (<code>Proteomes</code>)         \u2013          <p>Proteomes object.</p> </li> <li> <code>hotspots</code>             (<code>Hotspots</code>)         \u2013          <p>Hotspots object.</p> </li> <li> <code>parameters</code>             (<code>Parameters</code>)         \u2013          <p>Parameters class object that holds all arguments.</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def __init__(self, proteomes: ilund4u.data_processing.Proteomes, hotspots: ilund4u.data_processing.Hotspots,\n             parameters: ilund4u.manager.Parameters):\n    \"\"\"DrawingManager class constructor\n\n    Arguments:\n        proteomes (ilund4u.data_processing.Proteomes): Proteomes object.\n        hotspots (ilund4u.data_processing.Hotspots): Hotspots object.\n        parameters (ilund4u.manager.Parameters): Parameters class object that holds all arguments.\n\n    \"\"\"\n    self.proteomes = proteomes\n    self.hotspots = hotspots\n    self.prms = parameters\n</code></pre>"},{"location":"API/package/#ilund4u.drawing.DrawingManager.plot_hotspot_communities","title":"<code>plot_hotspot_communities(communities=None, shortest_labels='auto')</code>","text":"<p>Run visualisation of hotspot list for each hotspot community.</p> <p>Parameters:</p> <ul> <li> <code>communities</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>list of communities to be plotted.</p> </li> <li> <code>shortest_labels</code>             (<code>bool | auto</code>, default:                 <code>'auto'</code> )         \u2013          <p>Whether to put 1-based cds index only instead of CDS id for hypothetical proteins.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_hotspot_communities(self, communities: typing.Union[None, list] = None,\n                             shortest_labels=\"auto\") -&gt; None:\n    \"\"\"Run visualisation of hotspot list for each hotspot community.\n\n    Arguments:\n        communities (None | list): list of communities to be plotted.\n        shortest_labels (bool | auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n            proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n        if os.path.exists(vis_output_folder):\n            shutil.rmtree(vis_output_folder)\n        os.mkdir(vis_output_folder)\n        if not communities:\n            communities = self.hotspots.communities.values()\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Visualisation of hotspot communities using lovis4u...\", file=sys.stdout)\n        bar = progress.bar.FillingCirclesBar(\" \", max=len(communities), suffix='%(index)d/%(max)d')\n        for hc in communities:\n            self.plot_hotspots(hc, vis_output_folder, shortest_labels=shortest_labels)\n            bar.next()\n        bar.finish()\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot hotspot communities.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.drawing.DrawingManager.plot_hotspots","title":"<code>plot_hotspots(hotspot_ids, output_folder='default', island_ids=None, proteome_ids=None, additional_annotation=None, keep_while_deduplication=[], shortest_labels='auto', compact_mode=False, keep_temp_data=True)</code>","text":"<p>Visualise set of hotspots using Lovis4u.</p> <p>Parameters:</p> <ul> <li> <code>hotspot_ids</code>             (<code>list</code>)         \u2013          <p>List of hotspot ids to be plotted.</p> </li> <li> <code>output_folder</code>             (<code>str</code>, default:                 <code>'default'</code> )         \u2013          <p>Output folder to save pdf file.</p> </li> <li> <code>island_ids</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>List of island ids. In case it's specified only listed islands will be plotted.</p> </li> <li> <code>proteome_ids</code>             (<code>None | list</code>, default:                 <code>None</code> )         \u2013          <p>List of proteome ids. In case it's specifiedd only listed proteome will be plotted.</p> </li> <li> <code>additional_annotation</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Additional LoVis4u feature annotation dict.</p> </li> <li> <code>keep_while_deduplication</code>             (<code>list</code>, default:                 <code>[]</code> )         \u2013          <p>List of island ids to be kept during deduplication.</p> </li> <li> <code>shortest_labels</code>             (<code>bool | auto</code>, default:                 <code>'auto'</code> )         \u2013          <p>Whether to put 1-based cds index only instead of CDS id for hypothetical proteins.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_hotspots(self, hotspot_ids: list, output_folder: str = \"default\",\n                  island_ids: typing.Union[None, list] = None,\n                  proteome_ids: typing.Union[None, list] = None,\n                  additional_annotation: typing.Union[None, dict] = None, keep_while_deduplication: list = [],\n                  shortest_labels: typing.Union[str, bool] = \"auto\", compact_mode: bool = False,\n                  keep_temp_data=True):\n    \"\"\"Visualise set of hotspots using Lovis4u.\n\n    Arguments:\n        hotspot_ids (list): List of hotspot ids to be plotted.\n        output_folder (str): Output folder to save pdf file.\n        island_ids (None | list): List of island ids. In case it's specified only listed islands will be plotted.\n        proteome_ids (None | list): List of proteome ids. In case it's specifiedd only listed proteome will\n            be plotted.\n        additional_annotation (dict): Additional LoVis4u feature annotation dict.\n        keep_while_deduplication (list): List of island ids to be kept during deduplication.\n        shortest_labels (bool| auto): Whether to put 1-based cds index only instead of CDS id for hypothetical\n            proteins.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if output_folder == \"default\":\n            output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots\")\n        if not os.path.exists(output_folder):\n            os.mkdir(output_folder)\n        locus_annotation_rows, feature_annotation_rows, mmseqs_results_rows, gff_files = [], [], [], []\n        cds_table_rows = []\n        already_added_groups = []\n        hotspot_subset = self.hotspots.hotspots.loc[hotspot_ids].to_list()\n        added_proteomes = []\n        for hotspot in hotspot_subset:\n            for h_island in hotspot.islands:\n                if island_ids:\n                    if h_island.island_id not in island_ids:\n                        continue\n                proteome = self.proteomes.proteomes.at[h_island.proteome]\n                if proteome_ids:\n                    if proteome.proteome_id not in proteome_ids:\n                        continue\n                if proteome.proteome_id in added_proteomes:\n                    continue\n                added_proteomes.append(proteome.proteome_id)\n                proteome_annotation = self.proteomes.annotation.loc[h_island.proteome]\n                proteome_cdss = proteome.cdss.to_list()\n                locus_indexes = h_island.get_all_locus_indexes(proteome.cdss)\n                locus_groups = h_island.get_locus_groups(proteome.cdss)\n                if locus_groups not in already_added_groups:\n                    already_added_groups.append(locus_groups)\n                else:\n                    if h_island.island_id not in keep_while_deduplication:\n                        continue\n                gff_files.append(proteome.gff_file)\n                start_coordinate = proteome_cdss[locus_indexes[0]].start\n                end_coordinate = proteome_cdss[locus_indexes[-1]].end\n                if compact_mode:\n                    nf = 1\n                    start_coordinate = proteome_cdss[h_island.indexes[0] - nf].start  #\n                    end_coordinate = proteome_cdss[h_island.indexes[-1] + nf].end  #\n                if end_coordinate &gt; start_coordinate:\n                    sequence_coordinate = f\"{start_coordinate}:{end_coordinate}:1\"\n                else:\n                    sequence_coordinate = f\"{start_coordinate}:{proteome_annotation['length']}:1,1:{end_coordinate}:1\"\n                locus_annotation_row = dict(sequence_id=h_island.proteome, coordinates=sequence_coordinate,\n                                            circular=proteome.circular, group=hotspot.proteome_community)\n                if len(hotspot_ids) &gt; 1:\n                    locus_annotation_row[\"description\"] = f\"proteome community: {hotspot.proteome_community}\"\n                locus_annotation_rows.append(locus_annotation_row)\n                for cds_ind, cds in enumerate(proteome_cdss):\n                    if cds_ind in locus_indexes:\n                        short_id = cds.cds_id.replace(proteome.proteome_id, \"\").strip().strip(\"_\").strip(\"-\")\n                        if cds_ind not in h_island.indexes:\n                            if cds.g_class == \"conserved\":\n                                group_type = \"conserved\"\n                                fcolour = \"#8B9697\"  # attention\n                            else:\n                                group_type = \"conserved\"\n                                fcolour = \"#D3D5D6\"\n                            scolour = \"#000000\"\n                        else:\n                            fcolour = \"default\"\n                            scolour = \"default\"\n                            group_type = \"variable\"\n                            cds_table_row = dict(hotspot=hotspot.hotspot_id, sequence=h_island.proteome,\n                                                 island_index=h_island.indexes.index(cds_ind),\n                                                 group=cds.group,\n                                                 coordinates=f\"{cds.start}:{cds.end}:{cds.strand}\",\n                                                 cds_id=cds.cds_id, cds_type=cds.g_class,\n                                                 name=cds.name)\n                            if cds.hmmscan_results:\n                                cds_table_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                                if \"db_name\" in cds.hmmscan_results.keys():\n                                    cds_table_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                                else:\n                                    cds_table_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                            cds_table_rows.append(cds_table_row)\n                        feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type,\n                                                      fill_colour=fcolour, stroke_colour=scolour,\n                                                      name=cds.name)\n                        if feature_annotation_row[\"name\"] == \"hypothetical protein\":\n                            feature_annotation_row[\"name\"] = \"\"\n                        if cds.hmmscan_results:\n                            feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                            if \"db_name\" in cds.hmmscan_results.keys():\n                                feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                            else:\n                                feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"]\n                        if additional_annotation:\n                            if cds.cds_id in additional_annotation.keys():\n                                feature_annotation_row.update(additional_annotation[cds.cds_id])\n                        if feature_annotation_row[\"name\"]:\n                            if not compact_mode:\n                                feature_annotation_row[\"name\"] += f\" ({short_id})\"\n                        else:\n                            if shortest_labels == True or \\\n                                    (shortest_labels == \"auto\" and len(h_island.indexes) &gt;= self.prms.args[\n                                        \"island_size_cutoff_to_show_index_only\"]):\n                                feature_annotation_row[\"name\"] = str(cds_ind + 1)\n                                if compact_mode:\n                                    feature_annotation_row[\"name\"] = \"\"\n                            else:\n                                feature_annotation_row[\"name\"] = str(short_id)\n                        feature_annotation_rows.append(feature_annotation_row)\n                    mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n        cds_tables_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_hotspots_annotation\")\n        if not os.path.exists(cds_tables_folder):\n            os.mkdir(cds_tables_folder)\n        cds_table = pd.DataFrame(cds_table_rows)\n        table_name = f\"{'_'.join(hotspot_ids)}\"\n        if len(table_name) &gt; 200:\n            table_name = f\"{table_name[:200]}..._{hotspot_ids[-1]}\"\n        cds_table.to_csv(os.path.join(cds_tables_folder, f\"{table_name}.tsv\"), sep=\"\\t\", index=False)\n\n        locus_annotation_t = pd.DataFrame(locus_annotation_rows)\n        feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n        temp_input_f = tempfile.NamedTemporaryFile()\n        temp_input_l = tempfile.NamedTemporaryFile()\n        locus_annotation_t.to_csv(temp_input_l.name, sep=\"\\t\", index=False)\n        feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n\n        l_parameters = lovis4u.Manager.Parameters()\n        if compact_mode:\n            l_parameters.load_config(\"A4p1\")\n        else:\n            l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n        # l_parameters.load_config(self.prms.args[\"lovis4u_hotspot_config_filename\"])\n        l_parameters.args[\"cluster_all_proteins\"] = False\n        l_parameters.args[\"locus_label_style\"] = \"id\"\n        l_parameters.args[\"locus_label_position\"] = \"bottom\"\n        l_parameters.args[\"verbose\"] = False\n        l_parameters.args[\"draw_individual_x_axis\"] = False\n\n        l_parameters.args[\"draw_middle_line\"] = True\n        l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n        l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n\n        if compact_mode:\n            l_parameters.args[\"feature_group_types_to_show_label\"] = []\n            l_parameters.args[\"feature_group_types_to_show_label_on_first_occurrence\"] = [\"conserved\", \"variable\"]\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n\n        loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n        loci.load_feature_annotation_file(temp_input_f.name)\n        loci.load_locus_annotation_file(temp_input_l.name)\n\n        mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n        loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n        loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n        loci.reorient_loci(ilund4u_mode=True)\n        loci.set_feature_colours_based_on_groups()\n        loci.set_category_colours()\n        loci.define_labels_to_be_shown()\n\n        loci.save_feature_annotation_table()\n        loci.save_locus_annotation_table()\n\n        canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n        canvas_manager.define_layout(loci)\n        canvas_manager.add_loci_tracks(loci)\n        canvas_manager.add_scale_line_track()\n        canvas_manager.add_categories_colour_legend_track(loci)\n        canvas_manager.add_homology_track()\n        pdf_name = f\"{'_'.join(hotspot_ids)}\"\n        if len(pdf_name) &gt; 200:\n            pdf_name = f\"{pdf_name[:200]}..._{hotspot_ids[-1]}\"\n        canvas_manager.plot(f\"{pdf_name}.pdf\")\n        os.system(f\"mv {l_parameters.args['output_dir']}/{pdf_name}.pdf {output_folder}/\")\n        if keep_temp_data:\n            if not os.path.exists(os.path.join(output_folder, \"lovis4u_output\")):\n                os.mkdir(os.path.join(output_folder, \"lovis4u_output\"))\n            os.system(f\"mv {l_parameters.args['output_dir']} \"\n                      f\"{os.path.join(output_folder, 'lovis4u_output', pdf_name)}\")\n            os.mkdir(os.path.join(output_folder, \"lovis4u_output\", pdf_name, \"gff_files\"))\n            for gff_file in gff_files:\n                os.system(f\"cp '{gff_file}' {os.path.join(output_folder, 'lovis4u_output', pdf_name, 'gff_files')}/\")\n        else:\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot set of hotspots using LoVis4u.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.drawing.DrawingManager.plot_proteome_communities","title":"<code>plot_proteome_communities()</code>","text":"<p>Run visualisation of proteome list for each proteome community.</p> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_proteome_communities(self):\n    \"\"\"Run visualisation of proteome list for each proteome community.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        if self.prms.args[\"verbose\"]:\n            print(f\"\u25cb Visualisation of proteome communities with corresponding hotspots using lovis4u...\",\n                  file=sys.stdout)\n            vis_output_folder = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_proteome_communities\")\n            if os.path.exists(vis_output_folder):\n                shutil.rmtree(vis_output_folder)\n            os.mkdir(vis_output_folder)\n            bar = progress.bar.FillingCirclesBar(\" \", max=len(self.proteomes.communities.keys()),\n                                                 suffix='%(index)d/%(max)d')\n            for com_id, com_pr_ids in self.proteomes.communities.items():\n                bar.next()\n                self.plot_proteome_community(com_id, vis_output_folder)\n            bar.finish()\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(\"Unable to plot proteome communities.\") from error\n</code></pre>"},{"location":"API/package/#ilund4u.drawing.DrawingManager.plot_proteome_community","title":"<code>plot_proteome_community(community, output_folder, mode='hotspot', proteome_ids=None, additional_annotation=None, filename=None)</code>","text":"<p>Visualise proteome community using LoVis4u.</p> <p>Parameters:</p> <ul> <li> <code>community</code>             (<code>int</code>)         \u2013          <p>Id of proteome community to be plotted.</p> </li> <li> <code>output_folder</code>             (<code>str</code>)         \u2013          <p>Output folder to save pdf file.</p> </li> <li> <code>mode</code>             (<code>str</code>, default:                 <code>'hotspot'</code> )         \u2013          <p>Mode of visualisation.</p> </li> <li> <code>proteome_ids</code>             (<code>Union[None, list]</code>, default:                 <code>None</code> )         \u2013          <p>(None | list): List of proteome ids. In case it's specified only listed proteomes will be plotted.</p> </li> <li> <code>additional_annotation</code>             (<code>dict</code>, default:                 <code>None</code> )         \u2013          <p>Additional LoVis4u feature annotation dict.</p> </li> <li> <code>filename</code>             (<code>None | str</code>, default:                 <code>None</code> )         \u2013          <p>Pdf file name. If not specified id of community will be used.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>None</p> </li> </ul> Source code in <code>ilund4u/drawing.py</code> <pre><code>def plot_proteome_community(self, community: int, output_folder: str, mode: str = \"hotspot\",\n                            proteome_ids: typing.Union[None, list] = None,\n                            additional_annotation: typing.Union[None, dict] = None,\n                            filename: typing.Union[None, str] = None):\n    \"\"\"Visualise proteome community using LoVis4u.\n\n    Arguments:\n        community (int): Id of proteome community to be plotted.\n        output_folder (str): Output folder to save pdf file.\n        mode (str): Mode of visualisation.\n        proteome_ids: (None | list): List of proteome ids. In case it's specified only listed proteomes will\n            be plotted.\n        additional_annotation (dict): Additional LoVis4u feature annotation dict.\n        filename (None | str): Pdf file name. If not specified id of community will be used.\n\n    Returns:\n        None\n\n    \"\"\"\n    try:\n        community_proteomes = self.proteomes.communities[community]\n        if mode == \"hotspot\":\n            hotspot_annotation_com = self.hotspots.annotation[\n                self.hotspots.annotation[\"proteome_community\"] == community]\n            num_of_hotspots = len(hotspot_annotation_com.index)\n            colours_rgb = seaborn.color_palette(\"husl\", num_of_hotspots, desat=1)\n            colours = list(map(lambda x: matplotlib.colors.rgb2hex(x), colours_rgb))\n            colours_dict = ({g: c for g, c in zip(list(hotspot_annotation_com.index.to_list()), colours)})\n            com_hotspots = self.hotspots.hotspots.loc[hotspot_annotation_com.index]\n            island_proteins_d = dict()\n            for hotspot in com_hotspots.to_list():\n                for island in hotspot.islands:\n                    proteome = self.proteomes.proteomes.at[island.proteome]\n                    island_indexes = island.indexes\n                    island_cds_ids = proteome.cdss.iloc[island_indexes].apply(lambda cds: cds.cds_id).to_list()\n                    for ic_id in island_cds_ids:\n                        island_proteins_d[ic_id] = hotspot.hotspot_id\n        gff_files = []\n        feature_annotation_rows = []\n        mmseqs_results_rows = []\n        n_of_added_proteomes = 0\n        for proteome_id in community_proteomes:\n            if proteome_ids:\n                if proteome_id not in proteome_ids:\n                    continue\n            n_of_added_proteomes += 1\n            proteome = self.proteomes.proteomes.at[proteome_id]\n            gff_files.append(proteome.gff_file)\n            for cds_ind, cds in enumerate(proteome.cdss.to_list()):\n                if cds.g_class == \"conserved\":\n                    group_type = \"conserved\"\n                else:\n                    group_type = \"variable\"\n                if mode == \"hotspot\":\n                    if cds.cds_id in island_proteins_d.keys():\n                        fcolour = colours_dict[island_proteins_d[cds.cds_id]]\n                    else:\n                        if cds.g_class == \"conserved\":\n                            fcolour = \"#BDC6CA\"\n                        else:\n                            fcolour = \"#8C9295\"\n                feature_annotation_row = dict(feature_id=cds.cds_id, group=cds.group, group_type=group_type)\n                if mode == \"hotspot\":\n                    feature_annotation_row[\"show_label\"] = 0\n                    feature_annotation_row[\"stroke_colour\"] = \"#000000\"\n                    feature_annotation_row[\"fill_colour\"] = fcolour\n                if cds.hmmscan_results and self.prms.args[\"show_hmmscan_hits_on_full_proteomes\"]:\n                    feature_annotation_row[\"name\"] = cds.hmmscan_results[\"target\"]\n                    if \"db_name\" in cds.hmmscan_results.keys():\n                        feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db_name\"]\n                    else:\n                        feature_annotation_row[\"category\"] = cds.hmmscan_results[\"db\"].lower()\n                if additional_annotation:\n                    if cds.cds_id in additional_annotation.keys():\n                        feature_annotation_row.update(additional_annotation[cds.cds_id])\n                feature_annotation_rows.append(feature_annotation_row)\n                mmseqs_results_rows.append(dict(cluster=cds.group, protein_id=cds.cds_id))\n\n        l_parameters = lovis4u.Manager.Parameters()\n        l_parameters.load_config(self.prms.args[\"lovis4u_proteome_config_filename\"])\n        l_parameters.args[\"cluster_all_proteins\"] = False\n        if n_of_added_proteomes &gt; 1:\n            l_parameters.args[\"draw_individual_x_axis\"] = False\n        else:\n            l_parameters.args[\"draw_individual_x_axis\"] = True\n        l_parameters.args[\"verbose\"] = False\n        l_parameters.args[\"locus_label_style\"] = \"id\"\n        if mode == \"hotspot\" and n_of_added_proteomes != 1:\n            l_parameters.args[\"gff_CDS_category_source\"] = \"-\"\n        l_parameters.args[\"draw_middle_line\"] = False\n        l_parameters.args[\"category_colours\"] = self.prms.args[\"category_colours\"]\n        l_parameters.args[\"output_dir\"] = os.path.join(self.prms.args[\"output_dir\"], \"lovis4u_tmp\")\n        if os.path.exists(l_parameters.args[\"output_dir\"]):\n            shutil.rmtree(l_parameters.args[\"output_dir\"])\n        os.mkdir(l_parameters.args[\"output_dir\"])\n        loci = lovis4u.DataProcessing.Loci(parameters=l_parameters)\n        feature_annotation_t = pd.DataFrame(feature_annotation_rows)\n        temp_input_f = tempfile.NamedTemporaryFile()\n        feature_annotation_t.to_csv(temp_input_f.name, sep=\"\\t\", index=False)\n        loci.load_feature_annotation_file(temp_input_f.name)\n        mmseqs_results_t = pd.DataFrame(mmseqs_results_rows).set_index(\"protein_id\")\n        loci.load_loci_from_extended_gff(gff_files, ilund4u_mode=True)\n        if len(gff_files) &lt;= self.prms.args[\"max_number_of_seqs_to_redefine_order\"]:\n            loci.cluster_sequences(mmseqs_results_t, one_cluster=True)\n        loci.reorient_loci(ilund4u_mode=True)\n        if mode == \"regular\" or n_of_added_proteomes == 1:\n            loci.define_labels_to_be_shown()\n        loci.set_feature_colours_based_on_groups()\n        loci.set_category_colours()\n        loci.save_feature_annotation_table()\n        canvas_manager = lovis4u.Manager.CanvasManager(l_parameters)\n        canvas_manager.define_layout(loci)\n        canvas_manager.add_loci_tracks(loci)\n        if n_of_added_proteomes &gt; 1:\n            canvas_manager.add_scale_line_track()\n        canvas_manager.add_categories_colour_legend_track(loci)\n        canvas_manager.add_homology_track()\n        if not filename:\n            filename = f\"{community}.pdf\"\n        canvas_manager.plot(filename)\n        os.system(f\"mv {l_parameters.args['output_dir']}/{filename} {output_folder}/\")\n        shutil.rmtree(l_parameters.args[\"output_dir\"])\n        return None\n    except Exception as error:\n        raise ilund4u.manager.ilund4uError(f\"Unable to plot proteome community {community}.\") from error\n</code></pre>"},{"location":"API/usage_examples/","title":"Short example-drived guide to iLund4u API","text":"<p>iLund4u has a simple API allowing it programmatic usage from within a python program. Below we describe several Python snippets that mimic results of command-line calls.</p> <p>See detailed description of each class and method in the \"Library\" section.</p>"},{"location":"API/usage_examples/#hotspot-annotation-mode","title":"Hotspot annotation mode","text":"<pre><code>import ilund4u\n\n# Creating a parameters object and loading config\nparameters = ilund4u.manager.Parameters()\nparameters.load_config(\"standard\")\n\n# Example of changing a particular parameter\nparameters.args[\"output_dir\"] = \"API_output_example\"\n\n# To turn off progress messages\nparameters.args[\"verbose\"] = False\n\n# Creating a proteomes object and loading gff files  \nproteomes = ilund4u.data_processing.Proteomes(parameters=parameters)\n\n# Loading folder with gff files (for ex. the example-driven guide)\ngff_folder = \"ilund4u_data/guide/gff_files\" \nproteomes.load_sequences_from_extended_gff(input_f=gff_folder)\n\n# Running mmseqs on all encoded proteins and processing results\nmmseqs_clusters = proteomes.mmseqs_cluster()\ncluster_to_sequence = proteomes.process_mmseqs_results(mmseqs_clusters)\n\n# Building proteome network, finding communities and defining protein classes        \nproteome_network = proteomes.build_proteome_network(cluster_to_sequence)\nproteomes.find_proteome_communities(proteome_network)\nproteomes.define_protein_classes()\n\n# Annotation of variable islands as regions with at least one non-conserved protein\nproteomes.annotate_variable_islands()\n\n# Building island network and finding hotspots there \nisland_networks = proteomes.build_islands_network()\nhotspots = proteomes.find_hotspots(island_networks)\n\n# Additional functional annotation of proetins\nhotspots.pyhmmer_annotation(proteomes)\n\n# Merging hotspots by building hotspot network\nhotspots.build_hotspot_network()\n\n# Calculate statistics\nhotspots.calculate_hotspot_and_island_statistics(proteomes)\nhotspots.get_each_protein_group_statistics(proteomes)\n\n# Saving results as database\ndatabase_manager = ilund4u.data_manager.DatabaseManager(parameters)\ndatabase_manager.build_database(proteomes, hotspots, \"database_dirname\")\n\n# Visualisation with LoVis4u\ndrawing_manager = ilund4u.drawing.DrawingManager(proteomes, hotspots, parameters)\ndrawing_manager.plot_hotspot_communities()\ndrawing_manager.plot_proteome_communities()\n</code></pre>"},{"location":"API/usage_examples/#protein-and-proteome-annotation-modes","title":"Protein and proteome annotation modes","text":"<pre><code>import ilund4u\n\n\n# Creating a parameters object and loading config\nparameters = ilund4u.manager.Parameters()\nparameters.load_config(\"standard\")\n\n# Example of changing a particular parameter\nparameters.args[\"output_dir\"] = \"API_output_example\"\n\n# Database loading\ndatabase_manager = ilund4u.data_manager.DatabaseManager(parameters)\ndatabase = database_manager.load_database(db_path=\"path_to_the_db\")\n\n# Protein annotation mode\nfasta_file_path = \"ilund4u_data/guide/RloC.fa\" \ndatabase.protein_search_mode(query_fasta=fasta_file_path, query_label=\"RloC\")\n\n# Proteome annotation mode\ngff_file_path = \"ilund4u_data/guide/NC_001895.1.gff\" \ndatabase.proteome_annotation_mode(query_gff=gff_file_path)\n</code></pre>"},{"location":"Algorithm/algorithm/","title":"Algorithm description","text":"<p>Here we describe in details each step of the iLund4u workflow, which schematically is shown on the Figure 1. For description of generated files, visualisation, and frequently used parameters, see the Example-driven guide page. </p> Figure 1. iLund4u workflow."},{"location":"Algorithm/algorithm/#hotspot-annotation-mode","title":"Hotspot annotation mode","text":""},{"location":"Algorithm/algorithm/#defining-groups-of-protein-homologues-and-proteome-filtering","title":"Defining groups of protein homologues and proteome filtering","text":"<p>The first step is reading input data in gff format with the BCBio-gff library. While reading the files, we store all encoded protein sequences in a fasta file, which will be used as input for MMseqs clustering. In iLund4u objects we do not store sequences. Instead, iLund4u objects hold links to the corresponding files (a gff file for proteomes and the fasta file with amino acid sequences), which helps to avoid large memory usage. This step includes the initial filtering round, based on proteome size. By default, we do not take for proteomes analysis if they have fewer than 15 proteins (\\(N_{CDSs} &lt; 15\\)).The minimal proteome size can be changed with <code>-mps, --min-proteome-size &lt;int&gt;</code> parameter or the config parameter min_proteome_size. </p> <p>After gff files are read, we use the MMseqs2 clustering pipeline with all encoded proteins as input. All the clustering parameters can be adjusted using the configuration file (see Parameters section). By default, we use the following parameters: <code>--cluster mode 0 --cov-mode 0 --min-seq-id 0.25 -c 0.8 -s 7</code> (see mmseqs documentation for a detailed description).</p> <p>We then treat each cluster of proteins as a group of homologues. For each group we set the identifier (id) attribute as the id of the representative protein for the cluster.</p> <p>While processing the MMseqs results we iterate over all proteomes and all encoded proteins of each proteome, setting the attribute \u201dgroup\u201d for each CDS as id of cluster it belongs to. At this step we also perform deduplication; if during iteration we find a proteome in which the same full list of identified protein groups encoded in the same order along the genome has already been seen in a previously processed proteome - then this proteome will be excluded from the analysis. That is, we do not use multiple proteomes with identical proteome compositions in terms of groups of homologues and their genomic locations. This is in order to reduce redundancy and make it possible to find variable genes and hotspots. All filtered data and reasons for filtering are recorded in results file dropped_sequences.tsv. A second filtering process at this step is exclusion of proteomes for which proteome composition uniqueness (in the same assembly) is less than a set cutoff [default: 0.7] (this can be changed in the config file with the proteome_uniqueness_cutoff parameter). This is to exclude cases where large and potentially artifactual DNA duplications appear in the assembly. Proteome uniqueness is defined as: \\(pu_{i} = {|G_i|}/{N_i}\\), where \\(|G_i|\\) is the length (number of unique elements) of the set of protein groups of \\(i^{th}\\) proteome, and \\(N_i\\) - the number of CDSs of \\(i^{th}\\) proteome. </p>"},{"location":"Algorithm/algorithm/#proteome-network-construction","title":"Proteome network construction","text":"<p>The second major step is building the network of proteomes. The idea is to construct a weighted graph where each node, say \\(n_i\\), represents the corresponding \\(i^{th}\\) proteome. Each graph edge, say \\(e_{ij}\\), connecting \\(i^{th}\\) and \\(j^{th}\\) proteomes has weight \\(w_{ij}:\\) $$ w_{ij} = (\\frac{\\vert G_i \\cap G_j \\vert}{\\vert G_i \\vert} + \\frac{\\vert G_i \\cap G_j|}{|G_j|})\\cdot\\frac{1}{2} = \\frac{|G_i \\cap G_j|\\cdot(|G_i|+|G_j|)}{2|G_i||G_j|}; \\,\\, w_{ij}\\in[0,1]; \\, \\, w_{ij} = w_{ji} \\,\\, \\forall\\,i,j $$  where \\(|G_i|\\), \\(|G_j|\\) are lengths (number of unique elements) of the sets of protein groups of \\(i^{th}\\) and \\(j^{th}\\) proteomes, respectively. \\(|G_i \\cap G_j|\\) is the number of shared (overlapped) protein groups between \\(i^{th}\\) and \\(j^{th}\\) proteomes. The meaning of \\(w_{ij}\\) is average value of two fractions: the first \\(|G_i \\cap G_j|/|G_i|\\) is the number of shared homologues normalised to the length of \\(i^{th}\\) proteome set of protein groups, and the second \\(|G_i \\cap G_j|/|G_j|\\) - the same number of shared homologues with normalisation to the \\(j^{th}\\) proteome. The average of the two normalisations gives the required symmetric property with respect to its indexes. That is, \\(w_{ij}\\) is a metric of similarity of two genomes in terms of their proteome composition.</p> <p>While building the graph we write only edges where the weight passes the cutoff, which is by default \\(w_{ij}\\geq 0.7\\). The cutoff can be changed with the config parameter proteome_similarity_cutoff. This not only reduces memory usage for a large set of input data but also allows us to skip calculation of those \\(w_{ij}\\) if for the corresponding \\(i^{th}\\) and \\(j^{th}\\) proteomes we have \\(|G_j| &gt; |G_i|/0.7\\). In other words, if we cannot reach weight 0.7 due to the proteome size difference we do not start its calculation. This is implemented in the algorithm as follows: we first sort all proteomes by length of the protein group set. Then for each unique length value \\(l\\) we calculate the upper length cutoff  \\(L^u(l)\\): \\(L^u(l)=l/0.7\\). This allows us to limit the set of potential network connection indexes \\(c_i\\) for \\(i^{th}\\) proteome, which we need to calculate, from \\(i &lt; c_i\\) to \\(i &lt; c_i &lt; U(L^u(|G_i|))\\), where \\(U(L^u(|G_i|))\\) is the first index U of a proteome in which the set length is \\(|G_U| = L^u(|G_i|)\\).  </p> <p>Below we show the python-like pseudocode of the proteome network construction function. </p> Network construction python-like pseudocode <pre><code># Initialise the dictionary with the following structure: key - protein group of homologues (str),\n# value - list of proteomes where it is encoded (as collections.deque object) \nprotein_group_to_proteome_index = {\n    \"protein_group_1\": [ind1_1, ind1_2, ind1_3, ...],\n    \"protein_group_2\": [ind2_1, ...], ... }\n# Initialise the dictionary of upper index cutoffs:\n# key - size of protein group set (int), value - upper proteome index cutoff (int)\nupper_index_cutoff = {\n    \"protein_group_set_size_1\": upper_proteome_index_cutoff1, ... }\n# Create a graph object to represent the proteome network\nproteome_network = GraphObject()\n# Iterate over each proteome index\nfor i in indexes_of_proteomes:\n    # Initialise a dictionary to count number of shared homologues\n    counts_i = collections.defaultdict(int)\n    # Iterate over each protein group in the ith proteome\n    for protein_group in ith_proteome_protein_groups:\n       # Iterate over each proteome index associated with the current protein group\n        for j in protein_group_to_proteome_index[protein_group].copy():\n            # Retrieve the upper limit for the current proteome size\n            upper_limit = upper_index_cutoff[ith_proteome_size]\n            # Check if the current index j is within the acceptable range\n            if i &lt; j &lt; upper_limit:\n                counts_i[j] += 1\n            elif j &lt;= i:\n                # Remove j from the upper index cutoff if it is less than or equal to i\n                upper_index_cutoff[ith_proteome_size].popleft()\n            else:\n                # Exit the loop if j exceeds the upper limit\n                break    \n    # Normalise the counts to get the weights\n    weights_i = normalise(counts_i)\n    # Update the proteome network with new weights\n    proteome_network.update(weights_i)      \n</code></pre> <p>The complexity of the network construction step is \\(O(N_p\\cdot\\bar{|G|}\\cdot\\bar{P})\\), where \\(N_p\\) is the number of proteomes, \\(\\bar{|G|}\\) is the average proteome size, and \\(\\bar{P}\\) is the average size of protein group homologues (in how many proteomes its members are encoded). In general, it equals to the sum of shared homologues between proteomes that can be connected. </p> <p>In our tests, the elapsed times for the network building function on different architectures are as follows: i) 70 seconds for 20K Enterobacteria phage proteomes on M1 MacBook Pro laptop; ii) 500 seconds (8.4 minutes) for 222K plasmid proteomes on AMD 7413 (2.65 Ghz) computing node;   iii) 3610 seconds (60 minutes) for 563K phage proteomes on AMD 7413 (2.65 Ghz) computing node. The relatively large difference between the phage and plasmid proteome processing steps is due to the much higher density of the phage network (22M vs 860K written edges).</p>"},{"location":"Algorithm/algorithm/#proteome-community-detection","title":"Proteome community detection","text":"<p>Once the network is constructed, the next step is to define communities of proteomes to reveal the substructure of more and less closely related proteomes within the network. For this task we use the Leiden algorithm (Traag Vincent A et al, Scientific reports (2019)) of leidenalg python library implementation with the quality function based on the Constant Potts Model (CPM): \\(Q =  \\sum_{c}{e_c - \\gamma \\binom{n_c}{2}}\\), where \\(e_c\\) is the weighted number of edges inside community \\(c\\), \\(n_c\\) is the number of nodes in community \\(c\\), and \\(\\gamma\\) is the resolution parameter, which can be interpreted as the inner and outer edge density threshold (see Traag Vincent A. et al Physical Review E\u2014Statistical, Nonlinear, and Soft Matter Physics 2011). By default the resolution parameter is set as: \\(\\gamma = 0.5\\) and can be changed with the leiden_resolution_parameter_p config argument.</p> <p>We treat each proteome community as a group of relatively more closely related proteomes, which we can use for our subsequent analyses.</p>"},{"location":"Algorithm/algorithm/#defining-protein-group-classes","title":"Defining protein group classes","text":"<p>Within each proteome community we can find homologous protein groups that are encoded in different fractions of the proteomes. Those that are found in the majority of proteomes we call \u201cconserved\u201d (equivalent to \u201ccore\u201d in pan-genome analyses). Those protein groups that are found in a relatively small fraction of proteomes then we call \u201cvariable\u201d (equivalent to \u201ccloud\u201d in pan-genome analyses). Finally, those protein groups that are neither very variable nor very conserved, we refer to as \u201cintermediate\u201d  (\u201cshell\u201d in pan-genome analyses).</p> Figure 2 <p>For each protein group \\(g_i\\) within a proteome community \\(c\\) that consists of \\(N_c\\) proteomes, we the define protein group presence fraction as \\(f_i =  N_c^{g_i}/N_c\\), where \\(N_c^{g_i}\\) is the number of proteomes of \\(c\\) community that contain the \\(g_i\\) protein group. if \\(f_i &lt; 0.25\\) \\(g_i\\) group class is set as \"variable\", if $ 0.25 \\leq f_i \\leq 0.75$ then \\(g_i\\) is \"intermediate\", and, if $ f_i &gt; 0.75 $ we say that \\(g_i\\) is \"conserved\". </p> <p>The number of \"conserved\" protein groups  \\(G_c^{con}\\) of any community, say community \\(c\\), will be proportional to the average proteome size \\(\\overline{|G|}_c\\) of community members, while the number of \"variable\" protein groups \\(G_c^{var}\\) will be proportional to the \\(N_c\\) - community size (number of proteomes). That is, \\(G_c^{con} \\propto \\overline{|G|}_c\\) and \\(G_c^{var} \\propto N_c\\).  </p> <p>See the trivial example illustration on Figure 2 with the distribution of protein classes for a proteome community with all proteomes having \\(K\\) proteins, \\(K-1\\) being a member of the \"conserved\" protein class with \\(f = 1\\) and \\(1\\) unique protein class per each proteome with \\(f = 1/N\\). </p>"},{"location":"Algorithm/algorithm/#annotation-of-variable-islands","title":"Annotation of variable islands","text":"<p>After assigning the protein group class to each protein within each proteome community, we can now define \u201dvariable island\u201d objects. We say that a variable island is a region containing a set of adjacent and non-conserved proteins with at least one protein being \u201cvariable\u201d. The minimal size of an island is one protein (and it should be \u201cvariable\u201d) while no upper limit is set. In other words, a \u201dvariable island\u201d is a locus of \u201cvariable\u201d protein(s) without internal interruption by \u201cconserved\u201d ORFs but possibly containing \u201cintermediate\u201d proteins.</p> <p>Here it is important to note that the annotation process also depends on the \u201ccircularity\u201d property of a query genome. If a genome is circular then the first and the last genes will be considered to be neighbours. By default we consider each genome as circular, which can be changed with the <code>-ncg, --non-circular-genomes</code> parameter. Additionally, the circularity property of each genome can be set independently with <code>-gct, --genome-circularity-table &lt;path&gt;</code> (see the Parameters sections for details). </p> <p>In addition, if a genome is not considered circular, islands located at the ends of sequences (flanked from only one side) will not be considered in clustering to hotspots by default in the next steps (even though will be reported in accumulated statistics about all islands). This is the standard iLund4u behaviour. However, you can force to consider them by using the <code>-rnf, --report-not-flanked</code> parameter.</p> <p>The most important island attribute that defines all subsequent steps is a set of its conserved flanking protein groups. When a variable island region is defined we iterate over its flanking proteins encoded on both sides. Over iteration we add each \u201cconserved\u201d protein to the attribute of conserved neighbours (either left or right). The iteration on each side stops either when five neighbours (adjustable with neighbours_one_side_max_size config parameter) are collected on each side or when we pass over eight neighbour proteins (neighbours_max_distance config parameter). The minimal number of collected \u201cconserved\u201d neighbours is set by default as five (neighbours_min_size config parameter) and if an island does not reach the total number of five \u201cconserved\u201d neighbours it will not be taken in subsequent steps. The limit on distance from the island is set to avoid cases where an island has only a few \u201cconserved\u201d proteins in flanking genes and the whole locus around consists of variable proteins, which will not allow us to analyse it as a hotspot.</p>"},{"location":"Algorithm/algorithm/#island-network-construction-and-hotspot-annotation","title":"Island network construction and hotspot annotation","text":"<p>A hotspot is a set of variable islands encoded in different genomes, but in the same locus. This is, they have similar \u201cconserved\u201d flanking genes. From this definition we can consider each variable island as a locus characterised by its \u201cconserved\u201d neighbours (more precisely by the set of protein groups of its \u201cconserved\u201d neighbours) and build a network of islands where each node, say, \\(n_i^c\\), represents the corresponding \\(i^{th}\\) variable island in proteome community \\(c\\) and each edge, say \\(e_{ij}^c\\), connecting \\(i^{th}\\) and \\(j^{th}\\) variable islands has weight \\(w_{ij}^c\\): $$ w_{ij}^{c} = \\frac{|G_{ic} \\cap G_{jc}|\\cdot(|G_{ic}|+|G_{jc}|)}{2|G_{ic}||G_{jc}|} $$ where \\(|G_{ic}|\\), \\(|G_{jc}|\\) are lengths (number of unique elements) of the sets of protein groups of \\(i^{th}\\) and \\(j^{th}\\) island \"conserved\" neighbours, respectively. \\(|G_{ic} \\cap G_{jc}|\\) is the number of shared (overlapped) protein groups between \\(i^{th}\\) and \\(j^{th}\\) island \"conserved\" neighbours. Here the index \\(c\\) representing the proteome community indicates that each proteome community is treated independently and each community has its own network of variable islands. </p> <p>We want to note again that islands located at the ends of not circular sequences (flanked from only one side) will not be taken for this clustering step. This is the standard iLund4u behaviour. However, you can force to consider them by using the <code>-rnf, --report-not-flanked</code> parameter. </p> <p>As can be seen from the definition, the network is identical to a network of proteomes if we consider each island as a new proteome with its conserved flanking genes (without cargo proteins) as its proteins. That allows us to apply the same algorithm of network construction and community detection as already was described in the section of proteome network building. We only have minor differences in parameters: instead of minimal weight \\(w_{ij}\\geq 0.7\\) in the proteome network construction we have the threshold  \\(w_{ij}^c\\geq 0.65\\) (island_neighbours_similarity_cutoff config parameter) and the Leiden CPM resolution parameter \\(\\gamma\\) is now set as: \\(\\gamma = 0.55\\) (leiden_resolution_parameter_i config parameter) instead \\(0.5\\) for the proteome network. </p> <p>When the network of islands is constructed in each proteome community, and we find communities in this network we can postulate that each community is a hotspot. From this definition we can see that a hotspot represents a cluster in a network with a high density of inner connections, which means they share a similar set of \u201cconserved\u201d flanking protein groups. At this step we add filtering. Firstly, the presence cutoff: if a hotspot, say \\(h_i^c\\), is found in proteome community \\(c\\) of the size \\(N_c\\) and consists of islands from \\(N_c^{h_i}\\) proteomes, then the hotspot is kept only if \\(N_c^{h_i}/N_c \\geq 0.3\\) (cutoff can be adjusted with hotspot_presence_cutoff config parameter or <code>-hpc, --hotspot-presence-cutoff</code> argument). ). In other words, clusters of variable islands that are found only in a small fraction of proteomes within a proteome community are not considered as hotspots. Additionally, if we have several islands on the same proteome clustered together in one hotspot then only the one with the highest sum of weights relative to other community members will be kept in the hotspot. This can happen if two islands are divided by few (one to three) conserved proteins and this step allows us to keep only the one that has a higher overlapping rate of flanking protein groups compared with the others.</p>"},{"location":"Algorithm/algorithm/#hotspot-merging","title":"Hotspot merging","text":"<p>We can expect that hotspots found in different proteome communities could have the same conserved neighbours due to the modularity of mobile elements such as phages or plasmids. In order to detect and merge such hotspots together we build the network of hotspots and apply the Leiden algorithm for community detection as was described above. In that case, for each hotspot we define the attribute signature, which represents a set of protein groups that are found in at least of \\(0.75\\) of its island's \"conserved\" flanking proteins (cutoff can be adjusted with hotspot_signature_presence_cutoff config parameter). That is, hotspot signature is a set of protein groups that are found as \u201cconserved\u201d flanking genes in the majority of its islands. Formally, in the case of hotspot network each node, say \\(h_i^c\\) represents the corresponding \\(ith\\) hotspot of proteome community \\(c\\) and each edge, say \\(e_{ij}\\), connecting \\(i^{th}\\) and \\(j^{th}\\) hotspots has weight \\(w_{ij}\\): $$ w_{ij} = \\frac{|G_i \\cap G_i|\\cdot(|G_i|+|G_j|)}{2|G_i||G_j|} $$ where \\(|G_i|\\), \\(|G_j|\\) are lengths (number of unique elements) of the set of protein groups of \\(i^{th}\\) and \\(j^{th}\\) hotspot signature, respectively. \\(|G_{i} \\cap G_{j}|\\) is the number of shared (overlapped) protein groups between \\(i^{th}\\) and \\(j^{th}\\) hotspot signatures. Network building and community detection parameters set by default are the same as in island network construction and can be adjusted with hotspot_similarity_cutoff and leiden_resolution_parameter_h config parameters.</p>"},{"location":"Algorithm/algorithm/#additional-functional-annotation","title":"Additional functional annotation","text":"<p>For additional annotation of proteins encoded either as cargo of annotated hotspot islands or as flanking genes we use pyhmmer API. The reason of using pyhmmer instead of MMseqs2 sequence vs profile search is that for several databases that are essential for hotspot protein annotation we have only hmmer database of profiles available without provided MSA or HMM models compatible with hh-suite or MMseqs \ud83e\udee4. In order to reduce the running time we use only representative proteins of each protein group as a query set for searching. As was defined above, each cluster of proteins is considered as a set of homologues, then, the search results are attributed to each protein group based on its representative sequence.</p> <p>The list of used databases and their versions or date of retrieval: </p> <ul> <li>Antimicrobial resistance genes (AMR): AMRFinderPlus: 02.05.2024.2</li> <li>Anti-Defence: dbAPIS_Acr: 19.09.2023</li> <li>Defence systems: DefenceFinder: 1.2.4, CasFinder: 3.1.0; PADLOC: (v. 22.10.2024)</li> <li>Virulence factors: VFDB : 10.05.2024</li> </ul>"},{"location":"Algorithm/algorithm/#lovis4u-visualisation","title":"LoVis4u visualisation","text":"<p>The last step of the algorithm is using of our tool LoVis4u for visualisation of each hotspot community and proteome community. We recommend you to read the Example-driven guide page for examples of visualisation with detailed descriptions. However, there is one step in the visualisation of hotspot communities we want to highlight here: we can expect that some variable islands of one hotspot community from different proteomes could have the same set of cargo and flanking protein groups. In that case iLund4u keeps only one of them for visualisation in order to avoid duplication on visualisation. The full list of hotspot loci is available in annotation tables.</p> <p>You can see an example with visualisation of proteome community that consists of 716 proteomes (LoVis4u community visualisation). The community has one hotspot containing 875 diverse cargo protein groups with 16 of them being annotated as Defence related proteins, 6 as virulence factors, 5 as AMR proteins, and 2 as anti-defence  (LoVis4u hotspot visualisation).</p>"},{"location":"Algorithm/algorithm/#ilund4u-database","title":"iLund4u database","text":"<p>Optionally during the hotspot annotation step you can build the corresponding to results iLund4u database that holds proteome and hotspot objects and can be used as query for two other modes: protein and proteome annotation. iLund4u database consists of the set of files (non-binary) that holds information about protein, proteome, CDS, island and hotspot objects.</p> Figure 3 <p>iLund4u has two precomputed databases of hotspots built on phage and plasmid sequences. The database of phages was built by running hotspot annotation mode on all available PhageScope database sequences (~870K sequences, version of September 2024). For the plasmids database we took IMG/PR database of plasmids (~700K sequences, version of June 2024).  </p> <p>Database sizes: Phages: 6.48GB; Plasmids: 1.07GB </p> <p>See the Example-driven guide page for details about database building and downloading.</p>"},{"location":"Algorithm/algorithm/#protein-annotation-mode","title":"Protein annotation mode","text":"<p>This mode uses the guilt by association (or rather in our case we can call it guilt by location) approach. In short, if an unannotated protein is found in a locus containing several for example phage defence systems or in a hotspot in which the annotated cargo proteins are frequently defence systems, then other genes in the locus or in the hotspot are more likely to be involved in this function. That is, the protein location as well as its mobility through hotspots is an important property that can help in functional annotation of proteins with unknown function.</p>"},{"location":"Algorithm/algorithm/#homology-search","title":"Homology search","text":"<p>In order to find hotspots containing homologues of your query protein of interest, we need to define homologues as the first step. iLund4u has two modes for this. Both start with an MMseqs2 search of the query protein versus all proteins of the iLund4u database. By default the following mmseqs search parameters are used: <code>-e 1e-5 -s 7</code> and subsequent query and target coverage cutoffs set as 0.6. Then there are two alternatives:</p> <ol> <li>In the \u201dgroup\u201d search mode the query protein is assigned to the same homologous group as that of the best search hit protein.</li> <li>In the \u201dproteins\u201d search mode all target proteins that pass the search cutoffs are considered to be homologues of your query without focusing on corresponding protein groups.</li> </ol>"},{"location":"Algorithm/algorithm/#hotspots-retrieval","title":"Hotspots retrieval","text":"<p>If homologues are found in the database, iLund4u searches for hotspots that contain these proteins either as cargo or flanking genes. The search results contain a LoVis4u visualisation and the set of annotation tables. See the Example-driven guide page for details.</p>"},{"location":"Algorithm/algorithm/#proteome-annotation-mode","title":"Proteome annotation mode","text":"<p>This mode is used to annotate hotspots even if the query is a single proteome, but it has a community of similar proteomes in the iLund4u database. If you have sequenced, let say a phage, then you can search it versus the iLund4u database of all phages and in case there is a proteome community of related phages the already known hotspots can be mapped to your query proteome. Additionally, it gives the ability to define \u201cconserved\u201d proteins in your query proteome, whose homologues are found in the majority of similar proteomes and \u201cvariable\u201d. See the detailed definition in the corresponding sections above. </p>"},{"location":"Algorithm/algorithm/#homology-search_1","title":"Homology search","text":"<p>The first step is to define the protein group for each CDS of the query proteome. For this task the \u201dgroup\u201d search mode of protein annotation mode is used.</p>"},{"location":"Algorithm/algorithm/#defining-proteome-communities","title":"Defining proteome communities","text":"<p>When protein groups are defined, we can calculate proteome similarity weights \\(w_{qi}\\) from the query proteome \\(q\\) to all other database proteomes. All thresholds for weights are set the same as for the initial network building described above. </p>"},{"location":"Algorithm/algorithm/#annotation-of-variable-islands-and-hotspot-mapping","title":"Annotation of variable islands and hotspot mapping","text":"<p>If a community of proteomes was assigned for a query proteome we can continue with defining variable islands. We define query protein group class values as already defined in the corresponding community. If we have a new protein group in query proteome it is set as new \"variable\" protein group. Each variable island then embedded in the network of islands in order to map already annotated hotspots. </p> <p>The search results contain LoVis4u visualisation including mapped islands, hotspots, the full-length proteome community, as well as several annotation tables. See the Example-driven guide page for details.</p>"},{"location":"ExampleDrivenGuide/cmd_guide/","title":"Example-driven guide","text":"<p>Here we show usage examples of the iLund4u command-line interface. Through this guide we will show you step-by-step how to use our tool in three available modes: hotspot annotation, protein search, and proteome annotation.</p>"},{"location":"ExampleDrivenGuide/cmd_guide/#before-start","title":"Before start","text":""},{"location":"ExampleDrivenGuide/cmd_guide/#data-preparation","title":"Data preparation","text":"<p>The necessary sample data as well as adjustable tool configuration files are provided by iLund4u at the post-install step:   <code>ilund4u --data</code> which copies the folder ilund4u_data to your working directory. </p> <p>If you work on a Linux machine after installation you should run: <code>ilund4u --linux</code> This command replaces the tools paths (MMseqs2) in the pre-made config files from the MacOS (default) to the Linux versions. If you run this command for fun and want to change it back you can use <code>ilund4u --mac</code>.</p> <p>Downloading HMM models: ilund4u uses pyhmmer for additional functional annotation of proteins with hmmscan versus a set of databases. You can download these database from our server (data-sharing.atkinson-lab.com/iLund4u) by running the following command: <code>ilund4u --get-hmms</code> List of databases: AMR: AMRFinderPlus (v. 02.05.2024.2); Anti-defence: dbAPIS_Acr (v. 19.09.2023); Defence: DefenceFinder (v. 1.2.4), CasFinder (v. 3.1.0); PADLOC (v. 22.10.2024); Virulence: VFDB (v. 10.05.2024).</p> <p>For this demonstration we will use pharokka-generated gff files with sequences of 70 P2-like phages. Gff files are stored at: ilund4u_data/guide/gff_files.  The main difference of pharokka/prokka generated gff files from a regular gff3 (for ex. which you can download from the NCBI) is that in addition to annotation rows they contain the corresponding nucleotide sequence in fasta format. </p> <p>Note: Each gff file should contain a corresponding nucleotide sequence. iLund4u is designed to handle pharokka/prokka produced annotation files.  </p>"},{"location":"ExampleDrivenGuide/cmd_guide/#help-messages","title":"Help messages","text":"<p>To show the main iLund4u help message you can use <code>ilund4u --help</code>. To show mode-specific help messages use: <code>ilund4u --help [mode]</code> (e.g. <code>ilund4u --help hotspots</code>)</p>"},{"location":"ExampleDrivenGuide/cmd_guide/#building-compatible-gff-files-based-on-nucleotide-sequences","title":"Building compatible gff files based on nucleotide sequences","text":"<p>If your query set of sequences for hotspot annotation contains only nucleotide fasta files, below we will provide the efficient way of using pharokka (phage annotation pipeline) and prokka (prokaryotic genome annotation pipeline) for preparing gff files compatible with iLund4u. </p>"},{"location":"ExampleDrivenGuide/cmd_guide/#using-pharokka-for-annotation-of-phage-genomes","title":"Using pharokka for annotation of phage genomes","text":"<p>Before running pharokka you need to install pharokka databases (and pharokka itself, of course). Due to the number of non-python dependencies we recommend using a conda environment for this task. See pharokka documentation page for clear instructions.</p> <p>Firstly, if you have up to ~1000 nucleotide sequences we recommend merging them into one fasta file (could be done simply by using cat: <code>cat folder_with_fasta_files/*.fa &gt; merged_fasta.fa</code>. We recommend to merge multiple sequences to one file and using meta mode since in that case we do not load databases for each contig while annotating. </p> <p>Then, you can use pharokka in meta mode with one command: <pre><code>pharokka.py -i merged_fasta.fa  -o pharokka_output  --meta --split -t Num_of_threads \\\n    --skip_mash --dnaapler  -database path_do_pharokka_database \n</code></pre></p> <p>Gff files for each query contig will be stored at: pharokka_output/single_gffs</p> <p>In case you have thousands of input sequences we recommend splitting them into several fasta files with ~1000 sequences in each. Then you can run pharokka on these sets of sequences. One of the ways to parallelise this process is shown below: <pre><code>for f in folder_with_fasta/*.fa; do fb=$(basename $f); nm=${fb//.fa/}; \\ \n    echo pharokka.py -i $f  -o pharokka_batches/$nm  --meta --split -t Num_of_threads \\\n    --skip_mash --dnaapler  --database bin/pharokka/databases ; \\\n    done | parallel -j Num_of_parallel_processes\n</code></pre> Here you set Num_of_threads\u30fbNum_of_parallel_processes equal to the number threads you want to use.</p> <p>Then you can move generated gff files to one folder using: <pre><code>for f in pharokka_batches/*; do mv $f/single_gffs/* pharokka_gffs/; done\n</code></pre></p>"},{"location":"ExampleDrivenGuide/cmd_guide/#using-prokka-for-annotation-of-prokaryotic-genomes","title":"Using prokka for annotation of prokaryotic genomes","text":"<p>Prokka does not have an equivalent of meta mode which we used in pharokka in the example above. Therefore, for any number of input sequences we prefer running prokka independently for each contig instead of dividing more complex gff files, which requires an additional step. Again, for prokka installation instruction and parameter description see  prokka documentation page.</p> <p>In case your input fasta query files (one sequence per file) are located in single_records folders you can run: <pre><code>for f in single_records/*.fa;  do fb=$(basename $f); nm=${fb//.fa/}; \\ \n    echo prokka --outdir prokka/$nm --prefix $nm --quiet --cpus 1 $f ; \\\n    done | parallel -j num_of_available_threads\n</code></pre> Then you can move generated gff files to one folder using: <pre><code>for f in prokka/*; do fb=$(basename $f); echo mv  $f/$fb.gff prokka_gffs/ ; done | parallel\n</code></pre></p>"},{"location":"ExampleDrivenGuide/cmd_guide/#ilund4u-workflow","title":"iLund4u workflow","text":"Figure 1. iLund4u workflow"},{"location":"ExampleDrivenGuide/cmd_guide/#hotspot-annotation-mode","title":"Hotspot annotation mode","text":""},{"location":"ExampleDrivenGuide/cmd_guide/#run-with-default-parameters","title":"Run with default parameters","text":"<p>Let's start with the main mode of iLund4u - hotspot annotation (see the workflow Figure 1). The only mandatory argument for this mode is a folder path containing compatible gff files. <pre><code>ilund4u hotspots --gff ilund4u_data/guide/gff_files\n</code></pre> With the first argument after ilund4u we specified the mode of usage <code>hotspots</code>.   </p> Log messages for the example run with default parameters <p><pre><code>\u25cb Reading gff files...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 70/70\n\u25cb Running mmseqs for protein clustering...\n  \u29bf 166 clusters for 2847 proteins were found with mmseqs\n  \u29bf mmseqs clustering results were saved to ilund4u_2024_10_01-23_11/mmseqs/mmseqs_clustering.tsv\n\u25cb Processing mmseqs results ...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 70/70\n  \u29bf 0 proteomes were excluded after proteome deduplication and filtering\n\u25cb Proteomes network construction...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 70/70\n  \u29bf Network building elapsed time: 0.03 sec\n  \u29bf Proteomes network with 2413 connections was built\n  \u29bf Network was saved as ilund4u_2024_10_01-23_11/proteome_network.gml\n\u25cb Proteome network partitioning using the Leiden algorithm...\n  \u29bf 1 proteome communities were found\n  \u29bf 1 proteomes communities with size &gt;= 10 were taken for further analysis\n  \u29bf 0 proteomes from smaller communities were excluded from the analysis\n\u25cb Defining protein classes within each community...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 1/1\n\u25cb Annotating variable islands within each proteome...\n  \u29bf 176 variable regions are annotated in 70 proteomes (2.514 per proteome)\n\u25cb Island network construction within each proteome community...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 1/1\n  \u29bf Island network building elapsed time: 0.09 sec\n\u25cb Searching for hotspots within each community...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 1/1\n  \u29bf 2 hotspots were found in 1 proteome communities  (Avg: 2.0 per community)\n  2/2 hotspots are flanked (consist of islands that have conserved genes on both sides)\n\u25cb Preparing data for additional island protein annotation with pyhmmer hmmscan...\n  \u29bf Running pyhmmer hmmscan versus DefenceFinder and CasFinder Databases...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 135/135\n    Number of hits: 21\n  \u29bf Running pyhmmer hmmscan versus Virulence Factor Database...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 135/135\n    Number of hits: 3\n  \u29bf Running pyhmmer hmmscan versus Anti-Prokaryotic Immune Systems Database (dbAPIS)...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 135/135\n    Number of hits: 0\n  \u29bf Running pyhmmer hmmscan versus AMRFinderPlus Database...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 135/135\n    Number of hits: 0\n\u25cb Hotspot network construction...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2/2\n\u25cb Hotspot network partitioning using the Leiden algorithm...\n  \u29bf 0 hotspots were merged to 0 not singleton communities\n\u25cb Hotspot and island statistics calculation...\n\u25cb Protein group statistics calculation...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2/2\n\u25cb Visualisation of hotspot communities using lovis4u...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2/2\n\u25cb Visualisation of proteome communities with corresponding hotspots using lovis4u...\n  \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 1/1\n</code></pre> Note: you can use <code>-q</code>, <code>--quite</code> command-line argument to hide progress messages. </p> <p>By running this command, an output folder named ilund4u_{current_date} (e.g. ilund4u_2024_06_25-16_36) will be created. The name of the output folder can be changed with <code>-o &lt;output_folder_name&gt;</code>.  </p> Output folder structure <ul> <li>all_proteins.fa - fasta file with all proteins encoded by input genomes.</li> <li>mmseqs - folder containing mmseqs clustering results.  </li> <li>dropped_sequences.tsv - table containing information about dropped sequences during the deduplication process (in order not to analyse several sequences with identical sets of protein groups).</li> <li>proteome_network.gml - proteome network in gml format.</li> <li>proteome_communities.tsv - proteome communities and annotation based on proteome network partitioning using the Leiden algorithm.</li> <li>protein_group_classes.tsv - annotation table for each protein group (set of homologous proteins defined by mmseqs clustering) with a group presence in a community of proteomes and corresponding class.</li> <li>island_networks - folder containing networks of islands in gml format.</li> <li>island_annotation.tsv - table with annotation of islands (regions of non-conserved proteins in each proteome).</li> <li>island_rep_proteins.fa - fasta file with representative island proteins.  </li> <li>hmmscan - folder containing pyhmmer search output.</li> <li>hotspot_annotation.tsv - annotation table of identified hotspots.</li> <li>hotspot_network.gml - network of hotspots in gml format.</li> <li>hotspot_communities.tsv - hotspot communities -we can expect that hotspots identified in different proteome communities could have the same flanking genes That is why we cluster together such hotspots. This file describes such communities.</li> <li>hotspot_community_annotation.tsv - annotation table of identified hotspot communities.</li> <li>protein_group_accumulated_statistics.tsv - annotation table of protein groups identified as \"variable\" or/and encoded in hotspots.</li> <li>lovis4u_proteome_communities - folder with LoVis4u visualisation of proteome communities.</li> <li>lovis4u_hotspots - folder with LoVis4u visualisation of each hotspot community.</li> <li>lovis4u_hotspots_annotation - folder with LoVis4u annotation tables for hotspot visualisation.  </li> </ul> Figure 2 <p>In the example above we used 70 P2-like phages that are known to be in one community (group of related proteomes) based on our iLund4u hotspot annotation run on 873K phage sequences. As expected we find all sequences to be members of one community of proteomes with average proteome composition similarity (fraction of shared protein homologues) equal to 0.856 (see proteome_communities.tsv table and proteome_network.gml network files).</p> <p>On the visualisation of full length sequences from the proteome community (Fig. 2) built with LoVis4u we can see two annotated hotspots highlighted. The presence of hotspots in P2-like phages has previously been reported and it is known that these hotspots (highlighted in red is known as old/tin-encoding, while highlighted in blue as Z/fun-encoding) contain diverse anti-phage defence cargo proteins  (See Francois, et al., 2022; Vassallo, et. Al, 2022). iLund4u indicates that some genomes contain virulence factors (cytolethal distending toxin (CDT) [VF0987]) in the same hotspot locus (it was reported in  Francois, et al., 2022)).</p> <p>On the proteome community figure (Fig. 2) you can see that most of the differences in terms of proteome composition between phages are concentrated in these two hotspots. However, there are also other variable regions with at least one non-conserved protein coding gene distributed across genomes. However, iLund4u does not merge these with other hotspots due to not passing the presence cutoff. Such genes are shown with in dark grey while conserved (or core) protein coding genes are light grey.</p> <p>Below you can see LoVis4u visualisation of each hotspot loci - cargo proteins and up to 10 conserved flanking genes. </p> LoVis4u visualisation of old/tin-encoding hotspot (0-0) (Figure 3) <p> </p> LoVis4u visualisation of Z/fun-encoding hotspot (0-1) (Figure 4) <p> </p>"},{"location":"ExampleDrivenGuide/cmd_guide/#building-the-ilund4u-database","title":"Building the iLund4u database","text":"<p>As you can see in the workflow (Fig. 1), during hotspot annotation, iLund4u can also create a database that then can be used for two other modes: protein and proteome annotation. To create a database in addition to hotspot annotation you need to add the argument <code>-o-db &lt;dir name&gt;</code> or <code>--output-database &lt;dir name&gt;</code> with the specified database folder name. For example: <pre><code>ilund4u hotspots --gff ilund4u_data/guide/gff_files -o P2-like_phages_guide  -o-db P2-like_phages_guide_DB\n</code></pre> This database, which is built on a guide set of 70 P2-like phages annotation gff files, is also available in the ilundu4_data folder under the following path: ilund4u_data/guide/like_phages_guide_DB. </p>"},{"location":"ExampleDrivenGuide/cmd_guide/#specifying-the-circularity-of-sequences","title":"Specifying the circularity of sequences","text":"<p>Another important optional argument is  <code>-gct, --genome-circularity-table &lt;path&gt;</code> which takes path to a table containing information for each genome about whether it is circular or not. Format: tab-separated; column names: id, circular. Values 1 or 0. For genomes in which the id is not listed in this table, the default value will be 0 (non circular) ; You can use <code>-cg/--circular-genomes</code> to change the default value to 1 (circular). This parameter is important since if the genome is circular then the first and last proteins will be considered as neighbours, which affects the flanking gene sets of proteins located near the ends of sequences. Since for these phages in the example we have circular genomes and this value is default, you can see that the breaks between the end and start of contigs are ignored for hotspot 0-0 (Fig. 3).  </p> <p>Please be aware that if a genome is not considered circular, islands located at the ends of sequences (flanked from only one side) will NOT be considered in clustering to hotspots by default. This is the standard iLund4u behaviour. However, you can force to consider them by using the <code>-rnf, --report-not-flanked</code> parameter. Note that each result table (for islands, hotspots, hotspot communities, and protein groups) includes a column indicating whether the object is flanked or not.</p>"},{"location":"ExampleDrivenGuide/cmd_guide/#protein-annotation-mode","title":"Protein annotation mode","text":""},{"location":"ExampleDrivenGuide/cmd_guide/#run-with-default-parameters_1","title":"Run with default parameters","text":"<p>For the demonstration of protein annotation mode we took the protein WP_171864480.1 , which  is annotated as E. coli RloC in the Defense Finder database. We chose RloC since it is an annotated cargo protein and is found on both of our annotated hotspots. Here we can also start with only the mandatory arguments set: <code>-fa &lt;fasta_path&gt;</code> - path to a fasta file with query protein amino acid sequence and <code>-db &lt;db_path&gt;</code> - path to iLund4u database folder. Again, optionally you can specify an output folder name using <code>-o &lt;folder_name&gt;</code> parameter. <pre><code>ilund4u protein -fa  ilund4u_data/guide/RloC.fa -db ilund4u_data/guide/P2-like_phages_guide_DB -o RloC_search\n</code></pre></p> Log messages for the example run with default parameters <pre><code>\u25cb Loading database from ilund4u_data/guide/P2-like_phages_guide_DB...\n\u25cb Loading cds objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2847/2847\n\u25cb Loading island objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 176/176\n\u25cb Loading proteome objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 70/70\n\u25cb Loading hotspot objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2/2\n\u29bf The ilund4u_data/guide/P2-like_phages_guide_DB database was successfully loaded\n\u25cb Running mmseqs for protein search versus the database of proteins...\n    \u29bf A homologous group was found for 1/1 query protein\n\u25cb Searching for hotspots with your query protein homologues...\n\u29bf Query protein homologues were found in 2 hotspot communities (2 hotspots) on 4 islands\n    Found as cargo: 4, as flanking gene: 0\n    4/4 islands where found as cargo are both side flanked (have conserved genes on both sides)\n\u29bf Homologous proteins were saved to RloC_search/homologous_proteins.fa and the MSA was visualised with MSA4u\n\u25cb Visualisation of the hotspot(s) with your query protein homologues using lovis4u...\n\u29bf Done!\n</code></pre> Output folder structure <ul> <li>mmseqs_homology_search_full.tsv - table with identified homologues.</li> <li>mmseqs - folder containing mmseqs results and logs.  </li> <li>homologous_proteins.fa - fasta file containing query protein and identified homologues.</li> <li>homologous_proteins_aln.fa - alignment fasta file containing query protein and identified homologues built with MAFFT.</li> <li>msa4u_homologous_proteines.pdf - MSA4u visualisation of the protein alignment.</li> <li>protein_group_stat.tsv - annotation table of protein group(s) that are homologoues to the query. </li> <li>found_island_annotation.tsv - annotation table of islands containing query protein homologues.</li> <li>found_hotspot_annotation.tsv - annotation table of hotspots containing query protein homologues. </li> <li>lovis4u_full - folder containing LoVis4u visualisation of hotspots where query protein homologues are found.</li> <li>lovis4u_with_query - folder containing LoVis4u visualisation of hotspots with only loci where query protein homologues are found.</li> <li>lovis4u_hotspots_annotation - folder with LoVis4u annotation tables for hotspot visualisation.  </li> </ul> <p>Note: iLund4u searches for homology not only versus cargo proteins but against flanking genes as well.</p>"},{"location":"ExampleDrivenGuide/cmd_guide/#specifying-homology-search-mode-and-setting-up-labels","title":"Specifying homology search mode and setting up labels","text":"<p>Protein annotation mode has three important optional arguments that we want to describe here.  </p> <p>The first one: <code>-hsm, --homology-search-mode &lt;group|proteins&gt;</code>  which specifies the mode of search for homologous proteins in the database. If \"group\" is selected, a query protein is assigned to the same homologous group as that of the best search hit protein. In \"proteins\" mode, all target proteins that pass the cutoffs are considered to be homologues. By default, \"group\" mode is set. We recommend to use \"proteins\" mode if you want to find more diverse set of homologues.  </p> <p>The second is: <code>-rnf, --report-not-flanked</code> controls whether islands and hotspots located at the ends of sequences (flanked from only one side) will be considered in search. The importance of it described in the hotspot annotation section which has exactly the same argument. </p> <p>The last one we want to highlight is <code>-ql, --query-label &lt;str&gt;</code> which simply changes the label of your query protein homologues on LoVis4u visualisation. For example: <pre><code>ilund4u protein -fa  ilund4u_data/guide/RloC.fa -db ilund4u_data/guide/P2-like_phages_guide_DB \\\n    -o RloC_search -ql \"RloC from the guide\"\n</code></pre> Below you can see visualisation of the same 0-0 and 0-1 hotspots with highlighted RloC proteins</p> LoVis4u visualisation of old/tin-encoding hotspot (0-0) with RloC (Figure 5) <p> </p> LoVis4u visualisation of Z/fun-encoding hotspot (0-1) with RloC (Figure 6) <p> </p> <p>Additionally, iLund4u returns visualisation (on one figure) of all loci where only query protein homologues are found (Fig. 7):</p> Figure 7"},{"location":"ExampleDrivenGuide/cmd_guide/#proteome-annotation-mode","title":"Proteome annotation mode","text":""},{"location":"ExampleDrivenGuide/cmd_guide/#run-with-default-parameters_2","title":"Run with default parameters","text":"<p>For demonstration of proteome annotation mode we took Enterobacteria phage P2 NC_001895.1 that is also known to be a member of the community of listed test set of phages based on our analysis of all phages (and which actually contains old/tin and Z/fun). In this mode mandatory arguments are <code>-gff &lt;gff_path&gt;</code> - path to gff file (again, pharokka/prokka annotated) of your query proteome and <code>-db &lt;db_path&gt;</code> - path to a database. Optionally you can specify output folder name using <code>-o &lt;folder_name&gt;</code> parameter. <pre><code>ilund4u proteome -gff  ilund4u_data/guide/NC_001895.1.gff \\\n    -db ilund4u_data/guide/P2-like_phages_guide_DB -o NC_001895.1_search\n</code></pre></p> Log messages for the example run with default parameters <pre><code>\u25cb Loading database from ilund4u_data/guide/P2-like_phages_guide_DB...\n\u25cb Loading cds objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2847/2847\n\u25cb Loading island objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 176/176\n\u25cb Loading proteome objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 70/70\n\u25cb Loading hotspot objects...\n\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 2/2\n\u29bf The ilund4u_data/guide/P2-like_phages_guide_DB database was successfully loaded\n\u25cb Reading gff file...\n\u25cb Running mmseqs for protein search versus the database of proteins...\n  \u29bf A homologous group was found for 39/42 query proteins\n\u25cb Searching for similar proteomes in the database network\n\u25cb Preparing data for protein annotation with pyhmmer hmmscan...\n  \u29bf Running pyhmmer hmmscan versus DefenceFinder and CasFinder Databases...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 42/42\n    Number of hits: 1\n  \u29bf Running pyhmmer hmmscan versus Virulence Factor Database...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 42/42\n    Number of hits: 0\n  \u29bf Running pyhmmer hmmscan versus Anti-Prokaryotic Immune Systems Database (dbAPIS)...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 42/42\n    Number of hits: 0\n\u29bf Running pyhmmer hmmscan versus AMRFinderPlus Database...\n    \u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9\u25c9 42/42\n    Number of hits: 0\n\u29bf 70 similar proteomes were found in the database network with max proteome similarity = 0.92\n\u29bf The query proteome was assigned to a community (id: 0) with connection to 1.0 of its members with avg weight = 0.86\n\u29bf Protein class distribution in query proteome: 36 conserved, 5 variable, 1 intermediate\n\u29bf 3 variable islands were annotated in the query proteome\n\u29bf 2 of annotated variable islands were mapped to the database hotspots\n\u25cb Lovis4u visualisation of communities and proteomes...\n\u25cb Lovis4u visualisation found hotspots..\n\u29bf Done!\n</code></pre> Output folder structure <ul> <li>all_proteins.fa - fasta file with query sequence proteins.</li> <li>mmseqs - folder containing mmseqs results and logs.  </li> <li>mmseqs_homology_search_full.tsv - table containing results of finding protein homologues for all proteins from the query proteome.</li> <li>hmmscan - folder containing additional functional annotation of all query sequence proteins versus a set of databases with pyhmmer. </li> <li>query_proteome_network.tsv - sequence id and weights of all edges to the database network of proteomes.</li> <li>similar_proteome_communities.tsv - table containing annotation of proteome communities with similar proteomes to your query.    - query_protein_clusters.tsv - annotation of query sequence protein classes - variable/intermediate/conserved depending on their presence in proteome community.</li> <li>protein_group_stat.tsv - annotation table of protein group(s) that are homologoues to the query proteins. </li> <li>island_to_hotspot_mapping.tsv - mapping of variable islands (regions with at least one non-conserved proteins in your query sequence) that were annotation as hotspots in the database.</li> <li>annotation_of_mapped_hotspots.tsv - annotation of mapped hotspots containing their description and statistics.</li> <li>lovis4u_proteome_community_hotspots.pdf* - LoVis4u visualisation of all members of proteome community with your query sequence where hotspots are highlighted.</li> <li>lovis4u_proteome_community.pdf - LoVis4u visualisation of all members of proteome community with your query sequence where highlighted all non-conserved proteins (found in a small fraction of proteomes within the community).  </li> <li>lovis4u_query_proteome_hotspot.pdf - LoVis4u visualisation of only query proteome with highlighted hotspots.</li> <li>lovis4u_query_proteome_variable.pdf - LoVis4u visualisation of only query proteome with highlighted non-conserved proteins.</li> <li>lovis4u_hotspot_full - folder containing LoVis4u visualisation of hotspots with your query proteome.</li> <li>lovis4u_hotspot_with_query - folder containing LoVis4u visualisation of hotspots but only with your query proteome.</li> <li>lovis4u_hotspots_annotation - folder with LoVis4u annotation tables for hotspot visualisation.  </li> </ul>"},{"location":"ExampleDrivenGuide/cmd_guide/#review-of-visualisation-types","title":"Review of visualisation types","text":"<p>This mode returns several types of visualisation built with LoVis4u. Firstly, it includes visualisation of your query proteome together with all other proteomes from a proteome community to which your query sequence was assigned.</p> LoVis4u visualisation of a proteome community + query sequence with hotspots highlighted (Figure 8) <p> </p> LoVis4u visualisation of a proteome community + query sequence with non-conserved proteins highlighted (Figure 9) <p> </p> <p>Additionally, iLund4u generates LoVis4u visualisation of only your query sequence highlighting annotated hotspots:</p> Figure 10 <p>Or highlighting non-conserved proteins based on a group of homologues corresponding to a protein and its presence in a community of proteomes. As you can see, non-conserved proteins that were not assigned to any hotspot are shown in more dark grey colour on the Fig. 10.</p> Figure 11 <p>Moreover, iLund4u plots each annotated hotspots in the same way: figures containing all proteomes from a community with your query proteome and only loci of your proteome (Fig. 12, 13).</p> LoVis4u visualisation of old-tin hotspot (0-0) with query sequence (Figure 12) <p> </p> <p>As you can see on Fig. 12, we have a hit to a Defense Finder database with a cargo protein encoded in our query sequence on the old-tin hotspot (as expected to old even though tin remains unannotated by pharokka and pyhmmer).</p> LoVis4u visualisation of Z/fun hotspot (0-1) with query sequence (Figure 13) <p> </p> <p>As we mentioned above, we also have LoVis4u visualisation of each hotspot with only query proteome locus. Below on Fig. 14 we show old-tin hotspot (0-0) locus of our query P2 phage. We see (as expected) that one out of three cargo proteins has a hit to the OLD exonuclease of Defense Finder database. </p> Figure 14"},{"location":"ExampleDrivenGuide/cmd_guide/#ilund4u-databases","title":"iLund4u databases","text":"Figure 15 <p>iLund4u has two precomputed databases of hotspots built on phage and plasmid sequences (Fig. 15). The database of phages was built based on running hotspot annotation mode on all available PhageScope database sequences (~870K sequences, version of September 2024). For plasmids database we took IMG/PR database of plasmids (~700K sequences, version of June 2024).  </p>"},{"location":"ExampleDrivenGuide/cmd_guide/#downloading-database","title":"Downloading database","text":"<p>To download iLund4u database from our server [data-sharing.atkinson-lab.com/iLund4u] you can use the following command: <code>--database &lt;phages|plasmids&gt;</code>. For example, to get plasmids database you need to run: <pre><code>ilund4u --database plasmids\n</code></pre></p> <p>Database sizes: Phages: 6.2GB; Plasmids: 1.12GB </p>"},{"location":"FAQ/FAQ/","title":"FAQ","text":"<p>Q: Something weird is happening, e.g. program is crashing with my data. What should I do? A: Please add <code>--debug</code> to the command line (or <code>--parsing-debug</code> if some files were not read correctly) and post the details in Issues.</p> <p>Q: How to get representative protein ids of flanking genes for a hotspot A: Each hotspot includes an attribute called conserved signature, which contains the protein IDs of representative sequences for protein families that are highly conserved as flanking genes (found as flanking genes in over 75% of hotspot islands). Starting with iLund4u version 0.0.8, this information is available in the conserved_signature column of the hotspot_annotation.tsv file.</p>"},{"location":"Parameters/cmd_parameters/","title":"\u0421ommand-line parameters","text":""},{"location":"Parameters/cmd_parameters/#no-mode","title":"No mode","text":""},{"location":"Parameters/cmd_parameters/#post-install-steps","title":"Post-install steps","text":"<code>--data</code> Creates the 'ilund4u_data' folder in the current working directory.  The folder contains adjustable configuration files used by iLund4u. <code>--linux</code> Replaces the mmseqs path in the pre-made config file from the MacOS  version [default] to the Linux version. <code>--mac</code> Replaces the mmseqs path in the pre-made config file from the Linux  version to the MacOS version. <code>--get-hmms</code> Download HMMs (hmmscan format) of defence, anti-defence, virulence,  and AMR proteins from our server [data-sharing.atkinson-lab.com] <code>--database &lt;phages|plasmids&gt;</code> Download iLund4u database from our server [data-sharing.atkinson-lab.com].  Available databases: \"phages\" and \"plasmids\" (see documentation for details)"},{"location":"Parameters/cmd_parameters/#miscellaneous-arguments","title":"Miscellaneous arguments","text":"<code>-h --help</code> Show this help message and exit. <code>-v, --version</code> Show program version. <code>--debug</code> Provide detailed stack trace for debugging purposes. <code>-q, --quiet</code> Don't show progress messages. <p>iLund4u can be used in three different modes:</p> <ul> <li> <p><code>ilund4u hotspots [parameters]</code> - hotspot annotation.</p> </li> <li> <p><code>ilund4u protein [parameters]</code> - protein search versus iLund4u database.</p> </li> <li> <p><code>ilund4u proteome [parameters]</code> - proteome annotation and search versus iLund4u database.</p> </li> </ul> <p>To show mode-specific help messages use: <code>ilund4u --help [mode]</code> (e.g. <code>ilund4u --help hotspots</code>)</p>"},{"location":"Parameters/cmd_parameters/#hotspot-annotation-mode","title":"Hotspot annotation mode","text":""},{"location":"Parameters/cmd_parameters/#mandatory-arguments","title":"Mandatory arguments","text":"<code>-gff &lt;folder&gt;</code> Path to a folder containing extended gff files.  Each gff file should contain a corresponding nucleotide sequence.  (designed to handle pharokka/prokka produced annotation files)."},{"location":"Parameters/cmd_parameters/#optional-arguments-data-processing","title":"Optional arguments | data processing","text":"<code>-ufid, --use-filename-as-id</code> Use filename (wo extension) as contig id instead  of the contig id written in a gff file. <code>-mps, --min-proteome-size &lt;int&gt;</code> Minimal number of proteins in a genome to be taken in analysis.  [default: 15] <code>-gct, --genome-circularity-table &lt;path&gt;</code> Path to a table containing information for each genome about whether  it is circular or not.  Format: tab-separated; column names: id, circular. Values 1 or 0.  For genomes which id is not listed, the default value will be used.  [default: non circular; use <code>-cg/--circular-genomes</code> to change] <code>-psc, --proteome-sim-cutoff &lt;float&gt;</code> Minimal fraction of shared homologous proteins between two proteomes to be connected in proteome network by edge. [default: 0.7] <code>-mpcs, --min-proteome-community-size &lt;int&gt;</code> Minimal size of proteome community which then will be taken in    hotspot search analysis [default: 10]. <code>-vpc, --variable-protein-cutoff &lt;float&gt;</code> Upper limit of fraction of proteomes in which a protein homology group should be found within a proteome community to be called \"variable\". [default: 0.25] <code>-cpc, --conserved-protein-cutoff &lt;float&gt;</code> Lower limit of fraction of proteomes in which a protein homology group should be found within a proteome community to be called \"conserved\". [default: 0.75] <code>-cg, --circular-genomes</code> Consider each input genome as circular. That means that first and last proteins will be considered as neighbours. [default: non circular] <code>-ncg, --non-circular-genomes</code> Consider each input genome as non-circular. That means that first and last proteins won't be considered as neighbours. [default: circular] <code>-hpc, --hotspot-presence-cutoff</code> Fraction of proteomes within a proteome community in which clustered variable islands should be found to be considered as a hotspot. [default: 0.3] <code>-rnf, --report-not-flanked</code> Report in results hotspots that have flanked conserved genes only on one side (located on the end of non-circular sequences) [default: False]"},{"location":"Parameters/cmd_parameters/#optional-arguments-others","title":"Optional arguments | others","text":"<code>-o &lt;dir name&gt;</code> Output dir name. This will be created if it does not exist. [default: ilund4u_{current_date}; e.g. ilund4u_2022_07_25-20_41] <code>-o-db, --output-database &lt;dirname&gt;</code> Output dir name for the database. If not specified then no database will be saved. <code>-c &lt;standard|&lt;file.cfg&gt;</code> Path to a configuration file or name of a premade config file [default: standard]"},{"location":"Parameters/cmd_parameters/#miscellaneous-arguments_1","title":"Miscellaneous arguments","text":"<code>--debug</code> Provide a detailed stack trace for debugging purposes. <code>-q, --quiet</code> Don't show progress messages."},{"location":"Parameters/cmd_parameters/#protein-annotation-mode","title":"Protein annotation mode","text":""},{"location":"Parameters/cmd_parameters/#mandatory-arguments_1","title":"Mandatory arguments","text":"<code>-fa &lt;path&gt;</code> Path to a fasta file with query protein sequence. <code>-db &lt;path&gt;</code> Path to iLund4u database."},{"location":"Parameters/cmd_parameters/#optional-arguments-data-processing_1","title":"Optional arguments | data processing","text":"<code>-hsm, --homology-search-mode &lt;group|proteins&gt;</code> Mode to define homologous proteins from the database. If \"group\" is selected, a query protein is assigned to the same homologous group as that of the best search hit protein. In \"proteins\" mode, all target proteins that pass the cutoffs are considered to be homologues. [default: group] <code>-msqc, --mmseqs-query-cov &lt;float&gt;</code> MMseqs search query coverage cutoff [default: 0.65] <code>-mstc, --mmseqs-target-cov &lt;float&gt;</code> MMseqs search query coverage cutoff [default: 0.65] <code>-msf, --mmseqs-fident &lt;float&gt;</code> MMseqs search fident (fraction of identical matches) cutoff [default: 0.2] <code>-mse, --mmseqs-evalue</code> MMseqs search evalue cutoff [default: 1e-5] <code>-fm, --fast-mmseqs</code> MMseqs search only against a database of representative sequences.  [default: False] <code>-rnf, --report-not-flanked</code> Report in results hotspots that have flanked conserved genes only on one side (located on the end of non-circular sequences) [default: False] <code>-ql, --query-label &lt;str&gt;</code> Label for query protein homologues on LoVis4u visualisation. [default: database protein names]"},{"location":"Parameters/cmd_parameters/#optional-arguments-others_1","title":"Optional arguments | others","text":"<code>-o &lt;name&gt;</code> Output dir name. This will be created if it does not exist. [default: ilund4u_{current_date}; e.g. ilund4u_2022_07_25-20_41] <code>-c &lt;standard|&lt;file.cfg&gt;</code> Path to a configuration file or name of a premade config file [default: standard]"},{"location":"Parameters/cmd_parameters/#miscellaneous-arguments_2","title":"Miscellaneous arguments","text":"<code>--debug</code> Provide a detailed stack trace for debugging purposes. <code>-q, --quiet</code> Don't show progress messages."},{"location":"Parameters/cmd_parameters/#proteome-annotation-mode","title":"Proteome annotation mode","text":""},{"location":"Parameters/cmd_parameters/#mandatory-arguments_2","title":"Mandatory arguments","text":"<code>-gff &lt;path&gt;</code> Path to a query proteome gff file. (designed to handle pharokka/prokka produced annotation files). <code>-db &lt;path&gt;</code> Path to iLund4u database."},{"location":"Parameters/cmd_parameters/#optional-arguments-data-processing_2","title":"Optional arguments | data processing","text":"<code>-ncg, --non-circular-genomes</code> Consider tje query genome as non-circular. This means that first and last proteins won't be considered to be neighbours. [default: circular] <p>Note: MMseqs arguments below are used for assigning homology       group for each protein in your query proteome.</p> <code>-msqc, --mmseqs-query-cov &lt;float&gt;</code> MMseqs search query coverage cutoff [default: 0.65] <code>-mstc, --mmseqs-target-cov &lt;float&gt;</code> MMseqs search query coverage cutoff [default: 0.65] <code>-msf, --mmseqs-fident &lt;float&gt;</code> MMseqs search fident (fraction of identical matches) cutoff [default: 0.2] <code>-mse, --mmseqs-evalue</code> MMseqs search evalue cutoff [default: 1e-5] <code>-fm, --fast-mmseqs</code> MMseqs search only against a database of representative sequences.  [default: False] <code>-rnf, --report-not-flanked</code> Report in results hotspots that have flanked conserved genes only on one side (located on the end of non-circular sequences) [default: False]"},{"location":"Parameters/cmd_parameters/#optional-arguments-others_2","title":"Optional arguments | others","text":"<code>-o &lt;name&gt;</code> Output dir name. It will be created if it does not exist. [default: ilund4u_{current_date}; e.g. ilund4u_2022_07_25-20_41] <code>-c &lt;standard|&lt;file.cfg&gt;</code> Path to a configuration file or name of a premade config file [default: standard]"},{"location":"Parameters/cmd_parameters/#miscellaneous-arguments_3","title":"Miscellaneous arguments","text":"<code>--debug</code> Provide a detailed stack trace for debugging purposes. <code>--parsing-debug</code> Provide detailed stack trace for debugging purposes  for failed reading of gff files. <code>-q, --quiet</code> Don't show progress messages."},{"location":"Parameters/config_parameters/","title":"Configuration file parameters","text":"<p>iLund4u configuration file allows detailed customization of the tool's parameters.</p> <p>Note: You can copy the lovis4u_data folder that contains the config files to your wiking directory with <code>ilund4u --data</code> command and safely edit and use them without affecting 'internal' config. If you want to use a copied config file, use <code>-c path_to_config</code>.</p>"},{"location":"Parameters/config_parameters/#detailed-description-is-under-construction","title":"Detailed description is under construction...","text":"<p>;[Data Loading and processing] default_transl_table = 11 gff_CDS_name_source = product min_proteome_size = 15 proteome_uniqueness_cutoff = 0.7 proteome_similarity_cutoff = 0.7 leiden_resolution_parameter_p = 0.5 min_proteome_community_size = 10 circular_genomes = True variable_protein_cluster_cutoff = 0.25 conserved_protein_cluster_cutoff = 0.75 neighbours_max_distance = 8 neighbours_one_side_max_size = 5 neighbours_min_size = 5 island_neighbours_similarity_cutoff = 0.65 leiden_resolution_parameter_i = 0.55 deduplicate_proteomes_within_hotspot = True hotspot_presence_cutoff = 0.3 hotspot_signature_presence_cutoff = 0.75 hotspot_similarity_cutoff = 0.65 leiden_resolution_parameter_h = 0.55 protein_search_target_mode = group  </p> <p>;[mmseqs parameters] mmseqs_cluster_mode = 0 mmseqs_cov_mode = 0 mmseqs_min_seq_id = 0.25 mmseqs_c = 0.8 mmseqs_s = 7 mmseqs_search_qcov = 0.6 mmseqs_search_tcov = 0.6 mmseqs_search_fident = 0.2 mmseqs_search_evalue = 1e-5  </p> <p>;[pyhmmer hmmscan parameters] hmmscan_query_coverage_cutoff = 0.7 hmmscan_hmm_coverage_cutoff = 0.5  </p> <p>;[visualisation] show_hmmscan_hits_on_full_proteomes = True lovis4u_hotspot_vis_figure_width = 8 lovis4u_hotspot_mm_per_nt = 0.0125 lovis4u_proteome_mm_per_nt = 0.005 index_only_for_hypothetical_proteins = auto island_size_cutoff_to_show_index_only = 7 max_number_of_seqs_to_redefine_order = 300  </p> <p>;[HMM] hmm_defence= {internal}/HMMs/DefenceFinder_CasFinder hmm_virulence =  {internal}/HMMs/VFDB hmm_anti_defence = {internal}/HMMs/dbAPIS_Acr hmm_amr = {internal}/HMMs/AMRFinderPlus database_names = Defence,Virulence,Anti-defence,AMR  </p> <p>;[Other paths] mmseqs_binary = {internal}/bin/mmseqs_mac/bin/mmseqs category_colours = {internal}/category_colours_proteome_groups.tsv  </p> <p>;[Output] verbose = True debug = False output_dir = ilund4u_{current_date}  </p> <p>;[URLs] hmm_models = https://data-sharing.atkinson-lab.com/iLund4u/HMMs.tar.gz plasmids_db = https://data-sharing.atkinson-lab.com/iLund4u/iLund4u_DB-plasmids.tar.gz phages_db = https://data-sharing.atkinson-lab.com/iLund4u/iLund4u_DB-phages.tar.gz </p>"},{"location":"VersionLog/versions/","title":"Version log","text":"<ul> <li> <p>Preprint release \u2728</p> </li> <li> <p>Ver 0.0.7.1 - 15 October 2024</p> <ul> <li><code>-use-filename-as-id</code> and <code>--parsing-deubg</code> parameters were added</li> </ul> </li> <li> <p>Ver 0.0.7 - 15 October 2024</p> <ul> <li>Minor bug fixes</li> <li>Visualisation adjusment</li> </ul> </li> <li> <p>Ver 0.0.6 - 30 September 2024 </p> <ul> <li>Hotspots that are located on the end of non-circular chromosomes are now treated separately. </li> <li><code>-rnf, --report-not-flanked</code> parameter is added.</li> </ul> </li> <li> <p>Ver 0.0.5.2 - 8 August 2024 </p> <ul> <li>Adjusment to the 0.0.5.1 v. update</li> </ul> </li> <li> <p>Ver 0.0.5.1 - 5 August 2024 </p> <ul> <li>Long table file name bug is fixed (adjusment to the 0.0.5 v. update).</li> </ul> </li> <li> <p>Ver 0.0.5 - 25 July 2024 </p> <ul> <li>Long pdf file name bug is fixed.</li> </ul> </li> <li> <p>Ver 0.0.4 - 27 June 2024 </p> <ul> <li>Minor fixes and parameter name changes.</li> </ul> </li> <li> <p>Ver 0.0.3 - 27 June 2024 </p> <ul> <li>Minor fixes.</li> </ul> </li> <li> <p>Ver 0.0.2 - 13 June 2024 </p> <ul> <li><code>-ncg</code>/<code>--non-circular-genomes</code> cmd parameter is added.</li> <li>Minor fixes.</li> </ul> </li> <li> <p>Ver 0.0.1 - 13 June 2024 - first public release. </p> </li> </ul>"}]}